{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "pd.set_option(\"display.width\", 500)\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.notebook_repr_html\", True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Import data\n",
    "We now import all data from all cases (not just a subsample)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data = pd.read_csv('all_cases.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_cite</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "      <th>us_cite</th>\n",
       "      <th>case</th>\n",
       "      <th>case_id</th>\n",
       "      <th>caseId</th>\n",
       "      <th>docketId</th>\n",
       "      <th>usCite</th>\n",
       "      <th>docket</th>\n",
       "      <th>dateArgument</th>\n",
       "      <th>caseOriginState</th>\n",
       "      <th>jurisdiction</th>\n",
       "      <th>issueArea</th>\n",
       "      <th>decisionDirection</th>\n",
       "      <th>decisionType</th>\n",
       "      <th>lawType</th>\n",
       "      <th>majOpinWriter</th>\n",
       "      <th>majVotes</th>\n",
       "      <th>minVotes</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eagles v. Samuels 329 U.S. 304 (1946)</td>\n",
       "      <td>Eagles v. Samuels No. 59 Argued November 21, 1...</td>\n",
       "      <td>https://supreme.justia.com/cases/federal/us/32...</td>\n",
       "      <td>329 U.S. 304</td>\n",
       "      <td>Eagles v. Samuels</td>\n",
       "      <td>0</td>\n",
       "      <td>1946-020</td>\n",
       "      <td>1946-020-01</td>\n",
       "      <td>329 U.S. 304</td>\n",
       "      <td>59</td>\n",
       "      <td>11/21/1946</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>81</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Eagles v. Horowitz 329 U.S. 317 (1946)</td>\n",
       "      <td>Eagles v. Horowitz No. 58 Argued November 21, ...</td>\n",
       "      <td>https://supreme.justia.com/cases/federal/us/32...</td>\n",
       "      <td>329 U.S. 317</td>\n",
       "      <td>Eagles v. Horowitz</td>\n",
       "      <td>1</td>\n",
       "      <td>1946-021</td>\n",
       "      <td>1946-021-01</td>\n",
       "      <td>329 U.S. 317</td>\n",
       "      <td>58</td>\n",
       "      <td>11/21/1946</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>81</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Labor Board v. A. J. Tower Co. 329 U.S. 324 (1...</td>\n",
       "      <td>Labor Board v. A. J. Tower Co. No. 60 Argued N...</td>\n",
       "      <td>https://supreme.justia.com/cases/federal/us/32...</td>\n",
       "      <td>329 U.S. 324</td>\n",
       "      <td>Labor Board v. A. J. Tower Co.</td>\n",
       "      <td>2</td>\n",
       "      <td>1946-022</td>\n",
       "      <td>1946-022-01</td>\n",
       "      <td>329 U.S. 324</td>\n",
       "      <td>60</td>\n",
       "      <td>11/21/1946</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>82</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gibson v. United States 329 U.S. 338 (1946)</td>\n",
       "      <td>Gibson v. United States No. 23 Argued January ...</td>\n",
       "      <td>https://supreme.justia.com/cases/federal/us/32...</td>\n",
       "      <td>329 U.S. 338</td>\n",
       "      <td>Gibson v. United States</td>\n",
       "      <td>3</td>\n",
       "      <td>1946-023</td>\n",
       "      <td>1946-023-01</td>\n",
       "      <td>329 U.S. 338</td>\n",
       "      <td>23</td>\n",
       "      <td>1/2/1946</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>85</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Illinois v. Campbell 329 U.S. 362 (1946)</td>\n",
       "      <td>Illinois ex rel. Gordon v. Campbell No. 35 Arg...</td>\n",
       "      <td>https://supreme.justia.com/cases/federal/us/32...</td>\n",
       "      <td>329 U.S. 362</td>\n",
       "      <td>Illinois v. Campbell</td>\n",
       "      <td>4</td>\n",
       "      <td>1946-024</td>\n",
       "      <td>1946-024-01</td>\n",
       "      <td>329 U.S. 362</td>\n",
       "      <td>35</td>\n",
       "      <td>3/28/1946</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>85</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           full_cite                                               text                                                url       us_cite                            case  case_id    caseId     docketId        usCite docket dateArgument  caseOriginState  jurisdiction  issueArea  decisionDirection  decisionType  lawType  majOpinWriter  majVotes  minVotes  year\n",
       "0              Eagles v. Samuels 329 U.S. 304 (1946)  Eagles v. Samuels No. 59 Argued November 21, 1...  https://supreme.justia.com/cases/federal/us/32...  329 U.S. 304               Eagles v. Samuels        0  1946-020  1946-020-01  329 U.S. 304     59   11/21/1946               35             1          3                  1             1        3             81         9         0  1946\n",
       "1             Eagles v. Horowitz 329 U.S. 317 (1946)  Eagles v. Horowitz No. 58 Argued November 21, ...  https://supreme.justia.com/cases/federal/us/32...  329 U.S. 317              Eagles v. Horowitz        1  1946-021  1946-021-01  329 U.S. 317     58   11/21/1946               35             1          3                  1             1        3             81         9         0  1946\n",
       "2  Labor Board v. A. J. Tower Co. 329 U.S. 324 (1...  Labor Board v. A. J. Tower Co. No. 60 Argued N...  https://supreme.justia.com/cases/federal/us/32...  329 U.S. 324  Labor Board v. A. J. Tower Co.        2  1946-022  1946-022-01  329 U.S. 324     60   11/21/1946              NaN             1          7                  2             1        3             82         8         1  1946\n",
       "3        Gibson v. United States 329 U.S. 338 (1946)  Gibson v. United States No. 23 Argued January ...  https://supreme.justia.com/cases/federal/us/32...  329 U.S. 338         Gibson v. United States        3  1946-023  1946-023-01  329 U.S. 338     23     1/2/1946               49             1          3                  2             1        3             85         9         0  1946\n",
       "4           Illinois v. Campbell 329 U.S. 362 (1946)  Illinois ex rel. Gordon v. Campbell No. 35 Arg...  https://supreme.justia.com/cases/federal/us/32...  329 U.S. 362            Illinois v. Campbell        4  1946-024  1946-024-01  329 U.S. 362     35    3/28/1946               17             1         12                  2             1        3             85         7         2  1946"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Train-test split\n",
    "We split the cases in the entire dataset into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#select training and test cases (70% training, 30% test) and add a train-test column to the dataframe\n",
    "#add train-test column to dataframe \n",
    "all_data['training'] = np.random.choice([0, 1], size=(len(all_data)), p=[.3, .7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Cleaning text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as in SVM.ipynb, the `get_parts` function below takes an opinion (a string), and returns the verbs and nouns in the opinion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re \n",
    "regex1 = r\"\\(.\\)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pattern.en import parse\n",
    "from pattern.en import pprint\n",
    "from pattern.en import conjugate, lemma, lexeme\n",
    "from pattern.vector import stem, PORTER, LEMMA\n",
    "from sklearn.feature_extraction import text\n",
    "import string\n",
    "\n",
    "#stopwords and punctuation\n",
    "stopwords=text.ENGLISH_STOP_WORDS\n",
    "punctuation = list('.,;:!?()[]{}`''\\\"@#$^&*+-|=~_')\n",
    "\n",
    "def get_parts(opinion):\n",
    "    oplow = opinion.lower()\n",
    "    #REMOVING CHARACTERS: we have ugly text, and remove unnecssary characters.\n",
    "    oplow = unicode(oplow, 'ascii', 'ignore') #remove non-unicode characters \n",
    "    oplow = str(oplow).translate(string.maketrans(\"\\n\\t\\r\", \"   \")) #remove characters like \\n \n",
    "    #justices (eg, Justice Breyer) are referred to as J. (eg,Breyer, J.); we remove the J., also JJ. for plural\n",
    "    oplow = oplow.replace('j.','')\n",
    "    oplow = oplow.replace('jj.','')\n",
    "    oplow = oplow.replace('c.','') #remove C. for chief justice \n",
    "    oplow = oplow.replace('pp.','') #page numbers\n",
    "    oplow = oplow.replace('  ','') #multiple spaces\n",
    "    oplow = ''.join([i for i in oplow if not i.isdigit()]) #remove digits \n",
    "    oplow=re.sub(regex1, ' ', oplow)\n",
    "    #Remove the Justia disclaimer at the end of the case, if it appears in the string\n",
    "    justiadisclaimer = \"disclaimer: official\"\n",
    "    if justiadisclaimer in oplow: \n",
    "        optouse = oplow.split(justiadisclaimer)[0]\n",
    "    else:\n",
    "        optouse = oplow\n",
    "    \n",
    "    #GET A LIST OF PRECEDENTS CITED IN THE OPINION \n",
    "    wordslist = optouse.split()\n",
    "    #find precedents based on string 'v.' (eg, 'Brown v. Board')\n",
    "    indices = [i for i in range(len(wordslist)) if wordslist[i]=='v.']\n",
    "    precedents = [wordslist[i-1]+ ' ' + wordslist[i]+ ' ' + wordslist[i+1] for i in indices]\n",
    "    \n",
    "    #remove precedents, as we have already accounted for these\n",
    "    for precedent in precedents:\n",
    "        optouse = optouse.replace(precedent,'')\n",
    "    \n",
    "    #PARSE INTO LIST OF LISTS --> GET WORDS\n",
    "    parsed = parse(optouse,tokenize=True,chunks=False,lemmata=True).split()\n",
    "    verbsnouns = [] \n",
    "    i=0\n",
    "    #Create lists of lists of verbs and nouns in each sentence.\n",
    "    for sentence in parsed: #for each sentence \n",
    "        verbsnouns.append([])\n",
    "        for token in sentence: #for each word in the sentence \n",
    "            if token[0] in punctuation or token[0] in stopwords or len(token[0])<=2:\n",
    "                continue\n",
    "            wordtouse = token[0]\n",
    "            for x in punctuation:\n",
    "                wordtouse = wordtouse.replace(x,' ') #if punctuation in word, take it out\n",
    "            if token[1] in ['VB','VBZ','VBP','VBD','VBN','VBG','NN','NNS','NNP','NNPS']:\n",
    "                verbsnouns[i].append(lemma(wordtouse)) #append the lemmatized word (we relemmatize because lemmata in parse does not seem to always work)\n",
    "        i+=1  \n",
    "    #Zip together lists so each tuple is a sentence. \n",
    "    #out=zip(verbs,nouns)\n",
    "    #verbs2 = []\n",
    "    #nouns2 = []\n",
    "    #for sentence in out: \n",
    "    #    if sentence[0]!=[] and sentence[1]!=0: #if the sentence has at least one verb and noun, keep it. Otherwise, drop it.\n",
    "    #        if type(sentence[0])==list: \n",
    "    #            verbs2.append(sentence[0])\n",
    "    #        else: \n",
    "    #            verbs2.append([sentence[0]]) #if verb is a string rather than a list, put string in list\n",
    "    #        if type(sentence[1])==list:\n",
    "    #            nouns2.append(sentence[1])\n",
    "    #        else:\n",
    "    #            nouns2.append([sentence[1]])\n",
    "    return(verbsnouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pattern.en import parse\n",
    "from pattern.en import pprint\n",
    "from pattern.en import conjugate, lemma, lexeme\n",
    "from pattern.vector import stem, PORTER, LEMMA\n",
    "from sklearn.feature_extraction import text\n",
    "import string\n",
    "\n",
    "#stopwords and punctuation\n",
    "stopwords=text.ENGLISH_STOP_WORDS\n",
    "punctuation = list('.,;:!?()[]{}`''\\\"@#$^&*+-|=~_')\n",
    "\n",
    "def get_parts(opinion):\n",
    "    oplow = opinion.lower()\n",
    "    #REMOVING CHARACTERS: we have ugly text, and remove unnecssary characters.\n",
    "    oplow = unicode(oplow, 'ascii', 'ignore') #remove non-unicode characters \n",
    "    oplow = str(oplow).translate(string.maketrans(\"\\n\\t\\r\", \"   \")) #remove characters like \\n \n",
    "    #justices (eg, Justice Breyer) are referred to as J. (eg,Breyer, J.); we remove the J., also JJ. for plural\n",
    "    oplow = oplow.replace('j.','')\n",
    "    oplow = oplow.replace('jj.','')\n",
    "    oplow = oplow.replace('c.','') #remove C. for chief justice \n",
    "    oplow = oplow.replace('pp.','') #page numbers\n",
    "    oplow = oplow.replace('  ','') #multiple spaces\n",
    "    oplow = ''.join([i for i in oplow if not i.isdigit()]) #remove digits \n",
    "    oplow=re.sub(regex1, ' ', oplow)\n",
    "    #Remove the Justia disclaimer at the end of the case, if it appears in the string\n",
    "    justiadisclaimer = \"disclaimer: official\"\n",
    "    if justiadisclaimer in oplow: \n",
    "        optouse = oplow.split(justiadisclaimer)[0]\n",
    "    else:\n",
    "        optouse = oplow\n",
    "    \n",
    "    #GET A LIST OF PRECEDENTS CITED IN THE OPINION \n",
    "    wordslist = optouse.split()\n",
    "    #find precedents based on string 'v.' (eg, 'Brown v. Board')\n",
    "    indices = [i for i in range(len(wordslist)) if wordslist[i]=='v.']\n",
    "    precedents = [wordslist[i-1]+ ' ' + wordslist[i]+ ' ' + wordslist[i+1] for i in indices]\n",
    "    \n",
    "    #remove precedents, as we have already accounted for these\n",
    "    for precedent in precedents:\n",
    "        optouse = optouse.replace(precedent,'')\n",
    "    \n",
    "    #PARSE INTO LIST OF LISTS --> GET WORDS\n",
    "    parsed = parse(optouse,tokenize=True,chunks=False,lemmata=True).split()\n",
    "    verbs = [] \n",
    "    nouns = [] \n",
    "    i=0\n",
    "    #Create lists of lists of verbs and nouns in each sentence.\n",
    "    for sentence in parsed: #for each sentence \n",
    "        verbs.append([])\n",
    "        nouns.append([])\n",
    "        for token in sentence: #for each word in the sentence \n",
    "            if token[0] in punctuation or token[0] in stopwords or len(token[0])<=2:\n",
    "                continue\n",
    "            wordtouse = token[0]\n",
    "            for x in punctuation:\n",
    "                wordtouse = wordtouse.replace(x,' ') #if punctuation in word, take it out\n",
    "            if token[1] in ['VB','VBZ','VBP','VBD','VBN','VBG']:\n",
    "                verbs[i].append(lemma(wordtouse)) #append the lemmatized verb (we relemmatize because lemmata in parse does not seem to always work)\n",
    "            if token[1] in ['NN','NNS','NNP','NNPS']:\n",
    "                nouns[i].append(lemma(wordtouse))\n",
    "        i+=1  \n",
    "    #Zip together lists so each tuple is a sentence. \n",
    "    out=zip(verbs,nouns)\n",
    "    verbs2 = []\n",
    "    nouns2 = []\n",
    "    for sentence in out: \n",
    "        if sentence[0]!=[] and sentence[1]!=0: #if the sentence has at least one verb and noun, keep it. Otherwise, drop it.\n",
    "            if type(sentence[0])==list: \n",
    "                verbs2.append(sentence[0])\n",
    "            else: \n",
    "                verbs2.append([sentence[0]]) #if verb is a string rather than a list, put string in list\n",
    "            if type(sentence[1])==list:\n",
    "                nouns2.append(sentence[1])\n",
    "            else:\n",
    "                nouns2.append([sentence[1]])\n",
    "    return(verbs2,nouns2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Lists of words, vocabularies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the dataframe does seem to have one case that does not have a string type for the text, for some reason. This cell removes all cases whose text is not a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_types = np.array(map(type, all_data.text)) == str\n",
    "all_data = all_data[str_types]\n",
    "all_data = all_data.reset_index(drop=True)\n",
    "# should return true, if the text column is only strings\n",
    "(np.array(map(type, all_data.text)) == str).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, we run get_parts on all the opinions to get lists of verbs and nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "words = []\n",
    "for op in all_data.text:\n",
    "    words.append(get_parts(op))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a list of issue areas (our y variable), which is in the same order as the cases in all_data (and thus matches the order of cases in verbwords, nounwords, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "issue_areas = all_data.issueArea.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next create vocabularies, and also create maps between word id's and words (and vice versa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create vocabs\n",
    "vocab = list(set([word for sublist in words for subsublist in sublist for word in subsublist]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dictionaries: id --> word\n",
    "id2word = dict(enumerate(vocab))\n",
    "#dictionaries: word --> id\n",
    "word2id = dict(zip(id2word.values(),id2word.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create corpuses (one for each word type). Each corpus is a list of lists: each inner list corresponds to an opinion, and has as its elements tuples of the form `(wordid, count)`, where `count` refers to the number of times the word appears in the opinion. This is described in further detail in SVM.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this function takes a list of words, and outputs a list of tuples \n",
    "counter = lambda x:list(set([(i,x.count(i)) for i in x]))\n",
    "\n",
    "#corpus_creator takes a list of lists of lists like verbwords, or a list of lists like precedents_all. \n",
    "#It also takes a word2id dictionary.\n",
    "def corpus_creator(sentence_word_list,word2id):\n",
    "    counter = lambda x:list(set([(word2id[i],x.count(i)) for i in x]))\n",
    "    op_word_list = []\n",
    "    if type(sentence_word_list[0][0])==list: #if list of lists of lists \n",
    "        for opinion in sentence_word_list: \n",
    "            #for each list (which corresponds to an opinion) in sentence_word_list, get a list of the words\n",
    "            op_word_list.append([word for sublist in opinion for word in sublist])\n",
    "    else: #if list of lists \n",
    "        op_word_list = sentence_word_list\n",
    "    corpus = []\n",
    "    for element in op_word_list: \n",
    "        corpus.append(counter(element))\n",
    "    return(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes a corpus and a number of words, and returns a matrix in which the element at row i and column j is the number of occurrences of word j in document i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# takes a corpus and a number of words, and returns a matrix in which the element at row i and column j is the number of\n",
    "# occurrences of word j in document i.\n",
    "def corpus_to_mat(corpus, num_words):\n",
    "    n = len(corpus)\n",
    "    M = np.zeros((n, num_words))\n",
    "    for i,doc in enumerate(corpus):\n",
    "        for word,count in doc:\n",
    "            M[i][word] = count\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 20.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "corpus = corpus_creator(words, word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 835 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_corpus = [corpus[i] for i in range(len(corpus)) if all_data['training'][i]==1]\n",
    "train_mat = corpus_to_mat(train_corpus, len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 379 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_corpus = [corpus[i] for i in range(len(corpus)) if all_data['training'][i]==0]\n",
    "test_mat = corpus_to_mat(test_corpus, len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "#this function takes a training matrix of size n_documents_training*vocab_size and a test matrix\n",
    "#of size n_documents_test*vocab_size. The function outputs the corresponding tfidf matrices.\n",
    "#Note that we fit on the training data, and then apply that fit to the test data.\n",
    "def tfidf_mat_creator(trainmatrix,testmatrix):\n",
    "    tf_idf_transformer=TfidfTransformer()\n",
    "    tfidf_fit = tf_idf_transformer.fit(trainmatrix)\n",
    "    tfidf_train = tfidf_fit.transform(trainmatrix).toarray()\n",
    "    tfidf_test = tfidf_fit.transform(testmatrix).toarray()\n",
    "    return(tfidf_train,tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_issue_areas = [issue_areas[i] for i in range(len(issue_areas)) if all_data['training'][i]==1]\n",
    "test_issue_areas = [issue_areas[i] for i in range(len(issue_areas)) if all_data['training'][i]==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_mat,test_mat = tfidf_mat_creator(train_mat,test_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "confusion_mat_creator\n",
    "\n",
    "Inputs\n",
    "------\n",
    "predictions: a list of length n_documents. Each value in the list is the predicted\n",
    "             issue area of the corresponding document. (\"Predicted\" as predicted by the SVM.)\n",
    "actuals: a list of length n_documents. Each value is the actual issue area of the corresponding document.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "A 14*14 confusion matrix. Cell i,j is the number of cases with actual issue area j \n",
    "that were predicted as issue area i. Thus, the diagonal represents correct predictions.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "see do_classify below for an example of how this is used\n",
    "\"\"\"\n",
    "\n",
    "def confusion_mat_creator(predictions,actuals): \n",
    "    confusion_mat = np.zeros((14,14))\n",
    "    for i in range(len(predictions)):\n",
    "        #get predicted and actual issue ares; subtract by 1 since matrix is 0-indexed\n",
    "        p_val = predictions[i]-1\n",
    "        a_val = actuals[i]-1\n",
    "        confusion_mat[p_val,a_val]+=1 #Matrix is thus predicted values*actual values \n",
    "    return(confusion_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "cv_optimize\n",
    "\n",
    "Inputs\n",
    "------\n",
    "clf : an instance of a scikit-learn classifier\n",
    "parameters: a parameter grid dictionary thats passed to GridSearchCV\n",
    "X: a document-word matrix (e.g., noun_train_tfidf). Should be training data.\n",
    "y: the response vector (train_issue_areas)\n",
    "n_folds: the number of cross-validation folds (default 5)\n",
    "score_func: a score function we might want to pass (default python None)\n",
    "   \n",
    "Returns\n",
    "-------\n",
    "Two things: (1) The best estimator from the GridSearchCV, after the GridSearchCV has been used to\n",
    "fit the model; and (2) the best parameter. \n",
    "     \n",
    "Notes\n",
    "-----\n",
    "see do_classify below for an example of how this is used\n",
    "\"\"\"\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "#note: this code comes directly from lab 6\n",
    "def cv_optimize(clf, parameters, X, y, n_folds=5, score_func=None):\n",
    "    if score_func:\n",
    "        gs = GridSearchCV(clf, param_grid=parameters, cv=n_folds, scoring=score_func)\n",
    "    else:\n",
    "        gs = GridSearchCV(clf, param_grid=parameters, cv=n_folds)\n",
    "    gs.fit(X, y)\n",
    "    best_estimator = gs.best_estimator_\n",
    "    best_param = gs.best_params_\n",
    "    return (best_estimator,best_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "do_classify\n",
    "\n",
    "Inputs\n",
    "------\n",
    "clf : an instance of a scikit-learn classifier\n",
    "parameters: a parameter grid dictionary thats passed to GridSearchCV\n",
    "Xtrain: a training matrix document-word matrix (e.g., noun_train_tfidf)\n",
    "ytrain: the corresponding training response vector (train_issue_areas)\n",
    "Xtest: a test matrix document-word matrix (e.g., noun_test_tfidf)\n",
    "ytest: the corresponding test resonse vector (e.g., noun)\n",
    "n_folds: the number of cross-validation folds (default 5)\n",
    "score_func: a score function we might want to pass (default python None)\n",
    "   \n",
    "Returns\n",
    "-------\n",
    "4 things, in the following order: (1) an array of predicted y values (ie, topic areas) for the test data; \n",
    "                                  (2) the accuracy score; (3) the confusion matrix of the test data predictions;\n",
    "                                  (4) the best parameter from the gridsearch.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.metrics import confusion_matrix\n",
    "def do_classify(clf, parameters, Xtrain, ytrain, Xtest, ytest, score_func=None, n_folds=5):\n",
    "    if parameters:\n",
    "        clf,best_param = cv_optimize(clf, parameters, Xtrain, ytrain, n_folds=n_folds, score_func=score_func)\n",
    "    clf=clf.fit(Xtrain, ytrain)\n",
    "    training_accuracy = clf.score(Xtrain, ytrain)\n",
    "    test_accuracy = clf.score(Xtest, ytest)\n",
    "    print \"############# based on standard predict ################\"\n",
    "    print \"Accuracy on training data: %0.2f\" % (training_accuracy)\n",
    "    print \"Accuracy on test data:     %0.2f\" % (test_accuracy)\n",
    "    print \"Best parameter: \", best_param\n",
    "    pred = clf.predict(Xtest)\n",
    "    print \"########################################################\"\n",
    "    print confusion_mat_creator(pred, ytest)\n",
    "    return(pred,test_accuracy,confusion_mat_creator(pred,ytest),best_param,clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we finally run the optimized SVM on the TF-IDF matrices, the issue areas contain some NaN's that we need to remove manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_valid_indices = ~np.isnan(train_issue_areas)\n",
    "test_valid_indices = ~np.isnan(test_issue_areas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5662L, 49101L)\n",
      "(5662L,)\n"
     ]
    }
   ],
   "source": [
    "train_mat = train_mat[train_valid_indices]\n",
    "train_issue_areas = np.array(train_issue_areas)[train_valid_indices]\n",
    "print train_mat.shape\n",
    "print train_issue_areas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2497L, 49101L)\n",
      "(2497L,)\n"
     ]
    }
   ],
   "source": [
    "test_mat = test_mat[test_valid_indices]\n",
    "test_issue_areas = np.array(test_issue_areas)[test_valid_indices]\n",
    "print test_mat.shape\n",
    "print np.array(test_issue_areas).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can run optimized SVM on the tfidf matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# based on standard predict ################\n",
      "Accuracy on training data: 0.94\n",
      "Accuracy on test data:     0.76\n",
      "Best parameter:  {'C': 1.0}\n",
      "########################################################\n",
      "[[ 507.   28.   14.   21.    2.    2.    1.    8.   28.    1.    1.    3.\n",
      "     1.    0.]\n",
      " [   5.  295.   11.   13.    2.    2.    3.   13.   27.   12.    0.    0.\n",
      "     1.    0.]\n",
      " [   4.    4.  162.    1.    0.    0.    0.    6.    8.    1.    0.    3.\n",
      "     0.    0.]\n",
      " [   7.    2.    1.   41.    0.    1.    0.    0.    0.    1.    0.    0.\n",
      "     0.    0.]\n",
      " [   1.    2.    0.    0.   19.    0.    0.    0.    2.    0.    0.    0.\n",
      "     1.    0.]\n",
      " [   1.    0.    1.    0.    0.   17.    0.    5.    1.    0.    0.    0.\n",
      "     0.    0.]\n",
      " [   0.    3.    1.    2.    0.    2.   93.    2.    5.   11.    0.    0.\n",
      "     0.    0.]\n",
      " [  18.   15.    9.    9.    1.    4.   10.  411.   32.   40.    2.   17.\n",
      "     1.    0.]\n",
      " [  17.   33.    8.    8.    0.    0.    6.   33.  218.   12.    1.    4.\n",
      "     3.    0.]\n",
      " [   0.    2.    1.    0.    1.    0.    2.    6.    4.   42.    2.    1.\n",
      "     0.    0.]\n",
      " [   0.    0.    0.    0.    0.    0.    0.    0.    1.    1.   18.    0.\n",
      "     1.    0.]\n",
      " [   1.    1.    0.    0.    0.    1.    0.    8.    3.    0.    0.   77.\n",
      "     0.    0.]\n",
      " [   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.]\n",
      " [   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.]]\n",
      "Wall time: 21.5 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Madhu\\Anaconda\\lib\\site-packages\\sklearn\\cross_validation.py:417: Warning: The least populated class in y has only 2 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=5.\n",
      "  % (min_labels, self.n_folds)), Warning)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.svm import LinearSVC\n",
    "best_predictions, best_accuracy, best_con_mat, best_param, best_clf=do_classify(LinearSVC(loss='hinge'), {'C':[1.0]},\n",
    "                                                                                train_mat, train_issue_areas,\n",
    "                                                                                test_mat, test_issue_areas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# turn issue areas into dummy column\n",
    "issue_areas = [\"criminal procedure\",\"civil rights\",\"first amendment\",\"due process\",\"privacy\",\"attorneys\",\n",
    "              \"unions\",\"economic activity\",\"judicial power\",\"federalism\",\"interstate  amendment\",\n",
    "              \"federal taxation\",\"miscellaneous\",\"private action\"]\n",
    "\n",
    "for issue, num in zip(issue_areas,range(1,15)):\n",
    "    all_data[issue] = all_data.issueArea.apply(lambda x: 1 if x == num else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# turn decision directions into dummy column (conservative, liberal, neutral)\n",
    "decision_areas = [\"conservative\",\"liberal\",\"neutral\"]\n",
    "\n",
    "for decision, num in zip(decision_areas,range(1,4)):\n",
    "    all_data[decision] = all_data.decisionDirection.apply(lambda x: 1 if x == num else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing Text\n",
    "\n",
    "We use the function **make_xy** to convert the syllabi (a collection of text documents) to numerical data (a matrix of token counts). The default vectorizer we use for this task is CountVectorizer, which produces a sparse representation of the counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function (from lab 9) to vectorize text - adapted to accomodate topics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def make_xy(df, issue, vectorizer=None):   \n",
    "    if vectorizer is None:\n",
    "        vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(df.text)\n",
    "    X = X.tocsc()\n",
    "    y = (df[issue] == 1).values.astype(np.int)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation\n",
    "\n",
    "We use the function **cv_score** to estimate the cross-validated value of a scoring function, given a classifier and data. In k-fold cross-validation, the test set is no longer needed; rather, the training set is split into *k* smaller sets. The following procedure is followed for each of the k “folds”: (1) the model is trained using k-1 of the folds as training data, (b) the model is validated on the remaining part of the data, (3) the performance of the model is \n",
    "calculated as the average of the values computed in the loop. Because we are working with a relatively small data sample (about 1,600 cases), approaches that are efficient in terms of data usage are preferable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "# function to return cross-validation score (lab 9)\n",
    "def cv_score(clf, X, y, scorefunc):\n",
    "    result = 0.\n",
    "    nfold = 5\n",
    "    for train, test in KFold(y.size, nfold): # split data into train/test groups, 5 times\n",
    "        clf.fit(X[train], y[train]) # fit\n",
    "        result += scorefunc(clf, X[test], y[test]) # evaluate score function on held-out data\n",
    "    return result / nfold # average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Probabilities\n",
    "\n",
    "We define a function to return an array of log probabilities of the samples for each class in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_likelihood(clf, x, y):\n",
    "    prob = clf.predict_log_proba(x)\n",
    "    not_topic = y == 0\n",
    "    topic = ~not_topic\n",
    "    return prob[not_topic, 0].sum() + prob[topic, 1].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Model\n",
    "\n",
    "Using the supporting functions above, we can now classify each issue area and store the output. One difference now is that we generally do not need to reuse cross-validation; since we already used cross-validatoin to determine optimal parameters for each topic area within the subsample data (in the Naive Bayes notebook), we can just use those optimal parameters here by loading them from a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_sample_results = pd.read_csv('naive_bayes_sample_model_results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
