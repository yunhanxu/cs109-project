{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "After deciding that using the full text of opinions would be an inefficient approach, we looked for ways to collect summarized legal data. In Supreme Court opinions, the *syllabus* is a formal, abbreviated section of the text that includes the major facts, arguments, and outcomes of the case. Fortunately, Justia has curated an open-source repository of Supreme Court cases (syllabi and opinions) organized by year and volume. We scraped all of the syllabi from years 1946 and beyond since our validation data set is limited by these temporal parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests as rq\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Data\n",
    "\n",
    "Fortunately, the Justia website organizes syllabi by year and volume. Because each of these categories follows a hierarchal structure (there exists a directory \"landing\" page for each year and volume), we can collect all of the individual case URLs from this directory and visit them separately. We want to limit our data to cases 1946 and beyond because this is the beginning of the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ---------- grab all case urls from respective year directory pages ----------\n",
    "\n",
    "# base url directory page for each year\n",
    "base_url = \"https://supreme.justia.com/cases/federal/us/year/%s.html\"\n",
    "\n",
    "# base url text page for each case\n",
    "case_url = \"https://supreme.justia.com\"\n",
    "id_url = []\n",
    "\n",
    "# iterate through years 1946 to 2015\n",
    "years = range(1946,2016)\n",
    "for year in years:\n",
    "    soup = BeautifulSoup(rq.get(base_url % year).text, \"lxml\")\n",
    "    results = soup.findAll(\"div\", attrs={\"class\":\"result\"})\n",
    "    \n",
    "    # collect all case urls on each year page\n",
    "    for result in results:\n",
    "        id_url.append(case_url + result.a[\"href\"])\n",
    "    \n",
    "    # prevent connection error\n",
    "    sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the Justia data only includes the data in HTML format. Therefore, because of inconsistencies in how the DOM is structured (for instance, some cases have no syllabi, some years have no metadata or different ways of organizing the text within the syllabi), we need to institute (1) a check for syllabi (skipping over cases that only include full opinions for consistency) and (2) directly scrape the entire text of the syllabi. We can clean the data after gathering it fully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ---------- visit each case page, scrape syllabus, store data ----------\n",
    "\n",
    "# initially split page into metadata and text (irregular formatting, some null)\n",
    "metadata,syllabus,citations,urls=[],[],[],[]\n",
    "\n",
    "# iterate through unique ids collected above\n",
    "for url in id_url:\n",
    "    # go to section of the DOM with text\n",
    "    soup = BeautifulSoup(rq.get(url).text, \"lxml\")\n",
    "    \n",
    "    # check if syllabus exists\n",
    "    header = soup.find(\"ul\", attrs={\"class\":\"centered-list clear\"})\n",
    "    exists = False\n",
    "    \n",
    "    if header is not None:\n",
    "        if header.text.lower().find(\"syllabus\") > -1:\n",
    "            exists = True\n",
    "\n",
    "        # if syllabus exists, collect text\n",
    "        if exists:    \n",
    "            # save name of case\n",
    "            name = soup.find(\"h1\", attrs={\"class\":\"title\"}).text\n",
    "\n",
    "            page_text = soup.find(\"span\", attrs={\"class\":\"headertext\"}) \n",
    "            if page_text is None:\n",
    "                page_text = soup.find(\"div\", attrs={\"id\":\"opinion\"})\n",
    "            \n",
    "            # collect syllabus text\n",
    "            syllabus_list = \"\"\n",
    "            for index in range(0,len(page_text.findAll(\"p\"))):\n",
    "\n",
    "                # don't append blank lines or returns\n",
    "                if page_text.findAll(\"p\")[index] != \"\":\n",
    "                    syllabus_list += ((page_text.findAll(\"p\")[index].text) + \" \")\n",
    "\n",
    "            metadata.append(name)\n",
    "            syllabus.append(syllabus_list)\n",
    "            citations.append(url.split(\"/\")[-3] + \" U.S. \" + url.split(\"/\")[-2])\n",
    "            urls.append(url)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing Data\n",
    "\n",
    "Now that we have stored the \"metadata\" (title of the page, which includes its full citation), syllabus, and original Justia URL for each case, we can do a bit of cleaning to make the citations mergable and then save as a CSV for ease of sharing among group members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ---------- create dataframe ----------\n",
    "rawdict = {}\n",
    "rawdict[\"full_cite\"] = metadata\n",
    "rawdict[\"us_cite\"] = citations\n",
    "rawdict[\"text\"] = syllabus\n",
    "rawdict[\"url\"] = urls\n",
    "dfclean = pd.DataFrame(rawdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ---------- clean up full_cite column -----------\n",
    "years,names, = [],[]\n",
    "for x in range(0,len(dfclean)):\n",
    "    years.append(re.findall(\"\\s\\((.*)\",dfclean.full_cite[x])[0][:-1])\n",
    "    names.append(re.findall(\".+?(?=\\s\\d)\",dfclean.full_cite[x])[0])\n",
    "    \n",
    "dfclean[\"year\"] = years\n",
    "dfclean[\"case\"] = names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfclean.to_csv(\"final_justia_data_merge.csv\", sep=',', encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cases scaped from Justia: 11224\n"
     ]
    }
   ],
   "source": [
    "print \"Number of cases scaped from Justia:\", len(dfclean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
