{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation\n",
    "\n",
    "Latent Dirichlet Allocation (LDA) represents documents as combinations of topics for which each word has a certain probability. The statistics underlying this approach to text analysis assumes a generative probabilistic model for any set of discrete data (in this case, text documents). In other words, LDA relies on the assumption that each topic may be modeled as an \"infinite mixture over an underlying set of topic probabilities\" [1]. The number of words in a document follows a certain distribution and the topic composition of the document follows a Dirichlet distribution over a fixed set of topics. \n",
    "<br>\n",
    "<img src=\"http://deliveryimages.acm.org/10.1145/1860000/1859210/figs/uf1.jpg\" align=\"center\" height=\"500\" width=\"500\">\n",
    "<br>\n",
    "The purpose of LDA is to determine a set of topics that are likely to have generated the documents from the words in the documents themselves. [2] One shortcoming of the LDA model is it uses the \"bag of words\" assumption that the order of words in a document may be ignored and that, therefore, documents are more or less exchangable. Non-probabilistic instantiations of LDA mainly use word counts as features.\n",
    "\n",
    "<br>\n",
    "[1] D. M. Blei, A. Y. Ng, M. I. Jordan, 2003. \"Latent Dirichlet Allocation.\" *Journal of Machine Learning Research*. pp. 993-1022. [Online](https://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf).<br>\n",
    "[2] E. Chen, 2013. \"Introduction to Latent Dirichlet Allocation.\" [Online](http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/). <br>\n",
    "[3] G. Anthes, 2010. \"Topic Models vs. Unstructured Data.\" *Communications of the ACM* 53(12): p. 16-18. DOI:\n",
    "10.1145/1859204.1859210. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "pd.set_option(\"display.width\", 500)\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.notebook_repr_html\", True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and Clean Data\n",
    "\n",
    "Using the methods defined in the Data Cleaning notebook, we can transform the text into training and test corpuses that are segmented by word type (noun, adjective, verb, foreign, and precedent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_df = pd.read_csv(\"sample_cases.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_cite</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "      <th>us_cite</th>\n",
       "      <th>year</th>\n",
       "      <th>case</th>\n",
       "      <th>case_id</th>\n",
       "      <th>caseId</th>\n",
       "      <th>caseOriginState</th>\n",
       "      <th>dateArgument</th>\n",
       "      <th>decisionDirection</th>\n",
       "      <th>decisionType</th>\n",
       "      <th>docket</th>\n",
       "      <th>docketId</th>\n",
       "      <th>issueArea</th>\n",
       "      <th>jurisdiction</th>\n",
       "      <th>lawType</th>\n",
       "      <th>majOpinWriter</th>\n",
       "      <th>majVotes</th>\n",
       "      <th>minVotes</th>\n",
       "      <th>usCite</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>United States v. Jimenez Recio 537 U.S. 270 (2...</td>\n",
       "      <td>OCTOBER TERM, 2002 Syllabus UNITED STATES v. J...</td>\n",
       "      <td>https://supreme.justia.com/cases/federal/us/53...</td>\n",
       "      <td>537 U.S. 270</td>\n",
       "      <td>2003</td>\n",
       "      <td>United States v. Jimenez Recio</td>\n",
       "      <td>10400</td>\n",
       "      <td>2002-016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/12/2002</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1/1/1184</td>\n",
       "      <td>2002-016-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>537 U.S. 270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>United States v. Jones 345 U.S. 377 (1953)</td>\n",
       "      <td>United States v. Jones No. 556 Decided April 1...</td>\n",
       "      <td>https://supreme.justia.com/cases/federal/us/34...</td>\n",
       "      <td>345 U.S. 377</td>\n",
       "      <td>1953</td>\n",
       "      <td>United States v. Jones</td>\n",
       "      <td>927</td>\n",
       "      <td>1952-078</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>556</td>\n",
       "      <td>1952-078-01</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>345 U.S. 377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Joy Oil Co., Ltd. v. State Tax Commission 337 ...</td>\n",
       "      <td>Joy Oil Co., Ltd. v. State Tax Commission No. ...</td>\n",
       "      <td>https://supreme.justia.com/cases/federal/us/33...</td>\n",
       "      <td>337 U.S. 286</td>\n",
       "      <td>1949</td>\n",
       "      <td>Joy Oil Co., Ltd. v. State Tax Commission</td>\n",
       "      <td>461</td>\n",
       "      <td>1948-090</td>\n",
       "      <td>27</td>\n",
       "      <td>1/6/1949</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>223</td>\n",
       "      <td>1948-090-01</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>337 U.S. 286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Witte v. United States 515 U.S. 389 (1995)</td>\n",
       "      <td>OCTOBER TERM, 1994 Syllabus WITTE v. UNITED ST...</td>\n",
       "      <td>https://supreme.justia.com/cases/federal/us/51...</td>\n",
       "      <td>515 U.S. 389</td>\n",
       "      <td>1995</td>\n",
       "      <td>Witte v. United States</td>\n",
       "      <td>9663</td>\n",
       "      <td>1994-076</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4/17/1995</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>94-6187</td>\n",
       "      <td>1994-076-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>104</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>515 U.S. 389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Warth v. Seldin 422 U.S. 490 (1975)</td>\n",
       "      <td>Warth v. Seldin No. 73-2024 Argued March 17, 1...</td>\n",
       "      <td>https://supreme.justia.com/cases/federal/us/42...</td>\n",
       "      <td>422 U.S. 490</td>\n",
       "      <td>1975</td>\n",
       "      <td>Warth v. Seldin</td>\n",
       "      <td>5850</td>\n",
       "      <td>1974-140</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3/17/1975</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>73-2024</td>\n",
       "      <td>1974-140-01</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>101</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>422 U.S. 490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           full_cite                                               text                                                url       us_cite  year                                       case  case_id    caseId  caseOriginState dateArgument  decisionDirection  decisionType    docket     docketId  issueArea  jurisdiction  lawType  majOpinWriter  majVotes  minVotes        usCite\n",
       "0  United States v. Jimenez Recio 537 U.S. 270 (2...  OCTOBER TERM, 2002 Syllabus UNITED STATES v. J...  https://supreme.justia.com/cases/federal/us/53...  537 U.S. 270  2003             United States v. Jimenez Recio    10400  2002-016              NaN   11/12/2002                  1             1  1/1/1184  2002-016-01          1             1      NaN            110         8         1  537 U.S. 270\n",
       "1         United States v. Jones 345 U.S. 377 (1953)  United States v. Jones No. 556 Decided April 1...  https://supreme.justia.com/cases/federal/us/34...  345 U.S. 377  1953                     United States v. Jones      927  1952-078               12          NaN                  1             2       556  1952-078-01          9             2        6            NaN         9         0  345 U.S. 377\n",
       "2  Joy Oil Co., Ltd. v. State Tax Commission 337 ...  Joy Oil Co., Ltd. v. State Tax Commission No. ...  https://supreme.justia.com/cases/federal/us/33...  337 U.S. 286  1949  Joy Oil Co., Ltd. v. State Tax Commission      461  1948-090               27     1/6/1949                  2             1       223  1948-090-01          8             1        1             80         6         3  337 U.S. 286\n",
       "3         Witte v. United States 515 U.S. 389 (1995)  OCTOBER TERM, 1994 Syllabus WITTE v. UNITED ST...  https://supreme.justia.com/cases/federal/us/51...  515 U.S. 389  1995                     Witte v. United States     9663  1994-076              NaN    4/17/1995                  1             1   94-6187  1994-076-01          1             1        2            104         8         1  515 U.S. 389\n",
       "4                Warth v. Seldin 422 U.S. 490 (1975)  Warth v. Seldin No. 73-2024 Argued March 17, 1...  https://supreme.justia.com/cases/federal/us/42...  422 U.S. 490  1975                            Warth v. Seldin     5850  1974-140              NaN    3/17/1975                  1             1   73-2024  1974-140-01          9             1      NaN            101         5         4  422 U.S. 490"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training, test data split \n",
    "trainingcoln = pd.read_csv('traintestarray.csv',sep=',',header=None).values.ravel()\n",
    "sample_df['training'] = trainingcoln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cleaning functions can also be found in the Data Cleaning notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re \n",
    "regex1 = r\"\\(.\\)\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pattern.en import parse\n",
    "from pattern.en import pprint\n",
    "from pattern.en import conjugate, lemma, lexeme\n",
    "from pattern.vector import stem, PORTER, LEMMA\n",
    "from sklearn.feature_extraction import text\n",
    "import string\n",
    "\n",
    "#stopwords and punctuation\n",
    "stopwords=text.ENGLISH_STOP_WORDS\n",
    "punctuation = list('.,;:!?()[]{}`''\\\"@#$^&*+-|=~_')\n",
    "\n",
    "def get_parts(opinion):\n",
    "    oplow = opinion.lower()\n",
    "    #REMOVING CHARACTERS: we have ugly text, and remove unnecssary characters.\n",
    "    oplow = unicode(oplow, 'ascii', 'ignore') #remove non-unicode characters \n",
    "    oplow = str(oplow).translate(string.maketrans(\"\\n\\t\\r\", \"   \")) #remove characters like \\n \n",
    "    #justices (eg, Justice Breyer) are referred to as J. (eg,Breyer, J.); we remove the J., also JJ. for plural\n",
    "    oplow = oplow.replace('j.','')\n",
    "    oplow = oplow.replace('jj.','')\n",
    "    oplow = oplow.replace('c.','') #remove C. for chief justice \n",
    "    oplow = oplow.replace('pp.','') #page numbers\n",
    "    oplow = oplow.replace('  ','') #multiple spaces\n",
    "    oplow = ''.join([i for i in oplow if not i.isdigit()]) #remove digits \n",
    "    oplow=re.sub(regex1, ' ', oplow)\n",
    "    #Remove the Justia disclaimer at the end of the case, if it appears in the string\n",
    "    justiadisclaimer = \"disclaimer: official\"\n",
    "    if justiadisclaimer in oplow: \n",
    "        optouse = oplow.split(justiadisclaimer)[0]\n",
    "    else:\n",
    "        optouse = oplow\n",
    "    \n",
    "    #GET A LIST OF PRECEDENTS CITED IN THE OPINION \n",
    "    wordslist = optouse.split()\n",
    "    #find precedents based on string 'v.' (eg, 'Brown v. Board')\n",
    "    indices = [i for i in range(len(wordslist)) if wordslist[i]=='v.']\n",
    "    precedents = [wordslist[i-1]+ ' ' + wordslist[i]+ ' ' + wordslist[i+1] for i in indices]\n",
    "    \n",
    "    #remove precedents, as we have already accounted for these\n",
    "    for precedent in precedents:\n",
    "        optouse = optouse.replace(precedent,'')\n",
    "    \n",
    "    #PARSE INTO LIST OF LISTS --> GET WORDS\n",
    "    parsed = parse(optouse,tokenize=True,chunks=False,lemmata=True).split()\n",
    "    verbs = [] \n",
    "    nouns = [] \n",
    "    adjectives = [] \n",
    "    foreign = [] \n",
    "    i=0\n",
    "    #Create lists of lists of verbs, nouns, adjectives and foreign words in each sentence.\n",
    "    for sentence in parsed: #for each sentence \n",
    "        verbs.append([])\n",
    "        nouns.append([])\n",
    "        adjectives.append([])\n",
    "        foreign.append([])\n",
    "        for token in sentence: #for each word in the sentence \n",
    "            if token[0] in punctuation or token[0] in stopwords or len(token[0])<=2:\n",
    "                continue\n",
    "            wordtouse = token[0]\n",
    "            for x in punctuation:\n",
    "                wordtouse = wordtouse.replace(x,' ') #if punctuation in word, take it out\n",
    "            if token[1] in ['VB','VBZ','VBP','VBD','VBN','VBG']:\n",
    "                verbs[i].append(lemma(wordtouse)) #append the lemmatized verb (we relemmatize because lemmata in parse does not seem to always work)\n",
    "            if token[1] in ['NN','NNS','NNP','NNPS']:\n",
    "                nouns[i].append(lemma(wordtouse))\n",
    "            if token[1] in ['JJ','JJR','JJS']:\n",
    "                adjectives.append(lemma(wordtouse))\n",
    "            if token[1] in ['FW']:\n",
    "                foreign.append(wordtouse)  \n",
    "        i+=1  \n",
    "    #Zip together lists so each tuple is a sentence. \n",
    "    out=zip(verbs,nouns,adjectives,foreign)\n",
    "    verbs2 = []\n",
    "    nouns2 = []\n",
    "    adjectives2 = []\n",
    "    foreign2 = []\n",
    "    for sentence in out: \n",
    "        if sentence[0]!=[] and sentence[1]!=0: #if the sentence has at least one verb and noun, keep it. Otherwise, drop it.\n",
    "            if type(sentence[0])==list: \n",
    "                verbs2.append(sentence[0])\n",
    "            else: \n",
    "                verbs2.append([sentence[0]]) #if verb is a string rather than a list, put string in list\n",
    "            if type(sentence[1])==list:\n",
    "                nouns2.append(sentence[1])\n",
    "            else:\n",
    "                nouns2.append([sentence[1]])\n",
    "            if type(sentence[2])==list:\n",
    "                adjectives2.append(sentence[2])\n",
    "            else:\n",
    "                adjectives2.append([sentence[2]])\n",
    "            if type(sentence[3])==list:\n",
    "                foreign2.append(sentence[3])\n",
    "            else:\n",
    "                foreign2.append([sentence[3]])\n",
    "    return(verbs2,nouns2,adjectives2,foreign2,precedents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary Creation\n",
    "\n",
    "Now that we have cleaned our text corpus, we want to create vocabularies segmented by word type. We use the **get_parts** function defined above to return all verbs, nouns, adjectives, foreign words, and precedents. This word list will be useful for creating matrices of word frequencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 39s, sys: 316 ms, total: 1min 39s\n",
      "Wall time: 1min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "verbwords = []\n",
    "nounwords = []\n",
    "adjwords = []\n",
    "forwords = []\n",
    "precedents_all = []\n",
    "for op in sample_df.text:\n",
    "    verbs,nouns,adjectives,foreign,precedents = get_parts(op)\n",
    "    verbwords.append(verbs)\n",
    "    nounwords.append(nouns)\n",
    "    adjwords.append(adjectives)\n",
    "    forwords.append(foreign)\n",
    "    precedents_all.append(precedents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "issue_areas = sample_df.issueArea.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create precedents vocab\n",
    "precedents_vocab = list(set([precedent for sublist in precedents_all for precedent in sublist]))\n",
    "#create other vocabs\n",
    "verbvocab = list(set([word for sublist in verbwords for subsublist in sublist for word in subsublist]))\n",
    "nounvocab = list(set([word for sublist in nounwords for subsublist in sublist for word in subsublist]))\n",
    "adjvocab = list(set([word for sublist in adjwords for subsublist in sublist for word in subsublist]))\n",
    "forvocab = list(set([word for sublist in forwords for subsublist in sublist for word in subsublist]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dictionaries: id --> word\n",
    "id2prec = dict(enumerate(precedents_vocab))\n",
    "id2verb = dict(enumerate(verbvocab))\n",
    "id2noun = dict(enumerate(nounvocab))\n",
    "id2adj = dict(enumerate(adjvocab))\n",
    "id2for = dict(enumerate(forvocab))\n",
    "#dictionaries: word --> id\n",
    "prec2id = dict(zip(id2prec.values(),id2prec.keys()))\n",
    "verb2id = dict(zip(id2verb.values(),id2verb.keys()))\n",
    "noun2id = dict(zip(id2noun.values(),id2noun.keys()))\n",
    "adj2id = dict(zip(id2adj.values(),id2adj.keys()))\n",
    "for2id = dict(zip(id2for.values(),id2for.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this function takes a list of words, and outputs a list of tuples \n",
    "counter = lambda x:list(set([(i,x.count(i)) for i in x]))\n",
    "\n",
    "#corpus_creator takes a list of lists of lists like verbwords, or a list of lists like precedents_all. \n",
    "#It also takes a word2id dictionary.\n",
    "def corpus_creator(sentence_word_list,word2id):\n",
    "    counter = lambda x:list(set([(word2id[i],x.count(i)) for i in x]))\n",
    "    op_word_list = []\n",
    "    if type(sentence_word_list[0][0])==list: #if list of lists of lists \n",
    "        for opinion in sentence_word_list: \n",
    "            #for each list (which corresponds to an opinion) in sentence_word_list, get a list of the words\n",
    "            op_word_list.append([word for sublist in opinion for word in sublist])\n",
    "    else: #if list of lists \n",
    "        op_word_list = sentence_word_list\n",
    "    corpus = []\n",
    "    for element in op_word_list: \n",
    "        corpus.append(counter(element))\n",
    "    return(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Pass (Untransformed)\n",
    "\n",
    "To adjust the parameters of our mode, we will use the corpus of nouns to check for model accuracy since nouns are most strongly indicative of topics. The first pass at LDA will use the untransformed word matrices created above. We will adjust the number of topic categories in order to finetune the model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = corpus_creator(nounwords,noun2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.23 s, sys: 39.7 ms, total: 4.27 s\n",
      "Wall time: 4.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# model with noun corpus, 5 topics\n",
    "lda1a = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2noun, num_topics=5, update_every=1, chunksize=200, passes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'0.033*court + 0.014*petitioner + 0.013*state + 0.013*sentence + 0.011*respondent + 0.009*opinion + 0.009*amendment + 0.009*evidence + 0.008*case + 0.007*offense',\n",
       " u'0.040*court + 0.017*opinion + 0.017*state + 0.015*trial + 0.014*rule + 0.013*petitioner + 0.013*case + 0.012*respondent + 0.011*post + 0.010*evidence',\n",
       " u'0.063*court + 0.036*state + 0.021*district + 0.013*act + 0.012*appeal + 0.011*petitioner + 0.011*law + 0.010*case + 0.010*respondent + 0.009*claim',\n",
       " u'0.018*court + 0.014*act + 0.012*state + 0.009*opinion + 0.008*district + 0.008*agency + 0.008*congres + 0.007*regulation + 0.007*government + 0.007*provision',\n",
       " u'0.025*act + 0.020*court + 0.014*employee + 0.012*union + 0.011*contract + 0.010*respondent + 0.009*employer + 0.009*labor + 0.009*petitioner + 0.008*board']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda1a.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.28 s, sys: 77.6 ms, total: 4.35 s\n",
      "Wall time: 4.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# model with noun corpus, 10 topics\n",
    "lda1b = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2noun, num_topics=10, update_every=1, chunksize=200, passes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'0.024*child + 0.018*bankruptcy + 0.017*court + 0.013*debtor + 0.012*performance + 0.010*children + 0.010*parent + 0.009*opinion + 0.008*petitioner + 0.008*state',\n",
       " u'0.063*court + 0.031*state + 0.025*district + 0.015*act + 0.012*opinion + 0.012*appeal + 0.011*respondent + 0.010*law + 0.010*case + 0.010*action',\n",
       " u'0.048*court + 0.041*state + 0.021*counsel + 0.017*petitioner + 0.015*claim + 0.015*rule + 0.015*jurisdiction + 0.014*habea + 0.014*trial + 0.013*tribe',\n",
       " u'0.032*state + 0.030*tax + 0.021*court + 0.014*property + 0.012*regulation + 0.010*congres + 0.010*government + 0.010*law + 0.010*clause + 0.009*bank',\n",
       " u'0.023*court + 0.018*amendment + 0.017*respondent + 0.014*officer + 0.011*search + 0.011*speech + 0.010*government + 0.010*opinion + 0.010*information + 0.009*police',\n",
       " u'0.018*water + 0.017*act + 0.016*patent + 0.016*u s  + 0.015*court + 0.012*decree + 0.011*land + 0.010*merger + 0.009*guideline + 0.009*rate',\n",
       " u'0.030*employee + 0.029*union + 0.023*board + 0.021*employer + 0.020*labor + 0.018*court + 0.016*benefit + 0.013*employment + 0.012*act + 0.012*plan',\n",
       " u'0.027*act + 0.026*court + 0.022*commission + 0.015*service + 0.013*petitioner + 0.013*carrier + 0.013*commerce + 0.011*respondent + 0.011*railroad + 0.010*district',\n",
       " u'0.038*court + 0.026*trial + 0.025*jury + 0.023*petitioner + 0.023*evidence + 0.016*sentence + 0.014*opinion + 0.013*respondent + 0.012*conviction + 0.012*offense',\n",
       " u'0.026*court + 0.019*act + 0.017*contract + 0.016*attorney + 0.013*cause + 0.012*general + 0.012*government + 0.011*action + 0.011*statute + 0.010*respondent']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda1b.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Pass (td-idf)\n",
    "\n",
    "We can see from the words and corresponding coefficients above that there are some frequent words that occur in each topic (i.e. court, case, action, opinion, respondent). In order to refine the words that are generated by each topic, we can use term frequencyâ€“inverse document frequency (td-idf) to weight the corpus of words by the number of times a word appears in the document. Each document in the corpus is reduced to a fixed-length list of numbers, roughly corresponding to the frequency of appearance for a basic set of words within that document. This should help adjust for the fact that some words appear across all documents at a consistently high frequency. <br>\n",
    "\n",
    "In order to implement this, we need several helper functions that turn our corpus of words (represented as a list of list of tuples) into a matrix (weighted frequencies) and then back into a corpus that can be inputted into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# takes a corpus and a number of words, and returns a matrix in which the element at row i and column j is the number of\n",
    "# occurrences of word j in document i.\n",
    "def corpus_to_mat(corpus, num_words):\n",
    "    n = len(corpus)\n",
    "    M = np.zeros((n, num_words))\n",
    "    for i,doc in enumerate(corpus):\n",
    "        for word,count in doc:\n",
    "            M[i][word] = count\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.29 s, sys: 48.8 ms, total: 2.34 s\n",
      "Wall time: 2.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#get noun corpus\n",
    "nouncorpus = corpus_creator(nounwords,noun2id)\n",
    "noun_train_corpus = [nouncorpus[i] for i in range(len(nouncorpus)) if sample_df['training'][i]==1]\n",
    "noun_test_corpus = [nouncorpus[i] for i in range(len(nouncorpus)) if sample_df['training'][i]==0]\n",
    "\n",
    "#get verb corpus\n",
    "verbcorpus = corpus_creator(verbwords,verb2id)\n",
    "verb_train_corpus = [verbcorpus[i] for i in range(len(verbcorpus)) if sample_df['training'][i]==1]\n",
    "verb_test_corpus = [verbcorpus[i] for i in range(len(verbcorpus)) if sample_df['training'][i]==0]\n",
    "\n",
    "#get adjective corpus\n",
    "adjcorpus = corpus_creator(adjwords,adj2id)\n",
    "adj_train_corpus = [adjcorpus[i] for i in range(len(adjcorpus)) if sample_df['training'][i]==1]\n",
    "adj_test_corpus = [adjcorpus[i] for i in range(len(adjcorpus)) if sample_df['training'][i]==0]\n",
    "\n",
    "#get foreign corpus\n",
    "forcorpus = corpus_creator(forwords,for2id)\n",
    "for_train_corpus = [forcorpus[i] for i in range(len(forcorpus)) if sample_df['training'][i]==1]\n",
    "for_test_corpus = [forcorpus[i] for i in range(len(forcorpus)) if sample_df['training'][i]==0]\n",
    "\n",
    "#get precedents corpus\n",
    "preccorpus = corpus_creator(precedents_all,prec2id)\n",
    "prec_train_corpus = [preccorpus[i] for i in range(len(preccorpus)) if sample_df['training'][i]==1]\n",
    "prec_test_corpus = [preccorpus[i] for i in range(len(preccorpus)) if sample_df['training'][i]==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "#this function takes a training matrix of size n_documents_training*vocab_size and a test matrix\n",
    "#of size n_documents_test*vocab_size. The function outputs the corresponding tfidf matrices.\n",
    "#Note that we fit on the training data, and then apply that fit to the test data.\n",
    "def tfidf_mat_creator(trainmatrix,testmatrix):\n",
    "    tf_idf_transformer=TfidfTransformer()\n",
    "    tfidf_fit = tf_idf_transformer.fit(trainmatrix)\n",
    "    tfidf_train = tfidf_fit.transform(trainmatrix).toarray()\n",
    "    tfidf_test = tfidf_fit.transform(testmatrix).toarray()\n",
    "    return(tfidf_train,tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "noun_tfidf_mat_train,noun_tfidf_mat_test = tfidf_mat_creator(corpus_to_mat(noun_train_corpus, len(nounvocab)),\n",
    "                                   corpus_to_mat(noun_test_corpus, len(nounvocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# takes a tfidf matrix and returns the corresponding matrix\n",
    "def tfidf_to_corpus(tfidf_mat): #takes as input: matrix of size n_documents*vocabulary size\n",
    "    tfidfcorpus = []\n",
    "    i=0 #keep track of document you are on\n",
    "    for doc in tfidf_mat: #for each case\n",
    "        tfidfcorpus.append([])\n",
    "        j=0\n",
    "        for word in doc: #for each word in the vocabulary, append tuple (wordid,num_times_word_used)\n",
    "            tfidfcorpus[i].append((j,tfidf_mat[i][j])) \n",
    "            j+=1\n",
    "        i+=1\n",
    "    return(tfidfcorpus) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_noun_corpus = tfidf_to_corpus(noun_tfidf_mat_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 4s, sys: 6.17 s, total: 2min 10s\n",
      "Wall time: 2min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# model with noun corpus (tf-idf weighted), 5 topics\n",
    "lda2a = gensim.models.ldamodel.LdaModel(corpus=tfidf_noun_corpus, id2word=id2noun, num_topics=5, update_every=1, chunksize=200, passes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'0.001*election + 0.001*tort + 0.001*house + 0.001*applicant + 0.001*religion + 0.001*deportation + 0.001*appellant + 0.001*court + 0.001*negligence + 0.001*vote',\n",
       " u'0.001*solicitation + 0.001*contempt + 0.001*copyright + 0.001*racketeer + 0.000*teacher + 0.000*school + 0.000*desegregation + 0.000*rent + 0.000*rico + 0.000*court',\n",
       " u'0.009*court + 0.006*state + 0.005*act + 0.004*respondent + 0.004*petitioner + 0.003*district + 0.003*case + 0.003*tax + 0.003*u s  + 0.003*opinion',\n",
       " u'0.000*deposit + 0.000*licensee + 0.000*tribe + 0.000*allotment + 0.000*apportionment + 0.000*land + 0.000*injury therefore + 0.000*court + 0.000*coleman + 0.000*secretary',\n",
       " u'0.001*veteran + 0.000*liquor + 0.000*dna + 0.000*sate + 0.000*tort + 0.000*hospital + 0.000*wholesaler + 0.000*gene + 0.000*fault + 0.000*schedule']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda2a.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 57s, sys: 4.75 s, total: 2min 2s\n",
      "Wall time: 2min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# model with noun corpus (tf-idf weighted), 10 topics\n",
    "lda2b = gensim.models.ldamodel.LdaModel(corpus=tfidf_noun_corpus, id2word=id2noun, num_topics=10, update_every=1, chunksize=200, passes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'0.006*contractor + 0.003*veteran + 0.003*tribe + 0.003*franchise + 0.002*nonresident + 0.001*marijuana + 0.001*division + 0.001*naturalization + 0.001*gift + 0.001*prima',\n",
       " u'0.012*court + 0.009*state + 0.008*act + 0.005*petitioner + 0.005*respondent + 0.005*statute + 0.004*case + 0.004*government + 0.004*district + 0.004*u s ',\n",
       " u'0.002*narcotic + 0.002*agriculture + 0.001*surveillance + 0.001*malice + 0.001*bro + 0.001*airport + 0.001*badge + 0.001*rent + 0.001*store + 0.000*contact',\n",
       " u'0.005*court + 0.004*ordinance + 0.003*amendment + 0.003*school + 0.003*opinion + 0.003*city + 0.003*market + 0.003* page + 0.003*child + 0.003*district',\n",
       " u'0.005*sentence + 0.005*trial + 0.004*court + 0.004*confession + 0.003*prison + 0.003*conviction + 0.003*death + 0.003*habea + 0.003*counsel + 0.003*capital',\n",
       " u'0.016*tax + 0.006*property + 0.004*revenue + 0.003*income + 0.003*peace + 0.003*taxpayer + 0.002*bankruptcy + 0.002*code + 0.002*ir + 0.002*railway',\n",
       " u'0.002*solicitation + 0.002*liquor + 0.001*price fix + 0.001*export + 0.001*suspension + 0.001*retailer + 0.001*production + 0.001*price + 0.001*retail + 0.001*hospital',\n",
       " u'0.002*arbitrator + 0.002*applicant + 0.002*defender + 0.002*disorder + 0.002*decedent + 0.002*due + 0.002*credit + 0.002*grievance + 0.002*receipt + 0.002*race',\n",
       " u'0.004*search + 0.002*warrant + 0.002*apartment + 0.002*arrest + 0.001*price + 0.001*convict + 0.001*seller + 0.001*hall + 0.001*conversation + 0.001*robinson patman',\n",
       " u'0.006*lien + 0.002*racketeer + 0.002*expenditure + 0.002*president + 0.001*recovery + 0.001*hearing + 0.001*detention + 0.001*tariff + 0.001*summons + 0.001*continuance']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda2b.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 0.64807654967956374), (2, 0.062449968248195029), (3, 0.23414863256490431), (4, 0.04324581438729512)]\n",
      "chertoff emphasi shannon time center lopez meza commission ginsburg public law driver commission both evil colby misbehavior term trial respondent resemblance precedent individual al  tuskey solicitor cruz seizure act attorney thornton purpose agreement convict sting probability court truck crime language jury feldman certiorari fact change evidence november car judge state cause acquaintance john operation general view dreeben alan breyer entrapment path reason criminality scalia circuit use jay goal help shurtliff enforcement opinion danger justice origin recio appeal january contact force objective police kennedy drug jimenez liability souter conspiracy government brief c ombination f d thoma majority conviction defeat steven threat conspirator rule object example olson case marcu text post\n",
      "==========================================\n",
      "[(0, 0.014573436036213812), (1, 0.63551267022168179), (3, 0.12648805317715925), (4, 0.15018690983595701), (9, 0.0698604514424358)]\n",
      "circumstance corpu judgment venue case verdict after appeal person custody ground indictment amendment duty publicity death hear order new subject accord conviction supreme police media habea process gibson murder writ panel opinion counsel attempt thepage state jury clause testimony locality evansville trial official failure inhabitant indiana release face change statute indignation court f d syllabuspetitioner press detention petitioner held petitioner november county crime proceeding juror vicinity sentence prosecutor district cause excitement guilt time record u s \n",
      "==========================================\n",
      "[(0, 0.23940476851629167), (1, 0.65127110756718865), (3, 0.056108432704093124), (5, 0.050844020565149514)]\n",
      "appeal respondent gaa summary period repurchase way congres chippewa ownership public parcel suit term land valorem proposition act year principle cas minnesota reference auction title government syllabu order nonindian intent forego purpose goudy settler tribe alienability judgment secretary certiorari nation circuitno interior court sale circuit allottee fee yakima hold allotment portion trust authority policy homestead effect indian consequence district argument leech mter state tax restriction be syllabu practice reservation lake century end cause jurisdiction exempt stand pine band penalty petitioner taxation proviso disposition simple protest county non indian patent decision case taxability alienable protection\n",
      "==========================================\n",
      "[(1, 0.51126652472130296), (3, 0.4829779761392215)]\n",
      "leave appeal employer case p member neutrality busines sex policy men vii burden action  steven exclusion reason pay way job court petitioner role  hence stewart theory post powell work statu pretext respondent act position title male disability district condition female analysi burger effect opinion proof employee rehnquist marshall respect december absent right necessity compensation brennan pretex t employment f d syllabuspetitioner show adoption judgment decision seniority women result violation discrimination case find opportunity absence while return blackmun benefit u s  pregnancy\n",
      "==========================================\n",
      "[(1, 0.63332905078989132), (3, 0.21862995817990913), (4, 0.036063636074373788), (5, 0.10545531408122329)]\n",
      "section conversion plan identity basi payment event settlement point period reason determination ownership order sale owner stewart exception question nonrecognition shareholder exchange opinion gain corporation revenue exces powell fix situation month taxpayer insurance code place insurer property rehnquist vendor burger adoption los dougla f d income transformation tax court obligation action liquidation proceed post blackmun  page brennan march incident marshall\n",
      "==========================================\n",
      "[(1, 0.77081799392906114), (2, 0.10877552297219849), (3, 0.095405614883680948)]\n",
      "decision consideration judgment legislature writ litigation syllabusmotion justification petition proceeding court mandamu reapportionment leave district directive year plan u s \n",
      "==========================================\n",
      "[(1, 0.80629697601531558), (3, 0.15041848306091934), (4, 0.035249465899860895)]\n",
      "petitioner relief souter page time u s a basi adea review proceed commission opinion appeal remedy turn joined steven position year question untimelines limitation victim timelines expiration intent october claim steven period nevertheles undera eeoc act personnel secretary matter prejudice period alternative issue action complaint contradiction state blackmun merit kennedy notification day regulations on scalia employee age opportunity office treasury discrimination intention need notice syllabuson avenue court dismissal department agency compliance statute u s  marshall practice grind event a requirement employer congres government attempt revenue route case jurisdiction procedure april district employment september toa exhaustion date incorrectly p suit march thepage\n",
      "==========================================\n",
      "[(0, 0.022482023179217655), (1, 0.6360848174724556), (3, 0.16420147367844098), (4, 0.16569171448346035)]\n",
      "appeal judgment consideration search court review confession property officer rule steal use district prisoner position procedure error writ proceeding granted upon mi suppression f d seizure legality government return case record u s \n",
      "==========================================\n",
      "[(1, 0.48178168764365581), (3, 0.16248425989696133), (4, 0.04998235270333528), (5, 0.24540192694457155), (7, 0.043681106115281046)]\n",
      "respondent clause revenue proceed portion bank policy syllabusin purpose wife code service life right decedent held a insurance tax january estate detroit amendment process march premium\n",
      "==========================================\n",
      "[(1, 0.61465770946150289), (3, 0.11503886637324216), (4, 0.25305259083000264), (5, 0.011589899902023039)]\n",
      "appeal joined burger marshall white relationship clause basi constitution case and claim the privacy nation rationale sodomy none conduct post authority case reversed page majority defendant reach state act family resemblance male law procreation district belief sodomy hardwick fact background steven liberty country respondent suit home failure opinion  steven rehnquist burger judiciary constitutionality bear right f d proposition history resistance bedroom motion laws p marriage statute syllabusafter brennan concept tradition court proscription rights otherwise homosexual powell process adult kind march u s \n",
      "==========================================\n",
      "[(1, 0.97199197825031436), (4, 0.01095707668749454), (5, 0.010909553698638533)]\n",
      "time abuse rule determination extension treat appeals p application appeal p u s  syllabusin defendant discretion statute motion appeal direction decision action delay reason case claim district finality procedure jurisdiction entry f d mean relief court requirement dismissal law judgment lack inpage claim p operation exercise\n",
      "==========================================\n",
      "[(1, 0.75645686330269912), (3, 0.20267134390240438), (7, 0.032734906696142617)]\n",
      "statement contrast decision presence difference columbu court respondent circuit price law indication plaintiff respondent respect suit exercise directive reference transportation presumption trade entity context route formulation term exclusion power action subdivision  petrey purpose safety absence bar level inspection april locality area agency authority city authorization subdivision language regulation unit truck household operation motor municipality summary preemption state id  owner prescription exception provision rule district design grow phrase legislature enforcement vehicle service consent preempt tow truck judgment mortier parallel force deregulatory read property component combination market resort odd grind operator petitioner plaintiff section limit link statute good congres tow structure russello word carrier case association comprehension\n",
      "==========================================\n",
      "[(1, 0.8314104148327901), (4, 0.076232019077691338), (7, 0.073935358033802218)]\n",
      "employer employee respondent syllabusrespondent state grind claim held under policy act so d lmra  judgment contract severance management lmra grievance suit type union procedure relation labor court air agreement bargain january november pay law u s \n",
      "==========================================\n",
      "[(1, 0.64274440885922979), (3, 0.059590662697382085), (5, 0.28535231885170337)]\n",
      "appeal time code iii light provision circuit payment debt fact march brief income pendency day term liability january bankruptcy priority solicitor attorney plan discharge section limitation thomassawyer corneliu failure certiorari elimination suzanne circuitno opportunity reorganization state cause claim case general recovery stay principle draft deputy lookback plaintiff scalia court policy petition i year district opinion text date justice period repose return step clark chapter petition tax component grenville millett faith petitioner defendant congres revenue certainty right taxpayer intent ir service olson f d file debtor toll wallace ellisen\n",
      "==========================================\n",
      "[(1, 0.3151513129542573), (3, 0.67423924175030536)]\n",
      "appeal time word kickback favor verb order convenience petitioner statessyllabusfreeman court  supreme give  circuit settlement fee settlement service services and unpersuasive section interpretation term usage provider relationship u s  reporter lumber accept  surplusage percentage section couple claim tense action subsection set  petitioner character surplusage portion conclusion feed circuitno respa render lawbreaker portion respondent headnote connation plaintiff person field canon use socii summary mortgage procedure district violation entirety argument opinion connection judgment retention result service stage partnership practice activity consumer seek split purposeto loan charge incertiorari prohibition pro end gen case mean timber commonsense hibition content thing state court warrant transaction f d reader decision estate text syllabu percentdoe\n",
      "==========================================\n",
      "[(1, 0.8595342573782403), (3, 0.1202075093342124), (5, 0.016668247753105145)]\n",
      "appeal injury test hand title question suit statement circuit turn al  backpay ambiguity protection remain existence exercise right power authority province article court employee action congres damage subject opinion swath abrogation language board respondent state basi accordance certiorari use requirement grant interpretation section immunity conduct money petitioner intent inquiry regent claim case individual text panel procedure subsection violation foreclose phrase distinction motion enforcement adea constitutionality sequence january age agency act proportionality congruence provision employer remedy determination jurisdiction clause employment end branch commerce mean classification rationality discrimination intention district legislation statute amendment government\n",
      "==========================================\n",
      "[(1, 0.79485270246935957), (3, 0.15389458585463259), (4, 0.01703685827593612), (5, 0.027621626730754696)]\n",
      "total appeal multi member litigation dougla joined brennan negroe variation maximum iii light election rehnquist page mexican american member part f su clause cf district use court plan syllabusin effect reapportionment equality statute opinion discrimination dalla state wide violation population jurisdiction protection scheme county deviation standard burger blackmun bexar order house history judgment disestablishment case injunction post state group stewart ideal marshall\n",
      "==========================================\n",
      "[(1, 0.51441105366650131), (3, 0.13307618693030573), (5, 0.32837458311784529)]\n",
      "fund gas indian lease property beneficiary reservation oil navajo county class utah syllabusa right compensation extension change statute november u s  resident royalty\n",
      "==========================================\n",
      "[(1, 0.66717506519244529), (4, 0.29472913847600513)]\n",
      "f d judgment case court motion evidence action january syllabusin employer brakeman liability verdict act railroad death negligence district\n",
      "==========================================\n",
      "[(1, 0.88483382045969095), (3, 0.075165844755151701)]\n",
      "court member petitioner appeal evidence case march alien act security syllabuson conclusion party district record u s  april\n",
      "==========================================\n"
     ]
    }
   ],
   "source": [
    "for bow in corpus[0:100:5]:\n",
    "    print lda2a.get_document_topics(bow)\n",
    "    print \" \".join([id2noun[e[0]] for e in bow])\n",
    "    print \"==========================================\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we can see that the tf-idf improved the performance of the model insofar as diluting the weights of words like \"court\" and \"action,\" the model still classifies most cases as belonging to Topic 1, which is predicted by the all-purposes legal terms \"court,\" \"state,\" \"act,\" \"petitioner,\" \"respondent,\" \"statute,\" \"case,\" \"government,\" while  \"district,\" and \"u s.\" So, although it is possible to glean what sort of topics might be represented by the other categories (i.e. Topic 9 contains predictor words such as \"search warrant\" and \"arrest\" and therefore might be mapped to the issue area of privacy, while Topic 5 contains words like \"tax,\" \"property,\" and \"revenue\" and therefore might be mapped to the issue area of taxation), the model is biased towards assigning cases to the general topic of Topic 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third Pass (LSI)\n",
    "\n",
    "Another model we might try before moving on to more supervised approaches (that might be more performant because they do not function under the assumption that there are no interactions among words). Latent Semantic Indexing (LSI) implements fast truncated SVD (Singular Value Decomposition), and performs well for text corpora much larger than RAM since only constant memory is needed. Since our text corpus is not terribly large, it's likely that LSI models will not output substantially better topic cluster than LDA, but it is worth trying this implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lsi1a = gensim.models.lsimodel.LsiModel(corpus=tfidf_noun_corpus, id2word=id2noun, num_topics=5, chunksize=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'0.398*\"court\" + 0.245*\"state\" + 0.194*\"petitioner\" + 0.163*\"act\" + 0.161*\"respondent\" + 0.146*\"district\" + 0.129*\"appeal\" + 0.127*\"case\" + 0.125*\"opinion\" + 0.122*\"trial\"',\n",
       " u'-0.333*\"union\" + -0.323*\"employee\" + -0.311*\"labor\" + -0.241*\"employer\" + -0.233*\"board\" + 0.206*\"trial\" + -0.203*\"act\" + -0.178*\"bargain\" + 0.143*\"jury\" + -0.142*\"relation\"',\n",
       " u'0.613*\"tax\" + -0.184*\"trial\" + 0.159*\"state\" + -0.157*\"union\" + -0.148*\"jury\" + -0.146*\"labor\" + 0.145*\"property\" + -0.134*\"petitioner\" + -0.133*\"employee\" + 0.128*\"income\"',\n",
       " u'-0.487*\"tax\" + 0.243*\"school\" + -0.233*\"jury\" + -0.191*\"trial\" + -0.179*\"union\" + -0.144*\"employee\" + -0.140*\"employer\" + -0.134*\"labor\" + 0.134*\"district\" + -0.131*\"sentence\"',\n",
       " u'-0.446*\"decree\" + -0.276*\"gas\" + -0.225*\"commission\" + -0.200*\"jurisdiction\" + -0.153*\"u s \" + 0.152*\"amendment\" + -0.144*\"court\" + -0.140*\"tribe\" + 0.135*\"school\" + -0.128*\"order\"']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi1a.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsi1b = gensim.models.lsimodel.LsiModel(corpus=tfidf_noun_corpus, id2word=id2noun, num_topics=10, chunksize=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'0.398*\"court\" + 0.246*\"state\" + 0.194*\"petitioner\" + 0.163*\"act\" + 0.161*\"respondent\" + 0.147*\"district\" + 0.129*\"appeal\" + 0.127*\"case\" + 0.125*\"opinion\" + 0.122*\"trial\"',\n",
       " u'-0.328*\"union\" + -0.323*\"employee\" + -0.318*\"labor\" + -0.244*\"employer\" + -0.228*\"board\" + 0.207*\"trial\" + -0.203*\"act\" + -0.178*\"bargain\" + 0.148*\"jury\" + -0.147*\"relation\"',\n",
       " u'0.622*\"tax\" + -0.175*\"union\" + 0.167*\"state\" + -0.163*\"trial\" + -0.157*\"labor\" + 0.144*\"commerce\" + 0.142*\"property\" + -0.142*\"petitioner\" + -0.136*\"employee\" + -0.131*\"jury\"',\n",
       " u'-0.502*\"tax\" + -0.223*\"jury\" + -0.212*\"trial\" + -0.161*\"labor\" + 0.156*\"school\" + -0.155*\"union\" + -0.142*\"employer\" + -0.140*\"employee\" + -0.136*\"petitioner\" + 0.122*\"district\"',\n",
       " u'0.309*\"school\" + 0.222*\"state\" + 0.209*\"amendment\" + -0.186*\"court\" + -0.183*\"act\" + -0.158*\"decree\" + 0.147*\"post\" + 0.137*\"opinion\" + 0.129*\"plan\" + -0.128*\"appeal\"']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi1b.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we predicted above, the LSI model did not output more discerning topics than the LDA model. Therefore, we can now move on to supervised models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
