{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Cleaning and organizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "pd.set_option(\"display.width\", 500)\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.notebook_repr_html\", True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Import data\n",
    "We import the sample dataset. This is a subsample of all the cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_df = pd.read_csv(\"sample_cases.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_cite</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "      <th>us_cite</th>\n",
       "      <th>year</th>\n",
       "      <th>case</th>\n",
       "      <th>case_id</th>\n",
       "      <th>caseId</th>\n",
       "      <th>caseOriginState</th>\n",
       "      <th>dateArgument</th>\n",
       "      <th>decisionDirection</th>\n",
       "      <th>decisionType</th>\n",
       "      <th>docket</th>\n",
       "      <th>docketId</th>\n",
       "      <th>issueArea</th>\n",
       "      <th>jurisdiction</th>\n",
       "      <th>lawType</th>\n",
       "      <th>majOpinWriter</th>\n",
       "      <th>majVotes</th>\n",
       "      <th>minVotes</th>\n",
       "      <th>usCite</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>United States v. Jimenez Recio 537 U.S. 270 (2...</td>\n",
       "      <td>OCTOBER TERM, 2002 Syllabus UNITED STATES v. J...</td>\n",
       "      <td>https://supreme.justia.com/cases/federal/us/53...</td>\n",
       "      <td>537 U.S. 270</td>\n",
       "      <td>2003</td>\n",
       "      <td>United States v. Jimenez Recio</td>\n",
       "      <td>10400</td>\n",
       "      <td>2002-016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/12/2002</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1/1/1184</td>\n",
       "      <td>2002-016-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>537 U.S. 270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>United States v. Jones 345 U.S. 377 (1953)</td>\n",
       "      <td>United States v. Jones No. 556 Decided April 1...</td>\n",
       "      <td>https://supreme.justia.com/cases/federal/us/34...</td>\n",
       "      <td>345 U.S. 377</td>\n",
       "      <td>1953</td>\n",
       "      <td>United States v. Jones</td>\n",
       "      <td>927</td>\n",
       "      <td>1952-078</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>556</td>\n",
       "      <td>1952-078-01</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>345 U.S. 377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Joy Oil Co., Ltd. v. State Tax Commission 337 ...</td>\n",
       "      <td>Joy Oil Co., Ltd. v. State Tax Commission No. ...</td>\n",
       "      <td>https://supreme.justia.com/cases/federal/us/33...</td>\n",
       "      <td>337 U.S. 286</td>\n",
       "      <td>1949</td>\n",
       "      <td>Joy Oil Co., Ltd. v. State Tax Commission</td>\n",
       "      <td>461</td>\n",
       "      <td>1948-090</td>\n",
       "      <td>27</td>\n",
       "      <td>1/6/1949</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>223</td>\n",
       "      <td>1948-090-01</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>337 U.S. 286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Witte v. United States 515 U.S. 389 (1995)</td>\n",
       "      <td>OCTOBER TERM, 1994 Syllabus WITTE v. UNITED ST...</td>\n",
       "      <td>https://supreme.justia.com/cases/federal/us/51...</td>\n",
       "      <td>515 U.S. 389</td>\n",
       "      <td>1995</td>\n",
       "      <td>Witte v. United States</td>\n",
       "      <td>9663</td>\n",
       "      <td>1994-076</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4/17/1995</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>94-6187</td>\n",
       "      <td>1994-076-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>104</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>515 U.S. 389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Warth v. Seldin 422 U.S. 490 (1975)</td>\n",
       "      <td>Warth v. Seldin No. 73-2024 Argued March 17, 1...</td>\n",
       "      <td>https://supreme.justia.com/cases/federal/us/42...</td>\n",
       "      <td>422 U.S. 490</td>\n",
       "      <td>1975</td>\n",
       "      <td>Warth v. Seldin</td>\n",
       "      <td>5850</td>\n",
       "      <td>1974-140</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3/17/1975</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>73-2024</td>\n",
       "      <td>1974-140-01</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>101</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>422 U.S. 490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           full_cite                                               text                                                url       us_cite  year                                       case  case_id    caseId  caseOriginState dateArgument  decisionDirection  decisionType    docket     docketId  issueArea  jurisdiction  lawType  majOpinWriter  majVotes  minVotes        usCite\n",
       "0  United States v. Jimenez Recio 537 U.S. 270 (2...  OCTOBER TERM, 2002 Syllabus UNITED STATES v. J...  https://supreme.justia.com/cases/federal/us/53...  537 U.S. 270  2003             United States v. Jimenez Recio    10400  2002-016              NaN   11/12/2002                  1             1  1/1/1184  2002-016-01          1             1      NaN            110         8         1  537 U.S. 270\n",
       "1         United States v. Jones 345 U.S. 377 (1953)  United States v. Jones No. 556 Decided April 1...  https://supreme.justia.com/cases/federal/us/34...  345 U.S. 377  1953                     United States v. Jones      927  1952-078               12          NaN                  1             2       556  1952-078-01          9             2        6            NaN         9         0  345 U.S. 377\n",
       "2  Joy Oil Co., Ltd. v. State Tax Commission 337 ...  Joy Oil Co., Ltd. v. State Tax Commission No. ...  https://supreme.justia.com/cases/federal/us/33...  337 U.S. 286  1949  Joy Oil Co., Ltd. v. State Tax Commission      461  1948-090               27     1/6/1949                  2             1       223  1948-090-01          8             1        1             80         6         3  337 U.S. 286\n",
       "3         Witte v. United States 515 U.S. 389 (1995)  OCTOBER TERM, 1994 Syllabus WITTE v. UNITED ST...  https://supreme.justia.com/cases/federal/us/51...  515 U.S. 389  1995                     Witte v. United States     9663  1994-076              NaN    4/17/1995                  1             1   94-6187  1994-076-01          1             1        2            104         8         1  515 U.S. 389\n",
       "4                Warth v. Seldin 422 U.S. 490 (1975)  Warth v. Seldin No. 73-2024 Argued March 17, 1...  https://supreme.justia.com/cases/federal/us/42...  422 U.S. 490  1975                            Warth v. Seldin     5850  1974-140              NaN    3/17/1975                  1             1   73-2024  1974-140-01          9             1      NaN            101         5         4  422 U.S. 490"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Cleaning text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only use one regular expression operator, which is any letter between parentheses, for example: (a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re \n",
    "regex1 = r\"\\(.\\)\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function takes an opinion (a string), and returns the verbs, nouns, adjectives, foreign words, and court precedents cited in the opinion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pattern.en import parse\n",
    "from pattern.en import pprint\n",
    "from pattern.en import conjugate, lemma, lexeme\n",
    "from pattern.vector import stem, PORTER, LEMMA\n",
    "from sklearn.feature_extraction import text\n",
    "import string\n",
    "\n",
    "#stopwords and punctuation\n",
    "stopwords=text.ENGLISH_STOP_WORDS\n",
    "punctuation = list('.,;:!?()[]{}`''\\\"@#$^&*+-|=~_')\n",
    "\n",
    "def get_parts(opinion):\n",
    "    oplow = opinion.lower()\n",
    "    #REMOVING CHARACTERS: we have ugly text, and remove unnecssary characters.\n",
    "    oplow = unicode(oplow, 'ascii', 'ignore') #remove non-unicode characters \n",
    "    oplow = str(oplow).translate(string.maketrans(\"\\n\\t\\r\", \"   \")) #remove characters like \\n \n",
    "    #justices (eg, Justice Breyer) are referred to as J. (eg,Breyer, J.); we remove the J., also JJ. for plural\n",
    "    oplow = oplow.replace('j.','')\n",
    "    oplow = oplow.replace('jj.','')\n",
    "    oplow = oplow.replace('c.','') #remove C. for chief justice \n",
    "    oplow = oplow.replace('pp.','') #page numbers\n",
    "    oplow = oplow.replace('  ','') #multiple spaces\n",
    "    oplow = ''.join([i for i in oplow if not i.isdigit()]) #remove digits \n",
    "    oplow=re.sub(regex1, ' ', oplow)\n",
    "    #Remove the Justia disclaimer at the end of the case, if it appears in the string\n",
    "    justiadisclaimer = \"disclaimer: official\"\n",
    "    if justiadisclaimer in oplow: \n",
    "        optouse = oplow.split(justiadisclaimer)[0]\n",
    "    else:\n",
    "        optouse = oplow\n",
    "    \n",
    "    #GET A LIST OF PRECEDENTS CITED IN THE OPINION \n",
    "    wordslist = optouse.split()\n",
    "    #find precedents based on string 'v.' (eg, 'Brown v. Board')\n",
    "    indices = [i for i in range(len(wordslist)) if wordslist[i]=='v.']\n",
    "    precedents = [wordslist[i-1]+ ' ' + wordslist[i]+ ' ' + wordslist[i+1] for i in indices]\n",
    "    \n",
    "    #remove precedents, as we have already accounted for these\n",
    "    for precedent in precedents:\n",
    "        optouse = optouse.replace(precedent,'')\n",
    "    \n",
    "    #PARSE INTO LIST OF LISTS --> GET WORDS\n",
    "    parsed = parse(optouse,tokenize=True,chunks=False,lemmata=True).split()\n",
    "    verbs = [] \n",
    "    nouns = [] \n",
    "    adjectives = [] \n",
    "    foreign = [] \n",
    "    i=0\n",
    "    #Create lists of lists of verbs, nouns, adjectives and foreign words in each sentence.\n",
    "    for sentence in parsed: #for each sentence \n",
    "        verbs.append([])\n",
    "        nouns.append([])\n",
    "        adjectives.append([])\n",
    "        foreign.append([])\n",
    "        for token in sentence: #for each word in the sentence \n",
    "            if token[0] in punctuation or token[0] in stopwords or len(token[0])<=2:\n",
    "                continue\n",
    "            wordtouse = token[0]\n",
    "            for x in punctuation:\n",
    "                wordtouse = wordtouse.replace(x,' ') #if punctuation in word, take it out\n",
    "            if token[1] in ['VB','VBZ','VBP','VBD','VBN','VBG']:\n",
    "                verbs[i].append(lemma(wordtouse)) #append the lemmatized verb (we relemmatize because lemmata in parse does not seem to always work)\n",
    "            if token[1] in ['NN','NNS','NNP','NNPS']:\n",
    "                nouns[i].append(lemma(wordtouse))\n",
    "            if token[1] in ['JJ','JJR','JJS']:\n",
    "                adjectives.append(lemma(wordtouse))\n",
    "            if token[1] in ['FW']:\n",
    "                foreign.append(wordtouse)  \n",
    "        i+=1  \n",
    "    #Zip together lists so each tuple is a sentence. \n",
    "    out=zip(verbs,nouns,adjectives,foreign)\n",
    "    verbs2 = []\n",
    "    nouns2 = []\n",
    "    adjectives2 = []\n",
    "    foreign2 = []\n",
    "    for sentence in out: \n",
    "        if sentence[0]!=[] and sentence[1]!=0: #if the sentence has at least one verb and noun, keep it. Otherwise, drop it.\n",
    "            if type(sentence[0])==list: \n",
    "                verbs2.append(sentence[0])\n",
    "            else: \n",
    "                verbs2.append([sentence[0]]) #if verb is a string rather than a list, put string in list\n",
    "            if type(sentence[1])==list:\n",
    "                nouns2.append(sentence[1])\n",
    "            else:\n",
    "                nouns2.append([sentence[1]])\n",
    "            if type(sentence[2])==list:\n",
    "                adjectives2.append(sentence[2])\n",
    "            else:\n",
    "                adjectives2.append([sentence[2]])\n",
    "            if type(sentence[3])==list:\n",
    "                foreign2.append(sentence[3])\n",
    "            else:\n",
    "                foreign2.append([sentence[3]])\n",
    "    return(verbs2,nouns2,adjectives2,foreign2,precedents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Lists of words, vocabularies\n",
    "In the next cell, we run get_parts on all the opinions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 54s, sys: 693 ms, total: 1min 55s\n",
      "Wall time: 1min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "verbwords = []\n",
    "nounwords = []\n",
    "adjwords = []\n",
    "forwords = []\n",
    "precedents_all = []\n",
    "for op in sample_df.text:\n",
    "    verbs,nouns,adjectives,foreign,precedents = get_parts(op)\n",
    "    verbwords.append(verbs)\n",
    "    nounwords.append(nouns)\n",
    "    adjwords.append(adjectives)\n",
    "    forwords.append(foreign)\n",
    "    precedents_all.append(precedents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a list of issue areas (our y variable), which is in the same order as the cases in the sample_df (and thus matches the order of cases in verbwords, nounwords, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "issue_areas = sample_df.issueArea.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next create vocabularies, and also create maps between word id's and words (and vice versa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create precedents vocab\n",
    "precedents_vocab = list(set([precedent for sublist in precedents_all for precedent in sublist]))\n",
    "#create other vocabs\n",
    "verbvocab = list(set([word for sublist in verbwords for subsublist in sublist for word in subsublist]))\n",
    "nounvocab = list(set([word for sublist in nounwords for subsublist in sublist for word in subsublist]))\n",
    "adjvocab = list(set([word for sublist in adjwords for subsublist in sublist for word in subsublist]))\n",
    "forvocab = list(set([word for sublist in forwords for subsublist in sublist for word in subsublist]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dictionaries: id --> word\n",
    "id2prec = dict(enumerate(precedents_vocab))\n",
    "id2verb = dict(enumerate(verbvocab))\n",
    "id2noun = dict(enumerate(nounvocab))\n",
    "id2adj = dict(enumerate(adjvocab))\n",
    "id2for = dict(enumerate(forvocab))\n",
    "#dictionaries: word --> id\n",
    "prec2id = dict(zip(id2prec.values(),id2prec.keys()))\n",
    "verb2id = dict(zip(id2verb.values(),id2verb.keys()))\n",
    "noun2id = dict(zip(id2noun.values(),id2noun.keys()))\n",
    "adj2id = dict(zip(id2adj.values(),id2adj.keys()))\n",
    "for2id = dict(zip(id2for.values(),id2for.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create corpuses (one for each word type and precedents). Each corpus is a list of lists. Each inner list corresponds to an opinion, and has as its elements tuples of the form `(wordid, count)`, where `count` refers to the number of times the word appears in the opinion. A sample corpus may look like the following (this sample corpus is taken from problem set 5): \n",
    "\n",
    "```\n",
    "[[(5912, 1), (3809, 1), (14131, 1), (3876, 1)],\n",
    " [(3266, 1), (3652, 1), (11644, 1), (2296, 1), (27516, 1), (8382, 1)],\n",
    " [(17217, 1), (22979, 1), (11210, 1), (18736, 1), (3893, 1), (21307, 1)],\n",
    " ...,\n",
    " [(23980, 1), (24730, 1), (22979, 1), (20012, 1), (11206, 2)]]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this function takes a list of words, and outputs a list of tuples \n",
    "counter = lambda x:list(set([(i,x.count(i)) for i in x]))\n",
    "\n",
    "#corpus_creator takes a list of lists of lists like verbwords, or a list of lists like precedents_all. \n",
    "#It also takes a word2id dictionary.\n",
    "def corpus_creator(sentence_word_list,word2id):\n",
    "    counter = lambda x:list(set([(word2id[i],x.count(i)) for i in x]))\n",
    "    op_word_list = []\n",
    "    if type(sentence_word_list[0][0])==list: #if list of lists of lists \n",
    "        for opinion in sentence_word_list: \n",
    "            #for each list (which corresponds to an opinion) in sentence_word_list, get a list of the words\n",
    "            op_word_list.append([word for sublist in opinion for word in sublist])\n",
    "    else: #if list of lists \n",
    "        op_word_list = sentence_word_list\n",
    "    corpus = []\n",
    "    for element in op_word_list: \n",
    "        corpus.append(counter(element))\n",
    "    return(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get nouncorpus, as an example\n",
    "nouncorpus = corpus_creator(nounwords,noun2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#function takes a corpus (a list of tuples, each tuple corresponding to a document) and a list of issue areas\n",
    "# (ie, a list of the issue area corresponding to each document, such as issue_areas above) as arguments. \n",
    "#For example, you might run: train_test_split(nouncorpus,issue_areas)\n",
    "def train_test_split(corpus,issue_areas):\n",
    "    #Run train-test split: randomly sample from the corpus \n",
    "    traintestarray = np.random.choice([0, 1], size=(len(corpus)), p=[.3, .7])\n",
    "    #create train and test corpuses\n",
    "    train_corpus = [corpus[i] for i in range(len(corpus)) if traintestarray[i]==1]\n",
    "    test_corpus = [corpus[i] for i in range(len(corpus)) if traintestarray[i]==0]\n",
    "    #create train and test issue_area lists\n",
    "    train_issue_areas = [issue_areas[i] for i in range(len(corpus)) if traintestarray[i]==1]\n",
    "    test_issue_areas = [issue_areas[i] for i in range(len(corpus)) if traintestarray[i]==0]\n",
    "    return(train_corpus,train_issue_areas,test_corpus,test_issue_areas)\n",
    "#The function returns four lists: train_corpus, train_issue_areas,test_corpus,test_issue_areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#this runs the above function on nouns\n",
    "train_corpus,train_issue_areas,test_corpus,test_issue_areas = train_test_split(nouncorpus,issue_areas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
