{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn.decomposition\n",
    "import sklearn.feature_extraction\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import bottleneck as bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.04082109  0.50448234  0.91513758]\n",
      " [ 0.          0.95532141  0.90909083]]\n",
      "\n",
      "[[ 0.33143314  0.92619924]\n",
      " [ 0.66168676  0.78120864]\n",
      " [ 0.97507615  0.49745405]\n",
      " [ 1.31486536  0.33994089]\n",
      " [ 1.64976882  0.08157003]\n",
      " [ 1.97286978 -0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# arbitrary 6x3 matrix (constructed from an example at http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html with a random column concatenated)\n",
    "M = np.concatenate((np.array([[1,1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]]),map(lambda x : [x], np.round(np.random.random_sample(6),1)+1)),1)\n",
    "M\n",
    "H = sklearn.decomposition.NMF(n_components=2).fit(M).components_\n",
    "W = sklearn.decomposition.NMF(n_components=2).fit_transform(M)\n",
    "print H\n",
    "print\n",
    "print W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -0.076   2.453   2.624   3.525]\n",
      " [  1.378   2.172   1.877   4.466]\n",
      " [  1.341   2.703   2.927   3.301]\n",
      " [  0.577   2.142   2.858   4.498]\n",
      " [  1.133   1.996   2.054   4.222]\n",
      " [ 10.546  -0.106   1.892   8.022]\n",
      " [  0.533   4.966   4.098   0.895]\n",
      " [  1.135   4.198   5.239   0.605]\n",
      " [  1.285   5.075   3.301   0.059]\n",
      " [  0.661   5.202   4.288   0.935]\n",
      " [ 10.801   0.461   2.413   7.35 ]\n",
      " [  9.816   0.594   1.737   7.998]\n",
      " [  1.542   5.036   5.683   0.576]\n",
      " [ -0.515   5.379   3.726   1.052]\n",
      " [  1.785   5.362   4.026   0.962]\n",
      " [  0.593   1.659   3.477   3.508]\n",
      " [  0.441   4.303   3.743   1.513]\n",
      " [  0.577   2.626   2.868   4.019]\n",
      " [  1.      4.923   4.458   1.311]\n",
      " [  1.274   2.745   3.452   4.477]]\n",
      "\n",
      "[[-0.01509947  0.4873554   0.52132922  0.70033746]\n",
      " [ 0.2512326   0.39599217  0.3422087   0.81422699]\n",
      " [ 0.25089134  0.50571162  0.54762039  0.61759307]\n",
      " [ 0.09995743  0.37107248  0.49510979  0.77921757]\n",
      " [ 0.21679732  0.38193067  0.39302886  0.8078714 ]\n",
      " [ 0.7878897  -0.00791924  0.14135097  0.59932213]\n",
      " [ 0.08172014  0.76139254  0.62830983  0.13722238]\n",
      " [ 0.166044    0.61414335  0.76643569  0.08850803]\n",
      " [ 0.20761773  0.81996886  0.53334329  0.00953264]\n",
      " [ 0.09666502  0.76074347  0.62707958  0.13673494]\n",
      " [ 0.81249764  0.0346784   0.18151623  0.55289859]\n",
      " [ 0.76722192  0.04642724  0.13576451  0.62512642]\n",
      " [ 0.19846476  0.64816378  0.7314366   0.0741347 ]\n",
      " [-0.07747327  0.80918197  0.56051534  0.15825608]\n",
      " [ 0.25481445  0.76544261  0.57472435  0.13732857]\n",
      " [ 0.11308164  0.31636163  0.66304363  0.66895515]\n",
      " [ 0.07453248  0.72724098  0.63259656  0.25570895]\n",
      " [ 0.10263299  0.46709573  0.51014111  0.71487348]\n",
      " [ 0.14613198  0.71940775  0.65145638  0.19157903]\n",
      " [ 0.19867983  0.42808174  0.53833813  0.69818651]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Negative values in data passed to NMF.fit",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-b76b8b636cfa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# run NMF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mfit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecomposition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNMF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[0mH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Madhu\\Anaconda\\lib\\site-packages\\sklearn\\decomposition\\nmf.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m         \"\"\"\n\u001b[1;32m--> 551\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    552\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Madhu\\Anaconda\\lib\\site-packages\\sklearn\\decomposition\\nmf.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m    475\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 476\u001b[1;33m         \u001b[0mcheck_non_negative\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"NMF.fit\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    478\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Madhu\\Anaconda\\lib\\site-packages\\sklearn\\decomposition\\nmf.pyc\u001b[0m in \u001b[0;36mcheck_non_negative\u001b[1;34m(X, whom)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Negative values in data passed to %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mwhom\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Negative values in data passed to NMF.fit"
     ]
    }
   ],
   "source": [
    "# a 20x4 example in which there are explicit types (that are unknown to the algorithm)\n",
    "n = 20\n",
    "types = np.trunc(np.random.random_sample(n)*3)\n",
    "means = [[1,2,3,4],[1,5,4,1],[10,1,2,8]]\n",
    "stddevs = 0.5 * np.ones([3,4])\n",
    "def gen(t):\n",
    "    mean = means[t]\n",
    "    sd = stddevs[t]\n",
    "    sz = len(mean)\n",
    "    return list(np.random.randn(sz) * sd + mean)\n",
    "# generate a data matrix\n",
    "M = np.round(map(gen, map(int, types)),3)\n",
    "\n",
    "M_tfidf = sklearn.feature_extraction.text.TfidfTransformer().fit_transform(M).toarray()\n",
    "print M\n",
    "print\n",
    "print M_tfidf\n",
    "print\n",
    "print\n",
    "\n",
    "# run NMF\n",
    "fit = sklearn.decomposition.NMF(n_components=3).fit(M)\n",
    "H = fit.components_\n",
    "W = fit.transform(M)\n",
    "#print H\n",
    "results = list(np.concatenate((10*W,map(lambda x:[x],types)), 1))\n",
    "results.sort(key = lambda row : row[3])\n",
    "results = np.array(map(lambda row : list(np.round(row,3)), results))\n",
    "print results\n",
    "#print map(lambda row : np.argmax(np.round(list(row[0:3]),3)), results)\n",
    "# predicted clusters\n",
    "print map(lambda row : np.argmax(np.round(list(row[0:3]),3)), W)\n",
    "# true clusters\n",
    "print map(int, types)\n",
    "# use fit from above to classify a new point\n",
    "print fit.transform([[1,2,4,5]])\n",
    "\n",
    "print\n",
    "print\n",
    "\n",
    "fit = sklearn.decomposition.NMF(n_components=3).fit(M_tfidf)\n",
    "H = fit.components_\n",
    "W = fit.transform(M_tfidf)\n",
    "#print H\n",
    "results = list(np.concatenate((10*W,map(lambda x:[x],types)), 1))\n",
    "results.sort(key = lambda row : row[3])\n",
    "results = np.array(map(lambda row : list(np.round(row,3)), results))\n",
    "print results\n",
    "#print map(lambda row : np.argmax(np.round(list(row[0:3]),3)), results)\n",
    "# predicted clusters\n",
    "print map(lambda row : np.argmax(np.round(list(row[0:3]),3)), W)\n",
    "# true clusters\n",
    "print map(int, types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 45.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# read in data from csv\n",
    "noun_train_mat = np.loadtxt(\"noun_train_mat.csv\", delimiter = \",\")\n",
    "# use TF-IDF to scale each document's vector to have norm 1 and place a lower weight on very common words\n",
    "tf_idf_fit = sklearn.feature_extraction.text.TfidfTransformer().fit(noun_train_mat)\n",
    "noun_train_mat = tf_idf_fit.transform(noun_train_mat).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 30.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# compute NMF fit\n",
    "NMF_fit = sklearn.decomposition.NMF(n_components=14).fit(noun_train_mat)\n",
    "H = NMF_fit.components_\n",
    "W = NMF_fit.transform(noun_train_mat)\n",
    "# contains a tuple (i,j) if document i is in cluster j, for each document\n",
    "clusters = map(np.argmax, W)\n",
    "# list of the documents in each cluster\n",
    "cluster_lists = [[i for i,j in enumerate(clusters) if j==cluster] for cluster in range(14)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents per category: [110, 81, 58, 29, 167, 64, 54, 84, 84, 64, 38, 126, 115, 77]\n"
     ]
    }
   ],
   "source": [
    "# classify each document into the category that fits it best\n",
    "print 'Number of documents per category:', [sum([x[1]==i for x in enumerate(clusters)]) for i in range(14)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load vocab from csv\n",
    "noun_vocab = np.loadtxt(\"noun_vocab.csv\", delimiter=\",\", dtype=\"str\")\n",
    "noun_vocab = [(int(i),j) for i,j in noun_vocab]\n",
    "id2noun = dict(noun_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['court', 'appeal', 'district', 'jurisdiction', 'judgment'],\n",
       " ['employee', 'board', 'labor', 'union', 'relation'],\n",
       " ['commerce', 'tax', 'income', 'revenue', 'taxpayer'],\n",
       " ['decree', 'master', 'orig ', 'entry', 'boundary'],\n",
       " ['amendment', 'court', 'law', 'state', 'statute'],\n",
       " ['sentence', 'offense', 'conviction', 'guideline', 'death'],\n",
       " ['water', 'land', 'tribe', 'act', 'congres'],\n",
       " ['search', 'officer', 'warrant', 'respondent', 'police'],\n",
       " ['commission', 'carrier', 'commerce', 'gas', 'act'],\n",
       " ['property', 'bankruptcy', 'lien', 'debtor', 'bank'],\n",
       " ['child', 'school', 'board', 'student', 'children'],\n",
       " ['act', 'employee', 'respondent', 'action', 'liability'],\n",
       " ['trial', 'petitioner', 'jury', 'counsel', 'evidence'],\n",
       " ['plan', 'benefit', 'security', 'provision', 'insurance']]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the 5 most important words for each category\n",
    "num_best = 5\n",
    "best_indices = map(lambda v : list(bn.argpartsort(-v,num_best)[0:num_best]), H)\n",
    "best_words = [[id2noun[i] for i in lst] for lst in best_indices]\n",
    "best_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 6, 7, 10, 2, 0, 7, 0, 7, 7, 1, 7, 0, 7]\n",
      "[8, 6, 7, 10, 2, 0, 7, 0, 7, 7, 1, 7, 0, 7]\n",
      "[5, 10, 10, 9, 1, 0, 1, 8, 0, 3, 3, 2, 6, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([ 0.2       ,  0.19090909,  0.04545455,  0.        ,  0.00909091,\n",
       "         0.03636364,  0.00909091,  0.1       ,  0.39090909,  0.01818182,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ]),\n",
       " array([ 0.02469136,  0.01234568,  0.02469136,  0.01234568,  0.02469136,\n",
       "         0.01234568,  0.61728395,  0.0617284 ,  0.11111111,  0.09876543,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ]),\n",
       " array([ 0.12068966,  0.06896552,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.4137931 ,  0.06896552,  0.03448276,\n",
       "         0.        ,  0.27586207,  0.01724138,  0.        ]),\n",
       " array([ 0.        ,  0.10344828,  0.03448276,  0.03448276,  0.        ,\n",
       "         0.        ,  0.        ,  0.10344828,  0.10344828,  0.20689655,\n",
       "         0.4137931 ,  0.        ,  0.        ,  0.        ]),\n",
       " array([ 0.13173653,  0.18562874,  0.20958084,  0.04790419,  0.01796407,\n",
       "         0.01796407,  0.        ,  0.08982036,  0.16766467,  0.1257485 ,\n",
       "         0.        ,  0.00598802,  0.        ,  0.        ]),\n",
       " array([ 0.71875 ,  0.1875  ,  0.015625,  0.015625,  0.      ,  0.      ,\n",
       "         0.      ,  0.015625,  0.046875,  0.      ,  0.      ,  0.      ,\n",
       "         0.      ,  0.      ]),\n",
       " array([ 0.        ,  0.2962963 ,  0.        ,  0.09259259,  0.        ,\n",
       "         0.        ,  0.        ,  0.35185185,  0.03703704,  0.16666667,\n",
       "         0.01851852,  0.01851852,  0.01851852,  0.        ]),\n",
       " array([ 0.66666667,  0.17857143,  0.03571429,  0.02380952,  0.02380952,\n",
       "         0.        ,  0.        ,  0.01190476,  0.04761905,  0.01190476,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ]),\n",
       " array([ 0.03571429,  0.01190476,  0.        ,  0.03571429,  0.        ,\n",
       "         0.        ,  0.07142857,  0.64285714,  0.11904762,  0.08333333,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ]),\n",
       " array([ 0.109375,  0.03125 ,  0.046875,  0.140625,  0.      ,  0.03125 ,\n",
       "         0.      ,  0.34375 ,  0.140625,  0.015625,  0.      ,  0.140625,\n",
       "         0.      ,  0.      ]),\n",
       " array([ 0.02631579,  0.55263158,  0.34210526,  0.02631579,  0.        ,\n",
       "         0.        ,  0.02631579,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.02631579,  0.        ,  0.        ]),\n",
       " array([ 0.03174603,  0.28571429,  0.07142857,  0.00793651,  0.00793651,\n",
       "         0.00793651,  0.03174603,  0.42857143,  0.0952381 ,  0.01587302,\n",
       "         0.        ,  0.00793651,  0.00793651,  0.        ]),\n",
       " array([ 0.71304348,  0.10434783,  0.06086957,  0.05217391,  0.        ,\n",
       "         0.        ,  0.        ,  0.04347826,  0.0173913 ,  0.        ,\n",
       "         0.        ,  0.        ,  0.00869565,  0.        ]),\n",
       " array([ 0.01298701,  0.2987013 ,  0.05194805,  0.        ,  0.        ,\n",
       "         0.02597403,  0.        ,  0.41558442,  0.07792208,  0.07792208,\n",
       "         0.        ,  0.03896104,  0.        ,  0.        ])]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_mat = map(lambda r : map(int, r), np.zeros((14,14)))\n",
    "for i,j in zip(clusters, noun_train_issue_areas - 1):\n",
    "    compare_mat[i][j] += 1\n",
    "compare_mat = map(lambda row : map(float,row) / sum(row), np.array(compare_mat))\n",
    "\n",
    "assignments = -1 * np.ones(14)\n",
    "compare_mat_flat = [x for lst in compare_mat for x in lst]\n",
    "while min(assignments) == -1:\n",
    "    max_ind = np.argmax(compare_mat_flat)\n",
    "    row = max_ind / 14\n",
    "    if assignments[row] == -1:\n",
    "        column = max_ind - 14 * row\n",
    "        assignments[row] = column\n",
    "    else:\n",
    "        compare_mat_flat[max_ind] = -1\n",
    "#5 / 2\n",
    "print map(int, assignments)\n",
    "print map(np.argmax, compare_mat)\n",
    "print map(np.argmax, np.array(compare_mat).T)\n",
    "\n",
    "compare_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['narcotic', 'middleman', 'middleman', 'city the', 'city the', 'middleman', 'city the', 'narcotic', 'fdca', 'city the', 'city the', 'middleman', 'city the', 'fdca']\n"
     ]
    }
   ],
   "source": [
    "# can probably ignore this stuff -- it's a different way to figure out the best words in each category, but it's not working\n",
    "# and produces nonsense\n",
    "width = len(M[0])\n",
    "best_words = np.zeros(14)\n",
    "for i in range(14):\n",
    "    # arbitrary high initial value\n",
    "    w = np.ones(width) * 5000\n",
    "    #w_val = np.ones(width)\n",
    "    for j in range(width):\n",
    "        for k in range(14):\n",
    "            if i <> k and H[k][j] > 0:\n",
    "                if w[j] >= float(H[i][j])/H[k][j]:\n",
    "                    w[j] = float(H[i][j])/H[k][j]\n",
    "    best_words[i] = np.argmax(w)\n",
    "print map(lambda x : id2noun[int(x)], best_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# read in test data from csv\n",
    "noun_test_mat = np.loadtxt(\"noun_test_mat.csv\", delimiter = \",\")\n",
    "# use TF-IDF to scale each document's vector to have norm 1 and place a lower weight on very common words\n",
    "noun_test_mat = tf_idf_fit.transform(noun_test_mat).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use NMF fit from training data to cluster test observations\n",
    "W_test = NMF_fit.transform(noun_test_mat)\n",
    "clusters_test = map(np.argmax, W_test)\n",
    "cluster_lists_test = [[i for i,j in enumerate(clusters_test) if j==cluster] for cluster in range(14)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 16, 29, 37, 40, 55, 69, 75, 76, 77, 79, 81, 88, 89, 99, 105, 118, 138, 164, 168, 179, 184, 188, 189, 201, 208, 218, 233, 250, 267, 271, 275, 284, 292, 321, 325, 336, 341, 349, 355, 358, 389, 396, 407], [28, 35, 38, 109, 113, 124, 125, 152, 157, 159, 177, 195, 203, 212, 219, 240, 246, 291, 302, 303, 310, 312, 319, 327, 360, 379, 416], [5, 8, 21, 24, 34, 57, 87, 104, 121, 145, 146, 174, 200, 254, 264, 268, 269, 272, 317, 335, 426, 428, 439, 441], [15, 82, 92, 123, 181, 241, 427], [3, 14, 22, 25, 30, 32, 42, 44, 46, 52, 71, 73, 91, 100, 101, 112, 114, 116, 130, 132, 141, 150, 158, 160, 165, 167, 169, 171, 186, 187, 194, 198, 204, 210, 221, 223, 225, 227, 234, 236, 239, 251, 257, 259, 261, 280, 308, 315, 318, 320, 330, 339, 340, 342, 345, 378, 382, 385, 394, 397, 399, 406, 412, 414, 422, 423, 429, 431, 438, 440], [9, 26, 58, 78, 97, 131, 149, 178, 190, 199, 202, 209, 222, 237, 258, 260, 265, 279, 298, 304, 348, 366, 372, 386, 408, 410], [11, 49, 102, 110, 129, 139, 276, 299, 324, 332, 333, 343, 346, 350, 365, 374, 405, 432, 437], [6, 7, 27, 33, 66, 70, 72, 95, 119, 135, 173, 213, 220, 224, 245, 283, 286, 314, 331, 370, 381, 391, 393, 395, 413, 415, 443], [19, 43, 53, 61, 65, 67, 106, 140, 144, 153, 155, 197, 205, 206, 207, 242, 253, 266, 281, 285, 305, 306, 316, 329, 347, 352, 353, 369, 376, 377, 384, 402, 419, 420, 424, 433], [12, 18, 63, 64, 96, 98, 103, 126, 147, 154, 182, 216, 238, 277, 282, 295, 398, 421, 434], [4, 23, 39, 84, 111, 120, 211, 232, 248, 344, 368, 383], [10, 13, 20, 41, 45, 47, 48, 51, 62, 68, 83, 85, 86, 90, 108, 115, 133, 142, 156, 162, 170, 172, 175, 176, 192, 215, 217, 235, 244, 247, 256, 263, 274, 278, 287, 290, 293, 294, 301, 309, 313, 322, 337, 338, 359, 363, 364, 367, 375, 390, 401, 417, 425, 436], [1, 2, 17, 36, 59, 80, 93, 94, 107, 117, 122, 127, 128, 134, 136, 143, 148, 161, 166, 180, 183, 185, 196, 214, 228, 229, 230, 231, 243, 255, 270, 273, 296, 297, 326, 334, 361, 373, 380, 403, 404, 409, 418, 430, 442], [31, 50, 54, 56, 60, 74, 137, 151, 163, 191, 193, 226, 249, 252, 262, 288, 289, 300, 307, 311, 323, 328, 351, 354, 356, 357, 362, 371, 387, 388, 392, 400, 411, 435]]\n"
     ]
    }
   ],
   "source": [
    "print cluster_lists_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
