{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn.decomposition\n",
    "import sklearn.feature_extraction\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import bottleneck as bn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Fake Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.07519602  0.44741457  0.77929221]\n",
      " [ 0.          0.96806042  1.1249227 ]]\n",
      "\n",
      "[[ 0.328143    1.01084916]\n",
      " [ 0.64770545  0.61618696]\n",
      " [ 0.97154435  0.61351523]\n",
      " [ 1.30079584  0.43170723]\n",
      " [ 1.62395529 -0.        ]\n",
      " [ 1.95505003  0.29763123]]\n"
     ]
    }
   ],
   "source": [
    "# arbitrary 6x3 matrix (constructed from an example at http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html with a random column concatenated)\n",
    "M = np.concatenate((np.array([[1,1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]]),map(lambda x : [x], np.round(np.random.random_sample(6),1)+1)),1)\n",
    "M\n",
    "H = sklearn.decomposition.NMF(n_components=2).fit(M).components_\n",
    "W = sklearn.decomposition.NMF(n_components=2).fit_transform(M)\n",
    "print H\n",
    "print\n",
    "print W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.072   4.956   4.011   0.802]\n",
      " [  9.865   0.675   2.673   8.355]\n",
      " [ 10.05    0.036   2.364   8.224]\n",
      " [  0.35    1.633   2.786   3.934]\n",
      " [  0.723   4.566   4.447   1.472]\n",
      " [  9.926   0.302   1.444   7.64 ]\n",
      " [  0.912   5.126   3.681   0.96 ]\n",
      " [  1.92    4.893   3.625   1.508]\n",
      " [  1.621   5.348   4.19    1.373]\n",
      " [  9.765   1.166   1.95    7.319]\n",
      " [  0.592   1.612   2.369   4.295]\n",
      " [  0.587   2.06    2.634   3.073]\n",
      " [  0.784   3.238   3.197   4.753]\n",
      " [  0.74    1.644   3.028   3.965]\n",
      " [ 10.597   1.569   2.334   8.535]\n",
      " [  1.112   2.592   3.101   3.911]\n",
      " [  1.872   1.559   2.914   3.758]\n",
      " [  0.638   5.422   3.827   0.295]\n",
      " [ 10.114   0.53    1.249   7.767]\n",
      " [  0.774   3.164   3.612   4.013]]\n",
      "\n",
      "[[ 0.01120381  0.77119543  0.62414545  0.12479797]\n",
      " [ 0.74631094  0.05106537  0.20221887  0.63207581]\n",
      " [ 0.7613932   0.00272738  0.17909786  0.6230545 ]\n",
      " [ 0.06860459  0.32008939  0.5460925   0.77111554]\n",
      " [ 0.1098565   0.69378255  0.67570106  0.22366358]\n",
      " [ 0.78700628  0.02394478  0.11449094  0.60575539]\n",
      " [ 0.14143533  0.79495342  0.57085906  0.1488793 ]\n",
      " [ 0.29265254  0.7458067   0.55253409  0.22985418]\n",
      " [ 0.22772341  0.75130462  0.58862497  0.19288355]\n",
      " [ 0.78666895  0.09393303  0.15709211  0.58961905]\n",
      " [ 0.11391317  0.31018248  0.4558451   0.82644774]\n",
      " [ 0.12818716  0.44985615  0.57520441  0.67107182]\n",
      " [ 0.11831201  0.48864065  0.48245342  0.71726653]\n",
      " [ 0.13949769  0.30991108  0.57080946  0.7474437 ]\n",
      " [ 0.76268611  0.11292389  0.16798239  0.61428007]\n",
      " [ 0.19396523  0.4521204   0.54090485  0.68219248]\n",
      " [ 0.35035775  0.29177763  0.54537525  0.70333569]\n",
      " [ 0.09559935  0.8124446   0.57344624  0.04420346]\n",
      " [ 0.78866499  0.0413281   0.09739397  0.60565167]\n",
      " [ 0.12274789  0.50177561  0.57282348  0.63641767]]\n",
      "\n",
      "\n",
      "[[  0.623   5.382  20.112   0.   ]\n",
      " [  1.242   4.606  20.24    0.   ]\n",
      " [  1.116   6.621  14.433   0.   ]\n",
      " [  1.672   9.138  21.269   0.   ]\n",
      " [  1.375   5.778  19.19    0.   ]\n",
      " [  2.224   8.07   16.616   0.   ]\n",
      " [  3.701   5.709  13.877   0.   ]\n",
      " [  1.487   9.937  18.584   0.   ]\n",
      " [  0.     16.097   4.498   1.   ]\n",
      " [  1.13   15.556   6.613   1.   ]\n",
      " [  1.706  16.056   1.593   1.   ]\n",
      " [  3.813  15.314   0.571   1.   ]\n",
      " [  3.111  17.128   1.492   1.   ]\n",
      " [  0.967  17.182   0.      1.   ]\n",
      " [ 20.495   2.11    6.24    2.   ]\n",
      " [ 20.862   0.363   5.119   2.   ]\n",
      " [ 20.79    0.      0.994   2.   ]\n",
      " [ 20.408   2.752   0.      2.   ]\n",
      " [ 22.189   3.645   2.797   2.   ]\n",
      " [ 21.256   0.191   0.317   2.   ]]\n",
      "[1, 0, 0, 2, 1, 0, 1, 1, 1, 0, 2, 2, 2, 2, 0, 2, 2, 1, 0, 2]\n",
      "[1, 2, 2, 0, 1, 2, 1, 1, 1, 2, 0, 0, 0, 0, 2, 0, 0, 1, 2, 0]\n",
      "[[ 0.18235397  0.7381213   2.43317361]]\n",
      "\n",
      "\n",
      "[[ 5.108  0.693  0.243  0.   ]\n",
      " [ 5.055  1.288  0.03   0.   ]\n",
      " [ 4.092  1.298  1.023  0.   ]\n",
      " [ 4.17   1.34   0.912  0.   ]\n",
      " [ 4.694  1.389  0.395  0.   ]\n",
      " [ 3.772  2.029  1.053  0.   ]\n",
      " [ 3.396  3.584  0.694  0.   ]\n",
      " [ 3.827  1.252  1.263  0.   ]\n",
      " [ 0.906  0.     3.212  1.   ]\n",
      " [ 1.306  0.889  2.933  1.   ]\n",
      " [ 0.333  1.345  3.339  1.   ]\n",
      " [ 0.143  2.96   3.186  1.   ]\n",
      " [ 0.293  2.227  3.243  1.   ]\n",
      " [ 0.     0.737  3.528  1.   ]\n",
      " [ 0.694  7.925  0.131  2.   ]\n",
      " [ 0.561  8.094  0.     2.   ]\n",
      " [ 0.192  8.417  0.034  2.   ]\n",
      " [ 0.087  8.395  0.363  2.   ]\n",
      " [ 0.348  8.156  0.355  2.   ]\n",
      " [ 0.123  8.462  0.072  2.   ]]\n",
      "[2, 1, 1, 0, 2, 1, 2, 2, 2, 1, 0, 0, 0, 0, 1, 0, 1, 2, 1, 0]\n",
      "[1, 2, 2, 0, 1, 2, 1, 1, 1, 2, 0, 0, 0, 0, 2, 0, 0, 1, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "# a 20x4 example in which there are explicit types (that are unknown to the algorithm)\n",
    "n = 20\n",
    "types = np.trunc(np.random.random_sample(n)*3)\n",
    "means = [[1,2,3,4],[1,5,4,1],[10,1,2,8]]\n",
    "stddevs = 0.5 * np.ones([3,4])\n",
    "def gen(t):\n",
    "    mean = means[t]\n",
    "    sd = stddevs[t]\n",
    "    sz = len(mean)\n",
    "    return list(np.random.randn(sz) * sd + mean)\n",
    "# generate a data matrix\n",
    "M = np.round(map(gen, map(int, types)),3)\n",
    "\n",
    "M_tfidf = sklearn.feature_extraction.text.TfidfTransformer().fit_transform(M).toarray()\n",
    "print M\n",
    "print\n",
    "print M_tfidf\n",
    "print\n",
    "print\n",
    "\n",
    "# run NMF\n",
    "fit = sklearn.decomposition.NMF(n_components=3).fit(M)\n",
    "H = fit.components_\n",
    "W = fit.transform(M)\n",
    "#print H\n",
    "results = list(np.concatenate((10*W,map(lambda x:[x],types)), 1))\n",
    "results.sort(key = lambda row : row[3])\n",
    "results = np.array(map(lambda row : list(np.round(row,3)), results))\n",
    "print results\n",
    "#print map(lambda row : np.argmax(np.round(list(row[0:3]),3)), results)\n",
    "# predicted clusters\n",
    "print map(lambda row : np.argmax(np.round(list(row[0:3]),3)), W)\n",
    "# true clusters\n",
    "print map(int, types)\n",
    "# use fit from above to classify a new point\n",
    "print fit.transform([[1,2,4,5]])\n",
    "\n",
    "print\n",
    "print\n",
    "\n",
    "fit = sklearn.decomposition.NMF(n_components=3).fit(M_tfidf)\n",
    "H = fit.components_\n",
    "W = fit.transform(M_tfidf)\n",
    "#print H\n",
    "results = list(np.concatenate((10*W,map(lambda x:[x],types)), 1))\n",
    "results.sort(key = lambda row : row[3])\n",
    "results = np.array(map(lambda row : list(np.round(row,3)), results))\n",
    "print results\n",
    "#print map(lambda row : np.argmax(np.round(list(row[0:3]),3)), results)\n",
    "# predicted clusters\n",
    "print map(lambda row : np.argmax(np.round(list(row[0:3]),3)), W)\n",
    "# true clusters\n",
    "print map(int, types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Fitting Model with Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 49.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# read in data from csv\n",
    "noun_train_mat = np.loadtxt(\"noun_train_mat.csv\", delimiter = \",\")\n",
    "# use TF-IDF to scale each document's vector to have norm 1 and place a lower weight on very common words\n",
    "tf_idf_fit = sklearn.feature_extraction.text.TfidfTransformer().fit(noun_train_mat)\n",
    "noun_train_mat = tf_idf_fit.transform(noun_train_mat).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 52s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Madhu\\Anaconda\\lib\\site-packages\\sklearn\\decomposition\\nmf.py:532: UserWarning: Iteration limit reached during fit. Solving for W exactly.\n",
      "  warnings.warn(\"Iteration limit reached during fit. Solving for W exactly.\")\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# compute NMF fit\n",
    "NMF_fit = sklearn.decomposition.NMF(n_components=14, init='nndsvda').fit(noun_train_mat)\n",
    "H = NMF_fit.components_\n",
    "W = NMF_fit.transform(noun_train_mat)\n",
    "# contains a tuple (i,j) if document i is in cluster j, for each document\n",
    "clusters = map(np.argmax, W)\n",
    "# list of the documents in each cluster\n",
    "cluster_lists = [[i for i,j in enumerate(clusters) if j==cluster] for cluster in range(14)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents per category: [100, 185, 74, 59, 68, 145, 44, 68, 88, 53, 35, 13, 26, 152]\n"
     ]
    }
   ],
   "source": [
    "# classify each document into the category that fits it best\n",
    "print 'Number of documents per category:', [sum([x==i for x in clusters]) for i in range(14)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Key Words for Each Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load vocab from csv\n",
    "noun_vocab = np.loadtxt(\"noun_vocab.csv\", delimiter=\",\", dtype=\"str\")\n",
    "noun_vocab = [(int(i),j) for i,j in noun_vocab]\n",
    "id2noun = dict(noun_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['court', 'district', 'petitioner', 'appeal', 'habea'],\n",
       " ['act', 'respondent', 'action', 'court', 'title'],\n",
       " ['labor', 'union', 'board', 'employee', 'employer'],\n",
       " ['warrant', 'police', 'search', 'officer', 'petitioner'],\n",
       " ['property', 'tax', 'revenue', 'income', 'busines'],\n",
       " ['jury', 'sentence', 'defendant', 'trial', 'offense'],\n",
       " ['carrier', 'railroad', 'icc', 'rate', 'commerce'],\n",
       " ['attorney', 'alien', 'general', 'brief', 'cause'],\n",
       " ['commission', 'price', 'act', 'sale', 'company'],\n",
       " ['contract', 'arbitration', 'agreement', 'government', 'contractor'],\n",
       " ['plan', 'board', 'school', 'student', 'education'],\n",
       " ['patent', 'invention', 'art', 'claim', 'royalty'],\n",
       " ['master', 'decree', 'orig ', 'entry', 'boundary'],\n",
       " ['state', 'court', 'statute', 'law', 'amendment']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the 5 most important words for each category\n",
    "num_best = 5\n",
    "best_indices = map(lambda v : list(bn.argpartsort(-v,num_best)[0:num_best]), H)\n",
    "best_words = [[id2noun[i] for i in lst] for lst in best_indices]\n",
    "best_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['narcotic', 'fdca', 'middleman', 'narcotic', 'city the', 'middleman', 'city the', 'middleman', 'fdca', 'narcotic', 'city the', 'fdca', 'city the', 'city the']\n"
     ]
    }
   ],
   "source": [
    "# can probably ignore this stuff -- it's a different way to figure out the best words in each category, but it's not working\n",
    "# and produces nonsense\n",
    "width = len(M[0])\n",
    "best_words = np.zeros(14)\n",
    "for i in range(14):\n",
    "    # arbitrary high initial value\n",
    "    w = np.ones(width) * 5000\n",
    "    #w_val = np.ones(width)\n",
    "    for j in range(width):\n",
    "        for k in range(14):\n",
    "            if i <> k and H[k][j] > 0:\n",
    "                if w[j] >= float(H[i][j])/H[k][j]:\n",
    "                    w[j] = float(H[i][j])/H[k][j]\n",
    "    best_words[i] = np.argmax(w)\n",
    "print map(lambda x : id2noun[int(x)], best_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Comparing with Supreme Court Database's Topic Areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents per category: [238, 188, 96, 36, 10, 8, 64, 242, 131, 56, 10, 30, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# read in SC Database's issue areas from csv\n",
    "noun_train_issue_areas = np.loadtxt(\"noun_train_issue_areas.csv\", delimiter = \",\", dtype=\"int\")\n",
    "# zero-index the array\n",
    "noun_train_issue_areas = noun_train_issue_areas - 1\n",
    "print 'Number of documents per category:', [sum([x==i for x in noun_train_issue_areas]) for i in range(14)]\n",
    "\n",
    "noun_train_issue_areas_dummy = np.array(map(lambda area : np.eye(1,14,area)[0], noun_train_issue_areas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 7, 6, 0, 7, 0, 7, 1, 7, 7, 1, 7, 10, 2]\n",
      "[8, 7, 6, 0, 7, 0, 7, 1, 7, 7, 1, 7, 10, 2]\n",
      "[3, 10, 13, 13, 13, 7, 2, 11, 0, 12, 12, 4, 4, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([ 0.27,  0.17,  0.07,  0.03,  0.  ,  0.  ,  0.01,  0.06,  0.35,\n",
       "         0.03,  0.  ,  0.01,  0.  ,  0.  ]),\n",
       " array([ 0.03783784,  0.27567568,  0.05945946,  0.04324324,  0.01621622,\n",
       "         0.        ,  0.01621622,  0.36216216,  0.15675676,  0.02702703,\n",
       "         0.        ,  0.00540541,  0.        ,  0.        ]),\n",
       " array([ 0.01351351,  0.01351351,  0.10810811,  0.01351351,  0.01351351,\n",
       "         0.        ,  0.60810811,  0.04054054,  0.08108108,  0.10810811,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ]),\n",
       " array([ 0.89830508,  0.05084746,  0.        ,  0.01694915,  0.01694915,\n",
       "         0.        ,  0.        ,  0.        ,  0.01694915,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ]),\n",
       " array([ 0.13235294,  0.04411765,  0.        ,  0.        ,  0.        ,\n",
       "         0.01470588,  0.        ,  0.38235294,  0.04411765,  0.02941176,\n",
       "         0.        ,  0.33823529,  0.01470588,  0.        ]),\n",
       " array([ 0.73103448,  0.12413793,  0.03448276,  0.03448276,  0.00689655,\n",
       "         0.        ,  0.        ,  0.04137931,  0.02758621,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ]),\n",
       " array([ 0.        ,  0.04545455,  0.        ,  0.04545455,  0.        ,\n",
       "         0.        ,  0.04545455,  0.68181818,  0.13636364,  0.04545455,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ]),\n",
       " array([ 0.23529412,  0.33823529,  0.04411765,  0.04411765,  0.01470588,\n",
       "         0.07352941,  0.01470588,  0.07352941,  0.13235294,  0.02941176,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ]),\n",
       " array([ 0.03409091,  0.01136364,  0.06818182,  0.01136364,  0.        ,\n",
       "         0.        ,  0.06818182,  0.61363636,  0.09090909,  0.07954545,\n",
       "         0.        ,  0.02272727,  0.        ,  0.        ]),\n",
       " array([ 0.05660377,  0.13207547,  0.03773585,  0.03773585,  0.        ,\n",
       "         0.        ,  0.09433962,  0.43396226,  0.05660377,  0.13207547,\n",
       "         0.        ,  0.01886792,  0.        ,  0.        ]),\n",
       " array([ 0.05714286,  0.57142857,  0.28571429,  0.02857143,  0.        ,\n",
       "         0.        ,  0.02857143,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.02857143,  0.        ,  0.        ]),\n",
       " array([ 0.07692308,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.69230769,  0.07692308,  0.15384615,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ]),\n",
       " array([ 0.03846154,  0.07692308,  0.        ,  0.03846154,  0.        ,\n",
       "         0.        ,  0.        ,  0.07692308,  0.07692308,  0.30769231,\n",
       "         0.38461538,  0.        ,  0.        ,  0.        ]),\n",
       " array([ 0.05921053,  0.26315789,  0.28947368,  0.05263158,  0.01973684,\n",
       "         0.01315789,  0.        ,  0.07236842,  0.15789474,  0.06578947,\n",
       "         0.        ,  0.00657895,  0.        ,  0.        ])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_mat = map(lambda r : map(int, r), np.zeros((14,14)))\n",
    "for i,j in zip(clusters, noun_train_issue_areas):\n",
    "    compare_mat[i][j] += 1\n",
    "compare_mat = map(lambda row : map(float,row) / sum(row), np.array(compare_mat))\n",
    "\n",
    "assignments = -1 * np.ones(14)\n",
    "compare_mat_flat = [x for lst in compare_mat for x in lst]\n",
    "while min(assignments) == -1:\n",
    "    max_ind = np.argmax(compare_mat_flat)\n",
    "    row = max_ind / 14\n",
    "    if assignments[row] == -1:\n",
    "        column = max_ind - 14 * row\n",
    "        assignments[row] = column\n",
    "    else:\n",
    "        compare_mat_flat[max_ind] = -1\n",
    "#5 / 2\n",
    "assignments = map(int, assignments)\n",
    "print assignments\n",
    "print map(np.argmax, compare_mat)\n",
    "print map(np.argmax, np.array(compare_mat).T)\n",
    "\n",
    "compare_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 8, 1: 4, 2: 6, 3: 0, 4: 11, 5: 13, 6: 7, 7: 5, 8: 12, 9: 3, 10: 1, 11: 9, 12: 10, 13: 2}\n"
     ]
    }
   ],
   "source": [
    "### NOTE: probably shouldn't run this, since it has very low accuracy\n",
    "# use Hungarian algorithm (computing assignments with cost minimization) to produce optimal assignments. \n",
    "import munkres\n",
    "m = munkres.Munkres()\n",
    "assignments = dict(m.compute(-np.array(compare_mat)))\n",
    "print assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.490990990991\n"
     ]
    }
   ],
   "source": [
    "new_clusters = map(lambda cluster : assignments[cluster], clusters)\n",
    "correct = map(lambda (c1,c2) : c1==c2, zip(new_clusters, noun_train_issue_areas))\n",
    "print float(sum(correct)) / len(correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Applying Model to Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# read in test data from csv\n",
    "noun_test_mat = np.loadtxt(\"noun_test_mat.csv\", delimiter = \",\")\n",
    "# use TF-IDF to scale each document's vector to have norm 1 and place a lower weight on very common words\n",
    "noun_test_mat = tf_idf_fit.transform(noun_test_mat).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use NMF fit from training data to cluster test observations\n",
    "W_test = NMF_fit.transform(noun_test_mat)\n",
    "clusters_test = map(np.argmax, W_test)\n",
    "cluster_lists_test = [[i for i,j in enumerate(clusters_test) if j==cluster] for cluster in range(14)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[21, 32, 40, 41, 47, 68, 76, 88, 91, 97, 98, 100, 101, 102, 107, 110, 130, 175, 181, 205, 221, 224, 225, 226, 227, 254, 267, 270, 271, 273, 288, 289, 312, 321, 324, 327, 335, 340, 364, 369, 381, 383, 388, 391, 404, 423, 446, 457, 459], [3, 10, 13, 24, 30, 43, 44, 48, 50, 75, 77, 80, 92, 104, 108, 112, 116, 135, 136, 139, 145, 152, 154, 165, 169, 179, 199, 210, 233, 239, 242, 245, 246, 247, 264, 266, 275, 276, 279, 295, 301, 305, 314, 315, 318, 320, 334, 337, 338, 343, 344, 345, 360, 371, 372, 374, 378, 386, 387, 390, 394, 401, 403, 408, 409, 428, 431, 451, 458, 465, 470, 471, 473, 480, 481, 482, 484], [12, 19, 26, 59, 62, 64, 103, 120, 144, 149, 180, 182, 193, 194, 240, 265, 272, 291, 304, 307, 309, 313, 319, 380, 389, 454, 460, 461, 467, 476], [22, 36, 54, 122, 132, 160, 162, 219, 228, 241, 244, 310, 347, 356, 368, 405, 407, 412, 424, 432, 453, 472], [6, 11, 23, 25, 33, 35, 38, 58, 83, 114, 115, 118, 119, 126, 129, 147, 148, 150, 166, 191, 197, 209, 211, 232, 252, 253, 269, 285, 290, 303, 339, 375, 379, 402, 413, 443, 462, 474, 479, 483], [5, 28, 34, 51, 52, 53, 55, 56, 65, 71, 78, 79, 84, 90, 113, 124, 140, 143, 146, 159, 167, 171, 174, 176, 183, 187, 202, 203, 204, 206, 213, 216, 218, 230, 234, 236, 248, 258, 261, 262, 268, 277, 282, 292, 297, 298, 299, 317, 325, 332, 341, 361, 362, 366, 370, 399, 406, 410, 411, 422, 425, 435, 438, 444, 448, 450, 464, 466, 468, 478], [14, 39, 60, 61, 63, 67, 87, 121, 127, 178, 208, 260, 263, 363, 455], [0, 20, 37, 42, 57, 125, 151, 161, 173, 177, 189, 220, 231, 235, 237, 249, 329, 346, 350, 354, 359, 365, 376, 416, 419, 429, 437, 440, 452], [8, 29, 66, 73, 74, 81, 94, 105, 117, 123, 131, 155, 156, 164, 185, 195, 196, 207, 214, 223, 251, 255, 283, 287, 311, 322, 326, 342, 348, 352, 367, 373, 385, 398, 433, 434, 441, 456, 469, 475], [15, 31, 106, 128, 133, 138, 257, 274, 284, 286, 296, 306, 316, 331, 351, 353, 400, 421, 449, 463], [1, 72, 153, 172, 217, 333, 395, 417, 427, 430], [9, 46, 82, 95, 142, 188, 293, 414, 420, 445], [27, 111, 157, 163, 170, 192, 198, 201, 294, 300, 349, 355, 382, 392], [2, 4, 7, 16, 17, 18, 45, 49, 69, 70, 85, 86, 89, 93, 96, 99, 109, 134, 137, 141, 158, 168, 184, 186, 190, 200, 212, 215, 222, 229, 238, 243, 250, 256, 259, 278, 280, 281, 302, 308, 323, 328, 330, 336, 357, 358, 377, 384, 393, 396, 397, 415, 418, 426, 436, 439, 442, 447, 477]]\n"
     ]
    }
   ],
   "source": [
    "print cluster_lists_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Compare Test Data Results to SC Database Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents per category: [104, 95, 26, 22, 3, 7, 18, 96, 63, 25, 5, 17, 4, 0]\n"
     ]
    }
   ],
   "source": [
    "# read in SC Database's issue areas from csv\n",
    "noun_test_issue_areas = np.loadtxt(\"noun_test_issue_areas.csv\", delimiter = \",\", dtype=\"int\")\n",
    "# zero-index the array\n",
    "noun_test_issue_areas = noun_test_issue_areas - 1\n",
    "print 'Number of documents per category:', [sum([x==i for x in noun_test_issue_areas]) for i in range(14)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.455670103093\n"
     ]
    }
   ],
   "source": [
    "new_clusters_test = map(lambda cluster : assignments[cluster], clusters_test)\n",
    "correct_test = map(lambda (c1,c2) : c1==c2, zip(new_clusters_test, noun_test_issue_areas))\n",
    "print float(sum(correct_test)) / len(correct_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
