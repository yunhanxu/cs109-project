{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn.decomposition\n",
    "import sklearn.feature_extraction\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import bottleneck as bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.07482336  0.45068959  0.77361709]\n",
      " [ 0.          0.97165209  1.21900555]]\n",
      "\n",
      "[[ 0.32248028  0.71847941]\n",
      " [ 0.65056118  0.73251539]\n",
      " [ 0.9764969   0.82779544]\n",
      " [ 1.30330612  0.56177196]\n",
      " [ 1.62465347 -0.        ]\n",
      " [ 1.95092579  0.09280544]]\n"
     ]
    }
   ],
   "source": [
    "# arbitrary 6x3 matrix (constructed from an example at http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html with a random column concatenated)\n",
    "M = np.concatenate((np.array([[1,1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]]),map(lambda x : [x], np.round(np.random.random_sample(6),1)+1)),1)\n",
    "M\n",
    "H = sklearn.decomposition.NMF(n_components=2).fit(M).components_\n",
    "W = sklearn.decomposition.NMF(n_components=2).fit_transform(M)\n",
    "print H\n",
    "print\n",
    "print W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  9.765   1.056   2.006   7.364]\n",
      " [  1.093   1.709   3.668   3.843]\n",
      " [  1.073   2.829   2.908   3.373]\n",
      " [ 10.009   0.776   1.864   7.537]\n",
      " [  9.836   0.746   2.556   8.405]\n",
      " [  9.506   0.247   1.486   7.809]\n",
      " [  0.907   2.303   2.934   4.137]\n",
      " [  9.924   0.81    1.19    8.159]\n",
      " [  0.68    1.667   2.758   3.854]\n",
      " [  8.266   0.659   1.633   8.18 ]\n",
      " [ 11.108   1.151   1.266   7.804]\n",
      " [ -0.012   5.182   4.274   0.99 ]\n",
      " [  9.99    1.43    1.487   7.603]\n",
      " [  0.958   4.137   4.366   1.021]\n",
      " [  0.613   1.511   4.101   4.063]\n",
      " [  1.13    2.417   2.68    3.508]\n",
      " [  1.138   2.017   3.046   3.967]\n",
      " [  9.685   0.622   3.102   7.705]\n",
      " [  1.146   1.745   2.604   3.718]\n",
      " [  0.805   2.019   3.369   3.315]]\n",
      "\n",
      "[[ 0.78504574  0.08489588  0.16127002  0.59202015]\n",
      " [ 0.19220391  0.30052743  0.64501733  0.67579106]\n",
      " [ 0.19929177  0.52543936  0.54011228  0.62647824]\n",
      " [ 0.78866512  0.06114538  0.14687499  0.59388241]\n",
      " [ 0.74463763  0.05647618  0.19350283  0.6363033 ]\n",
      " [ 0.76697782  0.01992884  0.11989575  0.63005784]\n",
      " [ 0.16071463  0.40807695  0.51988613  0.73305008]\n",
      " [ 0.76764975  0.06265581  0.0920499   0.63112196]\n",
      " [ 0.13413202  0.32882071  0.5440237   0.76021296]\n",
      " [ 0.70278311  0.0560288   0.1388392   0.69547131]\n",
      " [ 0.81182513  0.08412052  0.09252526  0.5703532 ]\n",
      " [-0.00176737  0.76321141  0.62948004  0.14580843]\n",
      " [ 0.78522358  0.11239937  0.11687963  0.59760309]\n",
      " [ 0.15512888  0.66990413  0.70698609  0.16533046]\n",
      " [ 0.10218787  0.25188561  0.68364187  0.67730722]\n",
      " [ 0.21906774  0.46857232  0.51955888  0.68007931]\n",
      " [ 0.20647089  0.3659506   0.55264528  0.71974518]\n",
      " [ 0.75817961  0.04869259  0.24283667  0.60317748]\n",
      " [ 0.22937126  0.34926078  0.52118915  0.74415563]\n",
      " [ 0.15473969  0.38809867  0.64760001  0.63721996]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Negative values in data passed to NMF.fit",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-b76b8b636cfa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# run NMF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mfit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecomposition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNMF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[0mH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Madhu\\Anaconda\\lib\\site-packages\\sklearn\\decomposition\\nmf.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m         \"\"\"\n\u001b[1;32m--> 551\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    552\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Madhu\\Anaconda\\lib\\site-packages\\sklearn\\decomposition\\nmf.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m    475\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 476\u001b[1;33m         \u001b[0mcheck_non_negative\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"NMF.fit\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    478\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Madhu\\Anaconda\\lib\\site-packages\\sklearn\\decomposition\\nmf.pyc\u001b[0m in \u001b[0;36mcheck_non_negative\u001b[1;34m(X, whom)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Negative values in data passed to %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mwhom\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Negative values in data passed to NMF.fit"
     ]
    }
   ],
   "source": [
    "# a 20x4 example in which there are explicit types (that are unknown to the algorithm)\n",
    "n = 20\n",
    "types = np.trunc(np.random.random_sample(n)*3)\n",
    "means = [[1,2,3,4],[1,5,4,1],[10,1,2,8]]\n",
    "stddevs = 0.5 * np.ones([3,4])\n",
    "def gen(t):\n",
    "    mean = means[t]\n",
    "    sd = stddevs[t]\n",
    "    sz = len(mean)\n",
    "    return list(np.random.randn(sz) * sd + mean)\n",
    "# generate a data matrix\n",
    "M = np.round(map(gen, map(int, types)),3)\n",
    "\n",
    "M_tfidf = sklearn.feature_extraction.text.TfidfTransformer().fit_transform(M).toarray()\n",
    "print M\n",
    "print\n",
    "print M_tfidf\n",
    "print\n",
    "print\n",
    "\n",
    "# run NMF\n",
    "fit = sklearn.decomposition.NMF(n_components=3).fit(M)\n",
    "H = fit.components_\n",
    "W = fit.transform(M)\n",
    "#print H\n",
    "results = list(np.concatenate((10*W,map(lambda x:[x],types)), 1))\n",
    "results.sort(key = lambda row : row[3])\n",
    "results = np.array(map(lambda row : list(np.round(row,3)), results))\n",
    "print results\n",
    "#print map(lambda row : np.argmax(np.round(list(row[0:3]),3)), results)\n",
    "# predicted clusters\n",
    "print map(lambda row : np.argmax(np.round(list(row[0:3]),3)), W)\n",
    "# true clusters\n",
    "print map(int, types)\n",
    "# use fit from above to classify a new point\n",
    "print fit.transform([[1,2,4,5]])\n",
    "\n",
    "print\n",
    "print\n",
    "\n",
    "fit = sklearn.decomposition.NMF(n_components=3).fit(M_tfidf)\n",
    "H = fit.components_\n",
    "W = fit.transform(M_tfidf)\n",
    "#print H\n",
    "results = list(np.concatenate((10*W,map(lambda x:[x],types)), 1))\n",
    "results.sort(key = lambda row : row[3])\n",
    "results = np.array(map(lambda row : list(np.round(row,3)), results))\n",
    "print results\n",
    "#print map(lambda row : np.argmax(np.round(list(row[0:3]),3)), results)\n",
    "# predicted clusters\n",
    "print map(lambda row : np.argmax(np.round(list(row[0:3]),3)), W)\n",
    "# true clusters\n",
    "print map(int, types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# read in data from csv\n",
    "M = np.loadtxt(\"mat.csv\", delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 746 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# use TF-IDF to scale each document's vector to have norm 1 \n",
    "M_tfidf = sklearn.feature_extraction.text.TfidfTransformer().fit_transform(M).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 33s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Madhu\\Anaconda\\lib\\site-packages\\sklearn\\decomposition\\nmf.py:532: UserWarning: Iteration limit reached during fit. Solving for W exactly.\n",
      "  warnings.warn(\"Iteration limit reached during fit. Solving for W exactly.\")\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fit = sklearn.decomposition.NMF(n_components=14).fit(M_tfidf)\n",
    "H = fit.components_\n",
    "W = fit.transform(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents per category: [301, 94, 99, 151, 304, 57, 218, 116, 141, 35, 65, 41, 30, 52]\n",
      "\n",
      "[(2, 10), (23, 10), (99, 10), (107, 10), (125, 10), (144, 10), (196, 10), (207, 10), (242, 10), (254, 10), (310, 10), (326, 10), (342, 10), (382, 10), (393, 10), (429, 10), (435, 10), (454, 10), (473, 10), (481, 10), (505, 10), (532, 10), (541, 10), (543, 10), (610, 10), (641, 10), (668, 10), (682, 10), (770, 10), (844, 10), (869, 10), (870, 10), (883, 10), (887, 10), (897, 10), (917, 10), (960, 10), (969, 10), (1008, 10), (1021, 10), (1042, 10), (1057, 10), (1097, 10), (1119, 10), (1139, 10), (1140, 10), (1148, 10), (1153, 10), (1163, 10), (1178, 10), (1254, 10), (1283, 10), (1286, 10), (1358, 10), (1406, 10), (1410, 10), (1428, 10), (1442, 10), (1489, 10), (1543, 10), (1582, 10), (1611, 10), (1647, 10), (1653, 10), (1677, 10)]\n"
     ]
    }
   ],
   "source": [
    "# classify each document into the category that fits it best\n",
    "clusters = [(i,j) for i,j in enumerate(map(np.argmax, W))]\n",
    "print 'Number of documents per category:', [sum([x[1]==i for x in clusters]) for i in range(14)]\n",
    "print\n",
    "print [(x,y) for x,y in clusters if y == 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[407, 580, 371, 374, 915, 1053, 10, 124, 81, 483, 80, 594, 2037, 207]\n",
      "Wall time: 5.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "width = len(M[0])\n",
    "best_words = np.zeros(14)\n",
    "for i in range(14):\n",
    "    w = np.ones(width) * 5000\n",
    "    #w_val = np.ones(width)\n",
    "    for j in range(width):\n",
    "        for k in range(14):\n",
    "            if i <> k and H[k][j] > 0:\n",
    "                if w[j] >= float(H[i][j])/H[k][j]:\n",
    "                    w[j] = float(H[i][j])/H[k][j]\n",
    "    best_words[i] = np.argmax(w)\n",
    "print map(int, best_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12504, 4478, 11714, 1097, 11506], [9864, 580, 11520, 4206, 11309], [13365, 11700, 13170, 5498, 1283], [4503, 955, 4504, 11167, 9748], [6666, 12504, 3721, 13610, 10459], [12786, 5053, 7679, 10869, 13263], [11137, 183, 12504, 1631, 11264], [2032, 11264, 11202, 8839, 6830], [4503, 8705, 720, 6028, 13198], [10541, 4206, 4494, 13834, 1828], [12372, 7580, 8577, 612, 11224], [11214, 5098, 2172, 12622, 9520], [7304, 355, 10144, 10983, 6712], [11463, 12171, 2625, 4860, 6120]]\n"
     ]
    }
   ],
   "source": [
    "# output the indices of the 5 most important words for each category\n",
    "#print map(np.argmax, H)\n",
    "num_best = 5\n",
    "print map(lambda v : list(bn.argpartsort(-v,num_best)[0:num_best]), H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
