{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn.decomposition\n",
    "import sklearn.feature_extraction\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import bottleneck as bn\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Fake Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  3.08686011e+00   5.19450651e-01   6.66252184e-01]\n",
      " [  1.70115150e-06   7.69621201e-01   1.34443684e+00]]\n",
      "\n",
      "[[ 0.32250324  1.21106046]\n",
      " [ 0.64847343  0.81094823]\n",
      " [ 0.97662223  0.47380269]\n",
      " [ 1.29791641  0.23617409]\n",
      " [ 1.61546449  0.33672095]\n",
      " [ 1.94355571 -0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# arbitrary 6x3 matrix (constructed from an example at http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html with a random column concatenated)\n",
    "M = np.concatenate((np.array([[1,1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]]),map(lambda x : [x], np.round(np.random.random_sample(6),1)+1)),1)\n",
    "M\n",
    "H = sklearn.decomposition.NMF(n_components=2).fit(M).components_\n",
    "W = sklearn.decomposition.NMF(n_components=2).fit_transform(M)\n",
    "print H\n",
    "print\n",
    "print W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.912   4.764   4.271   1.035]\n",
      " [  1.16    2.034   3.073   3.923]\n",
      " [  0.688   5.084   3.982   1.942]\n",
      " [  0.577   2.52    3.041   3.083]\n",
      " [  1.491   5.25    3.009  -0.091]\n",
      " [  1.322   4.423   3.577   0.976]\n",
      " [  1.546   2.266   2.607   3.338]\n",
      " [ 10.782   0.992   1.418   8.514]\n",
      " [  0.775   5.259   4.441  -0.258]\n",
      " [  9.641   1.76    1.764   8.594]\n",
      " [  1.919   5.395   2.8     0.884]\n",
      " [  0.681   1.33    2.459   3.3  ]\n",
      " [  0.538   5.633   4.439   1.07 ]\n",
      " [  9.833   0.901   1.539   7.052]\n",
      " [  1.205   4.392   3.42    0.557]\n",
      " [  9.67    0.9     1.321   7.563]\n",
      " [  1.188   2.299   3.261   3.432]\n",
      " [  0.425   5.228   3.779   0.846]\n",
      " [ 10.399   0.473   1.429   7.663]\n",
      " [  9.453   0.446   3.176   8.562]]\n",
      "\n",
      "[[ 0.139338    0.72785771  0.65253574  0.1581303 ]\n",
      " [ 0.21067934  0.36941532  0.55811862  0.71249572]\n",
      " [ 0.10149735  0.75001823  0.58744544  0.28649398]\n",
      " [ 0.11440691  0.49966277  0.60296607  0.61129378]\n",
      " [ 0.23921788  0.84231646  0.48276766 -0.01460015]\n",
      " [ 0.22327349  0.74700351  0.60412199  0.16483731]\n",
      " [ 0.30637257  0.44905579  0.51663215  0.66149525]\n",
      " [ 0.77866237  0.07164098  0.10240616  0.61487028]\n",
      " [ 0.11180718  0.75870188  0.6406912  -0.03722097]\n",
      " [ 0.73295985  0.13380452  0.13410862  0.65336137]\n",
      " [ 0.29820996  0.83837557  0.43511614  0.13737238]\n",
      " [ 0.15554051  0.30377221  0.56163598  0.75372052]\n",
      " [ 0.07399095  0.77470455  0.61049414  0.14715673]\n",
      " [ 0.80393663  0.07366489  0.12582716  0.57656474]\n",
      " [ 0.21056802  0.76748112  0.59762874  0.0973331 ]\n",
      " [ 0.78110234  0.07269825  0.10670488  0.61090765]\n",
      " [ 0.22019079  0.42610995  0.60441259  0.63610672]\n",
      " [ 0.065185    0.80185213  0.57960964  0.12975648]\n",
      " [ 0.79962377  0.036371    0.10988195  0.58924098]\n",
      " [ 0.71879604  0.03391336  0.24149965  0.65104535]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Negative values in data passed to NMF.fit",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-b76b8b636cfa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# run NMF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mfit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecomposition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNMF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[0mH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Madhu\\Anaconda\\lib\\site-packages\\sklearn\\decomposition\\nmf.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m         \"\"\"\n\u001b[1;32m--> 551\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    552\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Madhu\\Anaconda\\lib\\site-packages\\sklearn\\decomposition\\nmf.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m    475\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 476\u001b[1;33m         \u001b[0mcheck_non_negative\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"NMF.fit\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    478\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Madhu\\Anaconda\\lib\\site-packages\\sklearn\\decomposition\\nmf.pyc\u001b[0m in \u001b[0;36mcheck_non_negative\u001b[1;34m(X, whom)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Negative values in data passed to %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mwhom\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Negative values in data passed to NMF.fit"
     ]
    }
   ],
   "source": [
    "# a 20x4 example in which there are explicit types (that are unknown to the algorithm)\n",
    "n = 20\n",
    "types = np.trunc(np.random.random_sample(n)*3)\n",
    "means = [[1,2,3,4],[1,5,4,1],[10,1,2,8]]\n",
    "stddevs = 0.5 * np.ones([3,4])\n",
    "def gen(t):\n",
    "    mean = means[t]\n",
    "    sd = stddevs[t]\n",
    "    sz = len(mean)\n",
    "    return list(np.random.randn(sz) * sd + mean)\n",
    "# generate a data matrix\n",
    "M = np.round(map(gen, map(int, types)),3)\n",
    "\n",
    "M_tfidf = sklearn.feature_extraction.text.TfidfTransformer().fit_transform(M).toarray()\n",
    "print M\n",
    "print\n",
    "print M_tfidf\n",
    "print\n",
    "print\n",
    "\n",
    "# run NMF\n",
    "fit = sklearn.decomposition.NMF(n_components=3).fit(M)\n",
    "H = fit.components_\n",
    "W = fit.transform(M)\n",
    "#print H\n",
    "results = list(np.concatenate((10*W,map(lambda x:[x],types)), 1))\n",
    "results.sort(key = lambda row : row[3])\n",
    "results = np.array(map(lambda row : list(np.round(row,3)), results))\n",
    "print results\n",
    "#print map(lambda row : np.argmax(np.round(list(row[0:3]),3)), results)\n",
    "# predicted clusters\n",
    "print map(lambda row : np.argmax(np.round(list(row[0:3]),3)), W)\n",
    "# true clusters\n",
    "print map(int, types)\n",
    "# use fit from above to classify a new point\n",
    "print fit.transform([[1,2,4,5]])\n",
    "\n",
    "print\n",
    "print\n",
    "\n",
    "fit = sklearn.decomposition.NMF(n_components=3).fit(M_tfidf)\n",
    "H = fit.components_\n",
    "W = fit.transform(M_tfidf)\n",
    "#print H\n",
    "results = list(np.concatenate((10*W,map(lambda x:[x],types)), 1))\n",
    "results.sort(key = lambda row : row[3])\n",
    "results = np.array(map(lambda row : list(np.round(row,3)), results))\n",
    "print results\n",
    "#print map(lambda row : np.argmax(np.round(list(row[0:3]),3)), results)\n",
    "# predicted clusters\n",
    "print map(lambda row : np.argmax(np.round(list(row[0:3]),3)), W)\n",
    "# true clusters\n",
    "print map(int, types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Fitting Model with Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 32.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# read in data from csv\n",
    "noun_train_mat = np.loadtxt(\"noun_train_mat.csv\", delimiter = \",\")\n",
    "# use TF-IDF to scale each document's vector to have norm 1 and place a lower weight on very common words\n",
    "tf_idf_fit = sklearn.feature_extraction.text.TfidfTransformer().fit(noun_train_mat)\n",
    "noun_train_mat = tf_idf_fit.transform(noun_train_mat).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# compute NMF fit\n",
    "NMF_fit = sklearn.decomposition.NMF(n_components=14, init='nndsvda').fit(noun_train_mat)\n",
    "H = NMF_fit.components_\n",
    "W = NMF_fit.transform(noun_train_mat)\n",
    "# contains a tuple (i,j) if document i is in cluster j, for each document\n",
    "clusters = map(np.argmax, W)\n",
    "# list of the documents in each cluster\n",
    "cluster_lists = [[i for i,j in enumerate(clusters) if j==cluster] for cluster in range(14)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents per category: [100, 185, 74, 59, 68, 145, 44, 68, 88, 53, 35, 13, 26, 152]\n"
     ]
    }
   ],
   "source": [
    "# classify each document into the category that fits it best\n",
    "print 'Number of documents per category:', [sum([x==i for x in clusters]) for i in range(14)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Key Words for Each Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load vocab from csv\n",
    "noun_vocab = np.loadtxt(\"noun_vocab.csv\", delimiter=\",\", dtype=\"str\")\n",
    "noun_vocab = [(int(i),j) for i,j in noun_vocab]\n",
    "id2noun = dict(noun_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['court', 'district', 'petitioner', 'appeal', 'habea'],\n",
       " ['act', 'respondent', 'action', 'court', 'title'],\n",
       " ['labor', 'union', 'board', 'employee', 'employer'],\n",
       " ['warrant', 'police', 'search', 'officer', 'petitioner'],\n",
       " ['property', 'tax', 'revenue', 'income', 'busines'],\n",
       " ['jury', 'sentence', 'defendant', 'trial', 'offense'],\n",
       " ['carrier', 'railroad', 'icc', 'rate', 'commerce'],\n",
       " ['attorney', 'alien', 'general', 'brief', 'cause'],\n",
       " ['commission', 'price', 'act', 'sale', 'company'],\n",
       " ['contract', 'arbitration', 'agreement', 'government', 'contractor'],\n",
       " ['plan', 'board', 'school', 'student', 'education'],\n",
       " ['patent', 'invention', 'art', 'claim', 'royalty'],\n",
       " ['master', 'decree', 'orig ', 'entry', 'boundary'],\n",
       " ['state', 'court', 'statute', 'law', 'amendment']]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find and output the 5 most important words for each category\n",
    "num_best = 5\n",
    "best_indices = map(lambda v : list(bn.argpartsort(-v,num_best)[0:num_best]), H)\n",
    "best_words = [[id2noun[i] for i in lst] for lst in best_indices]\n",
    "best_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['narcotic', 'fdca', 'middleman', 'narcotic', 'city the', 'middleman', 'city the', 'middleman', 'fdca', 'narcotic', 'city the', 'fdca', 'city the', 'city the']\n"
     ]
    }
   ],
   "source": [
    "# can probably ignore this stuff -- it's a different way to figure out the best words in each category, but it's not working\n",
    "# and produces nonsense\n",
    "width = len(M[0])\n",
    "best_words = np.zeros(14)\n",
    "for i in range(14):\n",
    "    # arbitrary high initial value\n",
    "    w = np.ones(width) * 5000\n",
    "    #w_val = np.ones(width)\n",
    "    for j in range(width):\n",
    "        for k in range(14):\n",
    "            if i <> k and H[k][j] > 0:\n",
    "                if w[j] >= float(H[i][j])/H[k][j]:\n",
    "                    w[j] = float(H[i][j])/H[k][j]\n",
    "    best_words[i] = np.argmax(w)\n",
    "print map(lambda x : id2noun[int(x)], best_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Comparing with Supreme Court Database's Topic Areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents per category: [238, 188, 96, 36, 10, 8, 64, 242, 131, 56, 10, 30, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# read in SC Database's issue areas from csv\n",
    "noun_train_issue_areas = np.loadtxt(\"noun_train_issue_areas.csv\", delimiter = \",\", dtype=\"int\")\n",
    "# zero-index the array\n",
    "noun_train_issue_areas = noun_train_issue_areas - 1\n",
    "print 'Number of documents per category:', [sum([x==i for x in noun_train_issue_areas]) for i in range(14)]\n",
    "\n",
    "# convert the nx1 issue areas vector into a nx14 matrix of dummies, in case that's ever useful\n",
    "noun_train_issue_areas_dummy = np.array(map(lambda area : np.eye(1,14,area)[0], noun_train_issue_areas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 7, 6, 0, 7, 0, 7, 1, 7, 7, 1, 7, 10, 2]\n",
      "[3, 10, 13, 13, 13, 7, 2, 11, 0, 12, 12, 4, 4, 0]\n"
     ]
    }
   ],
   "source": [
    "# create a 14x14 matrix (where each row is an NMF cluster and each column is an SCDB cluster) measuring the degree of\n",
    "# related-ness between each cluster pair\n",
    "compare_mat = map(lambda r : map(int, r), np.zeros((14,14)))\n",
    "# first, assign the (i,j) element in the matrix to the number of cases in NMF cluster i and SCDB cluster j, for each (i,j)\n",
    "for i,j in zip(clusters, noun_train_issue_areas):\n",
    "    compare_mat[i][j] += 1\n",
    "# normalize each row to have a sum of 1\n",
    "compare_mat = map(lambda row : map(float,row) / sum(row), np.array(compare_mat))\n",
    "\n",
    "### commented out because I realized this method of assignment does the same thing as the one-line assignment below\n",
    "#assignments = -1 * np.ones(14)\n",
    "#compare_mat_flat = [x for lst in compare_mat for x in lst]\n",
    "#while min(assignments) == -1:\n",
    "#    max_ind = np.argmax(compare_mat_flat)\n",
    "#    row = max_ind / 14\n",
    "#    if assignments[row] == -1:\n",
    "#        column = max_ind - 14 * row\n",
    "#        assignments[row] = column\n",
    "#    else:\n",
    "#        compare_mat_flat[max_ind] = -1\n",
    "#assignments = map(int, assignments)\n",
    "#print assignments\n",
    "\n",
    "# assign each NMF cluster to a (not necessarily unique) SCDB cluster by picking the highest element of each row of compare_mat\n",
    "assignments = map(np.argmax, compare_mat)\n",
    "print assignments\n",
    "# this would be the equivalent way of assigning each SCDB cluster to an NMF cluster; this is printed here but not used anywhere\n",
    "print map(np.argmax, np.array(compare_mat).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (14,) (16,) (14,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-67df0d529591>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmunkres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmunkres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMunkres\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0massignments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompare_mat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0massignments\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Madhu\\Anaconda\\lib\\site-packages\\munkres.pyc\u001b[0m in \u001b[0;36mcompute\u001b[1;34m(self, cost_matrix)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m         \"\"\"\n\u001b[1;32m--> 388\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    389\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moriginal_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Madhu\\Anaconda\\lib\\site-packages\\munkres.pyc\u001b[0m in \u001b[0;36mpad_matrix\u001b[1;34m(self, matrix, pad_value)\u001b[0m\n\u001b[0;32m    357\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtotal_rows\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mrow_len\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m                 \u001b[1;31m# Row too short. Pad it.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 359\u001b[1;33m                 \u001b[0mnew_row\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpad_value\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtotal_rows\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mrow_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    360\u001b[0m             \u001b[0mnew_matrix\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnew_row\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (14,) (16,) (14,) "
     ]
    }
   ],
   "source": [
    "### NOTE: probably don't run this -- it generates assignments that have very low accuracy\n",
    "# use Hungarian algorithm (which computes assignments that minimize cost) to assign each NMF cluster to an SCDB cluster (without\n",
    "# any collisions)\n",
    "import munkres\n",
    "m = munkres.Munkres()\n",
    "assignments = dict(m.compute(-np.array(compare_mat)))\n",
    "print assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.490990990991\n"
     ]
    }
   ],
   "source": [
    "# use the assignments above to convert the clusters into equivalent SCDB categories for each document\n",
    "new_clusters = map(lambda cluster : assignments[cluster], clusters)\n",
    "# compute and output accuracy rate\n",
    "correct = map(lambda (c1,c2) : c1==c2, zip(new_clusters, noun_train_issue_areas))\n",
    "print float(sum(correct)) / len(correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Use Rand Index to Compute Accuracy on Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each pair of documents is called a true positive ($tp$) if the two documents end up in the same cluster correctly, a true negative ($tn$) if the documents end up in different clusters correctly, a false positive ($fp$) if the documents are placed in the same cluster but should have been in different clusters, and a false negative ($fn$) if the documents are placed in different clusters but should have been in the same cluster.\n",
    "\n",
    "The Rand index (measuring the similarity between the NMF clustering and the SCDB categories) is defined as $\\frac{tp+tn}{tp+tn+fp+fn}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39504 963864 81614 147118\n",
      "0.814355977599\n",
      "Wall time: 1.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if 'cluster_pairs_model' in globals():\n",
    "    del cluster_pairs_model\n",
    "if 'cluster_pairs_actual' in globals():\n",
    "    del cluster_pairs_actual\n",
    "\n",
    "# use itertools to create pairs of documents, using the NMF clusters and SCDB categories\n",
    "cluster_pairs_model = itertools.product(clusters, clusters)\n",
    "cluster_pairs_actual = itertools.product(noun_train_issue_areas, noun_train_issue_areas)\n",
    "# loop through all document pairs to compute tp, tn, fp, and fn\n",
    "(tp,tn,fp,fn) = 0, 0, 0, 0\n",
    "for pair1,pair2 in zip(cluster_pairs_model, cluster_pairs_actual):\n",
    "    if (pair1[0]==pair1[1]):\n",
    "        if (pair2[0]==pair2[1]):\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "    else:\n",
    "        if (pair2[0]==pair2[1]):\n",
    "            fn += 1\n",
    "        else:\n",
    "            tn += 1\n",
    "\n",
    "# output tp, tn, fp, and fn\n",
    "print tp,tn,fp,fn\n",
    "# compute and output Rand index using formula from above\n",
    "rand_index_train = float(tp+tn)/(tp+tn+fp+fn)\n",
    "print rand_index_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Applying Model to Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# read in test data from csv\n",
    "noun_test_mat = np.loadtxt(\"noun_test_mat.csv\", delimiter = \",\")\n",
    "# use TF-IDF fit from training data to scale each document's vector to have norm 1 and place a lower weight on very common words\n",
    "noun_test_mat = tf_idf_fit.transform(noun_test_mat).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# use NMF fit from training data to cluster test observations\n",
    "W_test = NMF_fit.transform(noun_test_mat)\n",
    "clusters_test = map(np.argmax, W_test)\n",
    "cluster_lists_test = [[i for i,j in enumerate(clusters_test) if j==cluster] for cluster in range(14)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Compare Test Data Results to SC Database Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents per category: [104, 95, 26, 22, 3, 7, 18, 96, 63, 25, 5, 17, 4, 0]\n"
     ]
    }
   ],
   "source": [
    "# read in SC Database's issue areas from csv\n",
    "noun_test_issue_areas = np.loadtxt(\"noun_test_issue_areas.csv\", delimiter = \",\", dtype=\"int\")\n",
    "# zero-index the array\n",
    "noun_test_issue_areas = noun_test_issue_areas - 1\n",
    "print 'Number of documents per category:', [sum([x==i for x in noun_test_issue_areas]) for i in range(14)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.455670103093\n"
     ]
    }
   ],
   "source": [
    "# use the assignments above to convert the clusters into equivalent SCDB categories for each document\n",
    "new_clusters_test = map(lambda cluster : assignments[cluster], clusters_test)\n",
    "# compute and output accuracy rate\n",
    "correct_test = map(lambda (c1,c2) : c1==c2, zip(new_clusters_test, noun_test_issue_areas))\n",
    "print float(sum(correct_test)) / len(correct_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Use Rand Index to Compute Accuracy on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7869 184414 15288 27654\n",
      "0.817442873844\n",
      "Wall time: 344 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if 'cluster_test_pairs_model' in globals():\n",
    "    del cluster_test_pairs_model\n",
    "if 'cluster_test_pairs_actual' in globals():\n",
    "    del cluster_test_pairs_actual\n",
    "\n",
    "# use itertools to create pairs of documents, using the NMF clusters and SCDB categories\n",
    "cluster_test_pairs_model = itertools.product(clusters_test, clusters_test)\n",
    "cluster_test_pairs_actual = itertools.product(noun_test_issue_areas, noun_test_issue_areas)\n",
    "# loop through all document pairs to compute tp, tn, fp, and fn\n",
    "(tp,tn,fp,fn) = 0, 0, 0, 0\n",
    "for pair1,pair2 in zip(cluster_test_pairs_model, cluster_test_pairs_actual):\n",
    "    if (pair1[0]==pair1[1]):\n",
    "        if (pair2[0]==pair2[1]):\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "    else:\n",
    "        if (pair2[0]==pair2[1]):\n",
    "            fn += 1\n",
    "        else:\n",
    "            tn += 1\n",
    "\n",
    "# output tp, tn, fp, and fn\n",
    "print tp,tn,fp,fn\n",
    "# compute and output Rand index using formula from above\n",
    "rand_index_test = float(tp+tn)/(tp+tn+fp+fn)\n",
    "print rand_index_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Compute Optimal Number of Clusters on Training Data (based on SCDB)\n",
    "Note: In reality, we should do this using one or more validation sets; I've just done this for the time being."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 31.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_cluster_lst = [5, 10, 14, 15, 20, 25, 30]\n",
    "\n",
    "results = dict()\n",
    "for num_clusters in num_cluster_lst:\n",
    "    NMF_fit = sklearn.decomposition.NMF(n_components=num_clusters, init='nndsvda').fit(noun_train_mat)\n",
    "    H = NMF_fit.components_\n",
    "    W = NMF_fit.transform(noun_train_mat)\n",
    "    # contains a tuple (i,j) if document i is in cluster j, for each document\n",
    "    clusters = map(np.argmax, W)\n",
    "    # list of the documents in each cluster\n",
    "    cluster_lists = [[i for i,j in enumerate(clusters) if j==cluster] for cluster in range(num_clusters)]\n",
    "    \n",
    "    # find the 10 most important words for each category\n",
    "    num_best = 10\n",
    "    best_indices = map(lambda v : list(bn.argpartsort(-v,num_best)[0:num_best]), H)\n",
    "    best_words = [[id2noun[i] for i in lst] for lst in best_indices]\n",
    "    \n",
    "    # create a k x 14 matrix (where each row is an NMF cluster and each column is an SCDB cluster) measuring the degree of\n",
    "    # related-ness between each cluster pair\n",
    "    compare_mat = map(lambda r : map(int, r), np.zeros((num_clusters,14)))\n",
    "    # first, assign the (i,j) element in the matrix to the number of cases in NMF cluster i and SCDB cluster j, for each (i,j)\n",
    "    for i,j in zip(clusters, noun_train_issue_areas):\n",
    "        compare_mat[i][j] += 1\n",
    "    # normalize each row to have a sum of 1\n",
    "    compare_mat = map(lambda row : map(float,row) / sum(row), np.array(compare_mat))\n",
    "    # assign each NMF cluster to a (not necessarily unique) SCDB cluster by picking the highest element of each row of compare_mat\n",
    "    assignments = map(np.argmax, compare_mat)\n",
    "    # use the assignments above to convert the clusters into equivalent SCDB categories for each document\n",
    "    new_clusters = map(lambda cluster : assignments[cluster], clusters)\n",
    "    # compute and output accuracy rate\n",
    "    correct = map(lambda (c1,c2) : c1==c2, zip(new_clusters, noun_train_issue_areas))\n",
    "    accuracy = float(sum(correct)) / len(correct)\n",
    "\n",
    "    if 'cluster_pairs_model' in globals():\n",
    "        del cluster_pairs_model\n",
    "    if 'cluster_pairs_actual' in globals():\n",
    "        del cluster_pairs_actual\n",
    "    # use itertools to create pairs of documents, using the NMF clusters and SCDB categories\n",
    "    cluster_pairs_model = itertools.product(clusters, clusters)\n",
    "    cluster_pairs_actual = itertools.product(noun_train_issue_areas, noun_train_issue_areas)\n",
    "    # loop through all document pairs to compute tp, tn, fp, and fn\n",
    "    (tp,tn,fp,fn) = 0, 0, 0, 0\n",
    "    for pair1,pair2 in zip(cluster_pairs_model, cluster_pairs_actual):\n",
    "        if (pair1[0]==pair1[1]):\n",
    "            if (pair2[0]==pair2[1]):\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "        else:\n",
    "            if (pair2[0]==pair2[1]):\n",
    "                fn += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "    # compute Rand index using formula from above\n",
    "    rand_index = float(tp+tn)/(tp+tn+fp+fn)\n",
    "    \n",
    "    results[num_clusters] = (NMF_fit, clusters, cluster_lists, best_words, new_clusters, accuracy, (tp,tn,fp,fn), rand_index)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
