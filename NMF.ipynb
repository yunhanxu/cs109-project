{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn.decomposition\n",
    "import sklearn.feature_extraction\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import bottleneck as bn\n",
    "import itertools\n",
    "from sklearn.cross_validation import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Fake Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  3.10114034e+00   4.83338531e-01   6.39050455e-01]\n",
      " [  1.74830868e-06   8.29630628e-01   1.46613606e+00]]\n",
      "\n",
      "[[ 0.32461333  0.81094869]\n",
      " [ 0.64345156  0.97058872]\n",
      " [ 0.96691658  0.92818676]\n",
      " [ 1.29143483  0.30334212]\n",
      " [ 1.61052846  0.19772203]\n",
      " [ 1.93555763 -0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# arbitrary 6x3 matrix (constructed from an example at http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html with a random column concatenated)\n",
    "M = np.concatenate((np.array([[1,1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]]),map(lambda x : [x], np.round(np.random.random_sample(6),1)+1)),1)\n",
    "M\n",
    "H = sklearn.decomposition.NMF(n_components=2).fit(M).components_\n",
    "W = sklearn.decomposition.NMF(n_components=2).fit_transform(M)\n",
    "print H\n",
    "print\n",
    "print W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.562   5.141   4.129   1.558]\n",
      " [  1.6     1.386   2.468   4.057]\n",
      " [  9.212   0.866   2.135   8.239]\n",
      " [  0.311   1.533   1.661   3.613]\n",
      " [  1.686   1.816   3.611   4.524]\n",
      " [  0.894   2.137   3.459   3.962]\n",
      " [  0.876   2.042   3.435   3.907]\n",
      " [  1.416   1.555   3.784   3.591]\n",
      " [  0.751   5.301   3.53    1.438]\n",
      " [  0.905   5.607   4.625   1.887]\n",
      " [ 10.037   1.016   1.883   8.234]\n",
      " [  1.515   5.462   3.475   0.451]\n",
      " [  9.206   1.244   2.145   8.246]\n",
      " [  1.541   4.772   4.04    1.036]\n",
      " [  1.31    2.317   3.579   3.279]\n",
      " [ 10.281   0.732   1.87    8.117]\n",
      " [  1.077   5.198   4.031   0.158]\n",
      " [ 10.159   0.503   1.791   8.835]\n",
      " [  0.943   1.846   3.358   4.413]\n",
      " [  0.829   5.285   3.624   1.092]]\n",
      "\n",
      "[[ 0.08266339  0.75617881  0.60732588  0.22916292]\n",
      " [ 0.307742    0.2665815   0.47469203  0.7803183 ]\n",
      " [ 0.73275087  0.06888431  0.16982448  0.65535545]\n",
      " [ 0.07278065  0.35875479  0.38870953  0.84551929]\n",
      " [ 0.26776633  0.28841261  0.57349005  0.71849044]\n",
      " [ 0.15555914  0.3718455   0.60187814  0.6894019 ]\n",
      " [ 0.15485393  0.36097229  0.60721833  0.69065561]\n",
      " [ 0.25174768  0.2764602   0.67274946  0.63843639]\n",
      " [ 0.11426981  0.80658359  0.53711376  0.21880158]\n",
      " [ 0.11965116  0.74130833  0.61147691  0.24948258]\n",
      " [ 0.76283871  0.0772187   0.14311301  0.62580591]\n",
      " [ 0.22734401  0.81963893  0.52146563  0.06767799]\n",
      " [ 0.7303233   0.09868805  0.17016549  0.65416532]\n",
      " [ 0.23626312  0.73163374  0.61940493  0.1588375 ]\n",
      " [ 0.23663922  0.41854433  0.6465128   0.59232062]\n",
      " [ 0.77580329  0.05523665  0.14111002  0.61250806]\n",
      " [ 0.16153435  0.77962446  0.60459142  0.0236977 ]\n",
      " [ 0.74746394  0.03700899  0.13177556  0.65004861]\n",
      " [ 0.15928759  0.31181855  0.56721923  0.74542539]\n",
      " [ 0.12650327  0.80647739  0.55301307  0.16663639]]\n",
      "\n",
      "\n",
      "[[  3.259   3.119  15.768   0.   ]\n",
      " [  0.953   2.444  16.109   0.   ]\n",
      " [  3.028   4.97   19.844   0.   ]\n",
      " [  1.364   5.702  19.496   0.   ]\n",
      " [  1.32    5.491  19.388   0.   ]\n",
      " [  2.115   5.203  17.753   0.   ]\n",
      " [  1.935   7.006  15.375   0.   ]\n",
      " [  1.638   4.55   21.255   0.   ]\n",
      " [  0.     15.386   8.522   1.   ]\n",
      " [  0.39   15.312   6.015   1.   ]\n",
      " [  0.309  16.864   9.81    1.   ]\n",
      " [  1.462  16.66    0.      1.   ]\n",
      " [  1.577  15.273   3.642   1.   ]\n",
      " [  0.405  16.809   1.122   1.   ]\n",
      " [  0.395  15.697   4.604   1.   ]\n",
      " [ 19.843   1.305   6.29    2.   ]\n",
      " [ 21.614   1.756   2.577   2.   ]\n",
      " [ 19.841   2.199   5.901   2.   ]\n",
      " [ 22.062   1.276   1.574   2.   ]\n",
      " [ 22.067   0.      4.988   2.   ]]\n",
      "[1, 2, 0, 2, 2, 2, 2, 2, 1, 1, 0, 1, 0, 1, 2, 0, 1, 0, 2, 1]\n",
      "[1, 0, 2, 0, 0, 0, 0, 0, 1, 1, 2, 1, 2, 1, 0, 2, 1, 2, 0, 1]\n",
      "[[ 0.16206001  0.51135694  2.49435048]]\n",
      "\n",
      "\n",
      "[[ 3.986  3.412  0.426  0.   ]\n",
      " [ 4.941  1.322  0.     0.   ]\n",
      " [ 4.094  2.686  0.877  0.   ]\n",
      " [ 4.383  1.429  1.279  0.   ]\n",
      " [ 4.425  1.414  1.213  0.   ]\n",
      " [ 4.05   2.174  1.244  0.   ]\n",
      " [ 3.587  1.98   2.241  0.   ]\n",
      " [ 4.648  1.645  0.629  0.   ]\n",
      " [ 1.63   0.     5.329  1.   ]\n",
      " [ 1.214  0.283  5.623  1.   ]\n",
      " [ 1.675  0.239  5.239  1.   ]\n",
      " [ 0.     1.097  6.459  1.   ]\n",
      " [ 0.728  1.247  5.841  1.   ]\n",
      " [ 0.204  0.283  6.438  1.   ]\n",
      " [ 0.929  0.283  5.881  1.   ]\n",
      " [ 0.849  8.176  0.188  2.   ]\n",
      " [ 0.468  8.486  0.383  2.   ]\n",
      " [ 0.812  8.145  0.378  2.   ]\n",
      " [ 0.366  8.601  0.323  2.   ]\n",
      " [ 0.671  8.408  0.     2.   ]]\n",
      "[2, 0, 1, 0, 0, 0, 0, 0, 2, 2, 1, 2, 1, 2, 0, 1, 2, 1, 0, 2]\n",
      "[1, 0, 2, 0, 0, 0, 0, 0, 1, 1, 2, 1, 2, 1, 0, 2, 1, 2, 0, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Madhu\\Anaconda\\lib\\site-packages\\sklearn\\decomposition\\nmf.py:532: UserWarning: Iteration limit reached during fit. Solving for W exactly.\n",
      "  warnings.warn(\"Iteration limit reached during fit. Solving for W exactly.\")\n"
     ]
    }
   ],
   "source": [
    "# a 20x4 example in which there are explicit types (that are unknown to the algorithm)\n",
    "n = 20\n",
    "types = np.trunc(np.random.random_sample(n)*3)\n",
    "means = [[1,2,3,4],[1,5,4,1],[10,1,2,8]]\n",
    "stddevs = 0.5 * np.ones([3,4])\n",
    "def gen(t):\n",
    "    mean = means[t]\n",
    "    sd = stddevs[t]\n",
    "    sz = len(mean)\n",
    "    return list(np.random.randn(sz) * sd + mean)\n",
    "# generate a data matrix\n",
    "M = np.round(map(gen, map(int, types)),3)\n",
    "\n",
    "M_tfidf = sklearn.feature_extraction.text.TfidfTransformer().fit_transform(M).toarray()\n",
    "print M\n",
    "print\n",
    "print M_tfidf\n",
    "print\n",
    "print\n",
    "\n",
    "# run NMF\n",
    "fit = sklearn.decomposition.NMF(n_components=3).fit(M)\n",
    "H = fit.components_\n",
    "W = fit.transform(M)\n",
    "#print H\n",
    "results = list(np.concatenate((10*W,map(lambda x:[x],types)), 1))\n",
    "results.sort(key = lambda row : row[3])\n",
    "results = np.array(map(lambda row : list(np.round(row,3)), results))\n",
    "print results\n",
    "#print map(lambda row : np.argmax(np.round(list(row[0:3]),3)), results)\n",
    "# predicted clusters\n",
    "print map(lambda row : np.argmax(np.round(list(row[0:3]),3)), W)\n",
    "# true clusters\n",
    "print map(int, types)\n",
    "# use fit from above to classify a new point\n",
    "print fit.transform([[1,2,4,5]])\n",
    "\n",
    "print\n",
    "print\n",
    "\n",
    "fit = sklearn.decomposition.NMF(n_components=3).fit(M_tfidf)\n",
    "H = fit.components_\n",
    "W = fit.transform(M_tfidf)\n",
    "#print H\n",
    "results = list(np.concatenate((10*W,map(lambda x:[x],types)), 1))\n",
    "results.sort(key = lambda row : row[3])\n",
    "results = np.array(map(lambda row : list(np.round(row,3)), results))\n",
    "print results\n",
    "#print map(lambda row : np.argmax(np.round(list(row[0:3]),3)), results)\n",
    "# predicted clusters\n",
    "print map(lambda row : np.argmax(np.round(list(row[0:3]),3)), W)\n",
    "# true clusters\n",
    "print map(int, types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Fitting Model with Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 30.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# read in training data from csv\n",
    "noun_train_mat = np.loadtxt(\"noun_train_mat.csv\", delimiter = \",\")\n",
    "# use TF-IDF to scale each document's vector to have norm 1 and place a lower weight on very common words\n",
    "tf_idf_fit = sklearn.feature_extraction.text.TfidfTransformer().fit(noun_train_mat)\n",
    "noun_train_mat = tf_idf_fit.transform(noun_train_mat).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# compute NMF fit\n",
    "NMF_fit = sklearn.decomposition.NMF(n_components=14, init='nndsvda').fit(noun_train_mat)\n",
    "H = NMF_fit.components_\n",
    "W = NMF_fit.transform(noun_train_mat)\n",
    "# contains a tuple (i,j) if document i is in cluster j, for each document\n",
    "clusters = map(np.argmax, W)\n",
    "# list of the documents in each cluster\n",
    "cluster_lists = [[i for i,j in enumerate(clusters) if j==cluster] for cluster in range(14)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents per category: [100, 185, 74, 59, 68, 145, 44, 68, 88, 53, 35, 13, 26, 152]\n"
     ]
    }
   ],
   "source": [
    "# classify each document into the category that fits it best\n",
    "print 'Number of documents per category:', [sum([x==i for x in clusters]) for i in range(14)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Key Words for Each Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load vocab from csv\n",
    "noun_vocab = np.loadtxt(\"noun_vocab.csv\", delimiter=\",\", dtype=\"str\")\n",
    "noun_vocab = [(int(i),j) for i,j in noun_vocab]\n",
    "id2noun = dict(noun_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['court', 'district', 'petitioner', 'appeal', 'habea'],\n",
       " ['act', 'respondent', 'action', 'court', 'title'],\n",
       " ['labor', 'union', 'board', 'employee', 'employer'],\n",
       " ['warrant', 'police', 'search', 'officer', 'petitioner'],\n",
       " ['property', 'tax', 'revenue', 'income', 'busines'],\n",
       " ['jury', 'sentence', 'defendant', 'trial', 'offense'],\n",
       " ['carrier', 'railroad', 'icc', 'rate', 'commerce'],\n",
       " ['attorney', 'alien', 'general', 'brief', 'cause'],\n",
       " ['commission', 'price', 'act', 'sale', 'company'],\n",
       " ['contract', 'arbitration', 'agreement', 'government', 'contractor'],\n",
       " ['plan', 'board', 'school', 'student', 'education'],\n",
       " ['patent', 'invention', 'art', 'claim', 'royalty'],\n",
       " ['master', 'decree', 'orig ', 'entry', 'boundary'],\n",
       " ['state', 'court', 'statute', 'law', 'amendment']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find and output the 5 most important words for each category\n",
    "num_best = 5\n",
    "best_indices = map(lambda v : list(bn.argpartsort(-v,num_best)[0:num_best]), H)\n",
    "best_words = [[id2noun[i] for i in lst] for lst in best_indices]\n",
    "best_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['narcotic', 'fdca', 'middleman', 'narcotic', 'city the', 'middleman', 'city the', 'middleman', 'fdca', 'narcotic', 'city the', 'fdca', 'city the', 'city the']\n"
     ]
    }
   ],
   "source": [
    "# can probably ignore this stuff -- it's a different way to figure out the best words in each category, but it's not working\n",
    "# and produces nonsense\n",
    "width = len(M[0])\n",
    "best_words = np.zeros(14)\n",
    "for i in range(14):\n",
    "    # arbitrary high initial value\n",
    "    w = np.ones(width) * 5000\n",
    "    #w_val = np.ones(width)\n",
    "    for j in range(width):\n",
    "        for k in range(14):\n",
    "            if i <> k and H[k][j] > 0:\n",
    "                if w[j] >= float(H[i][j])/H[k][j]:\n",
    "                    w[j] = float(H[i][j])/H[k][j]\n",
    "    best_words[i] = np.argmax(w)\n",
    "print map(lambda x : id2noun[int(x)], best_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Comparing with Supreme Court Database's Topic Areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents per category: [238, 188, 96, 36, 10, 8, 64, 242, 131, 56, 10, 30, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# read in SC Database's issue areas from csv\n",
    "noun_train_issue_areas = np.loadtxt(\"noun_train_issue_areas.csv\", delimiter = \",\", dtype=\"int\")\n",
    "# zero-index the array\n",
    "noun_train_issue_areas = noun_train_issue_areas - 1\n",
    "print 'Number of documents per category:', [sum([x==i for x in noun_train_issue_areas]) for i in range(14)]\n",
    "\n",
    "# convert the nx1 issue areas vector into a nx14 matrix of dummies, in case that's ever useful\n",
    "noun_train_issue_areas_dummy = np.array(map(lambda area : np.eye(1,14,area)[0], noun_train_issue_areas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Assignment method 1: use the cluster assignments computed above to find the best SCDB category match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 7, 6, 0, 7, 0, 7, 1, 7, 7, 1, 7, 10, 2]\n",
      "[3, 10, 13, 13, 13, 7, 2, 11, 0, 12, 12, 4, 4, 0]\n"
     ]
    }
   ],
   "source": [
    "# create a 14x14 matrix (where each row is an NMF cluster and each column is an SCDB cluster) measuring the degree of\n",
    "# related-ness between each cluster pair\n",
    "compare_mat = map(lambda r : map(int, r), np.zeros((14,14)))\n",
    "# first, assign the (i,j) element in the matrix to the number of cases in NMF cluster i and SCDB cluster j, for each (i,j)\n",
    "for i,j in zip(clusters, noun_train_issue_areas):\n",
    "    compare_mat[i][j] += 1\n",
    "# normalize each row to have a sum of 1\n",
    "compare_mat = map(lambda row : map(float,row) / sum(row), np.array(compare_mat))\n",
    "\n",
    "# assign each NMF cluster to a (not necessarily unique) SCDB cluster by picking the highest element of each row of compare_mat\n",
    "assignments = map(np.argmax, compare_mat)\n",
    "print assignments\n",
    "# this would be the equivalent way of assigning each SCDB cluster to an NMF cluster; this is printed here but not used anywhere\n",
    "print map(np.argmax, np.array(compare_mat).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Assignment method 2: use the rows of the W matrix (from NMF) to find the best SCDB category match for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 7, 6, 0, 7, 0, 7, 0, 7, 7, 1, 7, 10, 1]\n"
     ]
    }
   ],
   "source": [
    "# Difference compared to above: Instead of just thinking of each case as getting assigned to a single cluster (as we did above),\n",
    "# we can think of each case as having a coefficient for each cluster (from the W matrix), and increment entries of compare_mat\n",
    "# using those coefficients instead of just incrementing by 1.\n",
    "\n",
    "# create a 14x14 matrix (where each row is an SCDB cluster and each column is an NMF cluster) measuring the degree of\n",
    "# related-ness between each cluster pai\n",
    "compare_mat = np.zeros((14,14))\n",
    "for i,j in zip(W, noun_train_issue_areas):\n",
    "    # increment the jth row of compare_mat by the entire row of W corresponding to the current case\n",
    "    compare_mat[j] = compare_mat[j] + i\n",
    "# transpose that matrix (so that it's in the same format as the matrix in assignment method 1 above)\n",
    "compare_mat = compare_mat.T\n",
    "\n",
    "# assign each NMF cluster to a (not necessarily unique) SCDB cluster by picking the highest element of each row of compare_mat\n",
    "assignments = map(np.argmax, compare_mat)\n",
    "print assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Assignment method 3: use Hungarian algorithm (which computes assignments that minimize cost) to assign each NMF cluster to an SCDB cluster (without any collisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 8, 1: 1, 2: 6, 3: 4, 4: 11, 5: 0, 6: 3, 7: 5, 8: 7, 9: 9, 10: 13, 11: 12, 12: 10, 13: 2}\n"
     ]
    }
   ],
   "source": [
    "### NOTE: probably don't run this -- it generates assignments that have very low accuracy\n",
    "import munkres\n",
    "m = munkres.Munkres()\n",
    "assignments = dict(m.compute(-np.array(compare_mat)))\n",
    "print assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Assign an SCDB category to each case, using an assignment array created above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.490990990991\n"
     ]
    }
   ],
   "source": [
    "# use the assignments above to convert the clusters into equivalent SCDB categories for each document\n",
    "new_clusters = map(lambda cluster : assignments[cluster], clusters)\n",
    "# compute and output accuracy rate\n",
    "correct = map(lambda (c1,c2) : c1==c2, zip(new_clusters, noun_train_issue_areas))\n",
    "print float(sum(correct)) / len(correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Use Rand Index to Compute Accuracy on Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each pair of documents is called a true positive ($tp$) if the two documents end up in the same cluster correctly, a true negative ($tn$) if the documents end up in different clusters correctly, a false positive ($fp$) if the documents are placed in the same cluster but should have been in different clusters, and a false negative ($fn$) if the documents are placed in different clusters but should have been in the same cluster.\n",
    "\n",
    "The Rand index (measuring the similarity between the NMF clustering and the SCDB categories) is defined as $\\frac{tp+tn}{tp+tn+fp+fn}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39504 963864 81614 147118\n",
      "0.814355977599\n",
      "Wall time: 1.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if 'cluster_pairs_model' in globals():\n",
    "    del cluster_pairs_model\n",
    "if 'cluster_pairs_actual' in globals():\n",
    "    del cluster_pairs_actual\n",
    "\n",
    "# use itertools to create pairs of documents, using the NMF clusters and SCDB categories\n",
    "cluster_pairs_model = itertools.product(clusters, clusters)\n",
    "cluster_pairs_actual = itertools.product(noun_train_issue_areas, noun_train_issue_areas)\n",
    "# loop through all document pairs to compute tp, tn, fp, and fn\n",
    "(tp,tn,fp,fn) = 0, 0, 0, 0\n",
    "for pair1,pair2 in zip(cluster_pairs_model, cluster_pairs_actual):\n",
    "    if (pair1[0]==pair1[1]):\n",
    "        if (pair2[0]==pair2[1]):\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "    else:\n",
    "        if (pair2[0]==pair2[1]):\n",
    "            fn += 1\n",
    "        else:\n",
    "            tn += 1\n",
    "\n",
    "# output tp, tn, fp, and fn\n",
    "print tp,tn,fp,fn\n",
    "# compute and output Rand index using formula from above\n",
    "rand_index_train = float(tp+tn)/(tp+tn+fp+fn)\n",
    "print rand_index_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Applying Model to Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# read in test data from csv\n",
    "noun_test_mat = np.loadtxt(\"noun_test_mat.csv\", delimiter = \",\")\n",
    "# use TF-IDF fit from training data to scale each document's vector to have norm 1 and place a lower weight on very common words\n",
    "noun_test_mat = tf_idf_fit.transform(noun_test_mat).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# use NMF fit from training data to cluster test observations\n",
    "W_test = NMF_fit.transform(noun_test_mat)\n",
    "clusters_test = map(np.argmax, W_test)\n",
    "cluster_lists_test = [[i for i,j in enumerate(clusters_test) if j==cluster] for cluster in range(14)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Compare Test Data Results to SC Database Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents per category: [104, 95, 26, 22, 3, 7, 18, 96, 63, 25, 5, 17, 4, 0]\n"
     ]
    }
   ],
   "source": [
    "# read in SC Database's issue areas from csv\n",
    "noun_test_issue_areas = np.loadtxt(\"noun_test_issue_areas.csv\", delimiter = \",\", dtype=\"int\")\n",
    "# zero-index the array\n",
    "noun_test_issue_areas = noun_test_issue_areas - 1\n",
    "print 'Number of documents per category:', [sum([x==i for x in noun_test_issue_areas]) for i in range(14)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.455670103093\n"
     ]
    }
   ],
   "source": [
    "# use the assignments above to convert the clusters into equivalent SCDB categories for each document\n",
    "new_clusters_test = map(lambda cluster : assignments[cluster], clusters_test)\n",
    "# compute and output accuracy rate\n",
    "correct_test = map(lambda (c1,c2) : c1==c2, zip(new_clusters_test, noun_test_issue_areas))\n",
    "print float(sum(correct_test)) / len(correct_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Use Rand Index to Compute Accuracy on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7869 184414 15288 27654\n",
      "0.817442873844\n",
      "Wall time: 356 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if 'cluster_test_pairs_model' in globals():\n",
    "    del cluster_test_pairs_model\n",
    "if 'cluster_test_pairs_actual' in globals():\n",
    "    del cluster_test_pairs_actual\n",
    "\n",
    "# use itertools to create pairs of documents, using the NMF clusters and SCDB categories\n",
    "cluster_test_pairs_model = itertools.product(clusters_test, clusters_test)\n",
    "cluster_test_pairs_actual = itertools.product(noun_test_issue_areas, noun_test_issue_areas)\n",
    "# loop through all document pairs to compute tp, tn, fp, and fn\n",
    "(tp,tn,fp,fn) = 0, 0, 0, 0\n",
    "for pair1,pair2 in zip(cluster_test_pairs_model, cluster_test_pairs_actual):\n",
    "    if (pair1[0]==pair1[1]):\n",
    "        if (pair2[0]==pair2[1]):\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "    else:\n",
    "        if (pair2[0]==pair2[1]):\n",
    "            fn += 1\n",
    "        else:\n",
    "            tn += 1\n",
    "\n",
    "# output tp, tn, fp, and fn\n",
    "print tp,tn,fp,fn\n",
    "# compute and output Rand index using formula from above\n",
    "rand_index_test = float(tp+tn)/(tp+tn+fp+fn)\n",
    "print rand_index_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Compute Optimal Number of Clusters on Training Data (based on SCDB)\n",
    "Note: In reality, we should do this using one or more validation sets; I've just done this for the time being."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 21min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_clusters_lst = [5, 10, 14, 15, 20, 25, 30]\n",
    "\n",
    "results = dict()\n",
    "for num_clusters in num_clusters_lst:\n",
    "    NMF_fit = sklearn.decomposition.NMF(n_components=num_clusters, init='nndsvda').fit(noun_train_mat)\n",
    "    H = NMF_fit.components_\n",
    "    W = NMF_fit.transform(noun_train_mat)\n",
    "    # contains a tuple (i,j) if document i is in cluster j, for each document\n",
    "    clusters = map(np.argmax, W)\n",
    "    # list of the documents in each cluster\n",
    "    cluster_lists = [[i for i,j in enumerate(clusters) if j==cluster] for cluster in range(num_clusters)]\n",
    "    \n",
    "    # find the 10 most important words for each category\n",
    "    num_best = 10\n",
    "    best_indices = map(lambda v : list(bn.argpartsort(-v,num_best)[0:num_best]), H)\n",
    "    best_words = [[id2noun[i] for i in lst] for lst in best_indices]\n",
    "    \n",
    "    # create a k x 14 matrix (where each row is an NMF cluster and each column is an SCDB cluster) measuring the degree of\n",
    "    # related-ness between each cluster pair\n",
    "    compare_mat = map(lambda r : map(int, r), np.zeros((num_clusters,14)))\n",
    "    # first, assign the (i,j) element in the matrix to the number of cases in NMF cluster i and SCDB cluster j, for each (i,j)\n",
    "    for i,j in zip(clusters, noun_train_issue_areas):\n",
    "        compare_mat[i][j] += 1\n",
    "    # normalize each row to have a sum of 1\n",
    "    compare_mat = map(lambda row : map(float,row) / sum(row), np.array(compare_mat))\n",
    "    # assign each NMF cluster to a (not necessarily unique) SCDB cluster by picking the highest element of each row of compare_mat\n",
    "    assignments = map(np.argmax, compare_mat)\n",
    "    # use the assignments above to convert the clusters into equivalent SCDB categories for each document\n",
    "    new_clusters = map(lambda cluster : assignments[cluster], clusters)\n",
    "    # compute and output accuracy rate\n",
    "    correct = map(lambda (c1,c2) : c1==c2, zip(new_clusters, noun_train_issue_areas))\n",
    "    accuracy = float(sum(correct)) / len(correct)\n",
    "\n",
    "    if 'cluster_pairs_model' in globals():\n",
    "        del cluster_pairs_model\n",
    "    if 'cluster_pairs_actual' in globals():\n",
    "        del cluster_pairs_actual\n",
    "    # use itertools to create pairs of documents, using the NMF clusters and SCDB categories\n",
    "    cluster_pairs_model = itertools.product(clusters, clusters)\n",
    "    cluster_pairs_actual = itertools.product(noun_train_issue_areas, noun_train_issue_areas)\n",
    "    # loop through all document pairs to compute tp, tn, fp, and fn\n",
    "    (tp,tn,fp,fn) = 0, 0, 0, 0\n",
    "    for pair1,pair2 in zip(cluster_pairs_model, cluster_pairs_actual):\n",
    "        if (pair1[0]==pair1[1]):\n",
    "            if (pair2[0]==pair2[1]):\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "        else:\n",
    "            if (pair2[0]==pair2[1]):\n",
    "                fn += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "    # compute Rand index using formula from above\n",
    "    rand_index = float(tp+tn)/(tp+tn+fp+fn)\n",
    "    \n",
    "    results[num_clusters] = (NMF_fit, clusters, cluster_lists, best_words, new_clusters, accuracy, (tp,tn,fp,fn), rand_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4315315315315315,\n",
       " 0.4828828828828829,\n",
       " 0.49099099099099097,\n",
       " 0.5198198198198198,\n",
       " 0.5423423423423424,\n",
       " 0.5423423423423424,\n",
       " 0.5387387387387388]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[results[num_clusters][5] for num_clusters in num_cluster_lst]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Precedents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.96 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# read in training data from csv\n",
    "prec_train_mat = np.loadtxt(\"prec_train_mat.csv\", delimiter = \",\")\n",
    "# use TF-IDF to scale each document's vector to have norm 1 and place a lower weight on very common words\n",
    "tf_idf_fit = sklearn.feature_extraction.text.TfidfTransformer().fit(prec_train_mat)\n",
    "prec_train_mat = tf_idf_fit.transform(prec_train_mat).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# compute NMF fit\n",
    "NMF_fit = sklearn.decomposition.NMF(n_components=14, init='nndsvda').fit(prec_train_mat)\n",
    "H = NMF_fit.components_\n",
    "W = NMF_fit.transform(prec_train_mat)\n",
    "# contains a tuple (i,j) if document i is in cluster j, for each document\n",
    "clusters = map(np.argmax, W)\n",
    "# list of the documents in each cluster\n",
    "cluster_lists = [[i for i,j in enumerate(clusters) if j==cluster] for cluster in range(14)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents per category: [201, 89, 6, 5, 8, 24, 17, 18, 115, 73, 114, 118, 126, 172]\n"
     ]
    }
   ],
   "source": [
    "# classify each document into the category that fits it best\n",
    "print 'Number of documents per category:', [sum([x==i for x in clusters]) for i in range(14)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load vocab from csv\n",
    "prec_vocab = np.genfromtxt(\"prec_vocab.csv\", delimiter=\",\", dtype=\"str\")\n",
    "#prec_vocab = dict(list(enumerate(prec_vocab)))\n",
    "prec_vocab = [(int(i),j) for i,j in prec_vocab]\n",
    "id2prec = dict(prec_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['co. v. national',\n",
       "  'states v. interstate',\n",
       "  'co. v. united',\n",
       "  'states v. el',\n",
       "  'corp. v. el'],\n",
       " ['states v. detroit',\n",
       "  'whitfield v. united',\n",
       "  'statessyllabuskloeckner v. solis;',\n",
       "  'association v. harris;',\n",
       "  'commission v. abercrombie'],\n",
       " ['in v. united',\n",
       "  'limitations.mcmahon v. united',\n",
       "  'corporation v. united',\n",
       "  'affirmed.pinkerton v. united',\n",
       "  'co. v. united'],\n",
       " ['states v. california;',\n",
       "  'trop v. dulles',\n",
       "  'states v. louisiana',\n",
       "  'lessee v. hagan;',\n",
       "  'coleman v. jiffy'],\n",
       " ['younger v. harris;',\n",
       "  'pennzoil v. texaco;',\n",
       "  'samuels v. mackell;',\n",
       "  'steffel v. thompson;',\n",
       "  'byrne v. karalexis'],\n",
       " ['brown v. board',\n",
       "  'green v. county',\n",
       "  'bolling v. sharpe',\n",
       "  'grutter v. bollinger;',\n",
       "  'in v. fcc;'],\n",
       " ['al. v. united',\n",
       "  'states v. mitchell;',\n",
       "  'bragdon v. abbott;',\n",
       "  'states v. grinnell',\n",
       "  'engel v. davenport;'],\n",
       " ['carroll v. united',\n",
       "  'florida v. whitecertiorari',\n",
       "  'corp. v. united',\n",
       "  'statute.fogarty v. united',\n",
       "  'corp. v. cbs;'],\n",
       " ['bivens v. six',\n",
       "  'monell v. new',\n",
       "  'afl-cio v. national',\n",
       "  'pembaur v. city',\n",
       "  'city v. tuttle'],\n",
       " ['corp. v. wages;',\n",
       "  'co. v. state',\n",
       "  'in v. brady;',\n",
       "  'ltd. v. county',\n",
       "  'co. v. bair;'],\n",
       " ['in v. commissioner',\n",
       "  'commissioner v. moore;',\n",
       "  '.commissioner v. glenshaw',\n",
       "  'in v. city',\n",
       "  'freedman v. maryland;'],\n",
       " ['strickland v. washington;',\n",
       "  'anders v. california;',\n",
       "  'williams v. taylor;',\n",
       "  'wiggins v. smith;',\n",
       "  'stone v. powell;'],\n",
       " ['security v. thomas',\n",
       "  'schools v. federal',\n",
       "  'bank v. united',\n",
       "  'in v. natural',\n",
       "  'al. v. brown'],\n",
       " ['apprendi v. new',\n",
       "  'taylor v. united',\n",
       "  'johnson v. united',\n",
       "  'james v. united',\n",
       "  'harris v. united']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find and output the 5 most important words for each category\n",
    "num_best = 5\n",
    "best_indices = map(lambda v : list(bn.argpartsort(-v,num_best)[0:num_best]), H)\n",
    "best_words = [[id2prec[i] for i in lst] for lst in best_indices]\n",
    "best_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents per category: [234, 195, 75, 36, 10, 13, 56, 235, 131, 52, 11, 33, 5, 0]\n"
     ]
    }
   ],
   "source": [
    "# read in SC Database's issue areas from csv\n",
    "prec_train_issue_areas = np.loadtxt(\"prec_train_issue_areas.csv\", delimiter = \",\", dtype=\"int\")\n",
    "# zero-index the array\n",
    "prec_train_issue_areas = prec_train_issue_areas - 1\n",
    "print 'Number of documents per category:', [sum([x==i for x in prec_train_issue_areas]) for i in range(14)]\n",
    "\n",
    "# convert the nx1 issue areas vector into a nx14 matrix of dummies, in case that's ever useful\n",
    "prec_train_issue_areas_dummy = np.array(map(lambda area : np.eye(1,14,area)[0], prec_train_issue_areas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 0, 7, 9, 8, 2, 1, 0, 7, 7, 0, 0, 7, 0]\n",
      "[1, 6, 5, 6, 6, 13, 12, 2, 4, 3, 10, 2, 6, 0]\n"
     ]
    }
   ],
   "source": [
    "# create a 14x14 matrix (where each row is an NMF cluster and each column is an SCDB cluster) measuring the degree of\n",
    "# related-ness between each cluster pair\n",
    "compare_mat = map(lambda r : map(int, r), np.zeros((14,14)))\n",
    "# first, assign the (i,j) element in the matrix to the number of cases in NMF cluster i and SCDB cluster j, for each (i,j)\n",
    "for i,j in zip(clusters, prec_train_issue_areas):\n",
    "    compare_mat[i][j] += 1\n",
    "# normalize each row to have a sum of 1\n",
    "compare_mat = map(lambda row : map(float,row) / sum(row), np.array(compare_mat))\n",
    "\n",
    "### commented out because I realized this method of assignment does the same thing as the one-line assignment below\n",
    "#assignments = -1 * np.ones(14)\n",
    "#compare_mat_flat = [x for lst in compare_mat for x in lst]\n",
    "#while min(assignments) == -1:\n",
    "#    max_ind = np.argmax(compare_mat_flat)\n",
    "#    row = max_ind / 14\n",
    "#    if assignments[row] == -1:\n",
    "#        column = max_ind - 14 * row\n",
    "#        assignments[row] = column\n",
    "#    else:\n",
    "#        compare_mat_flat[max_ind] = -1\n",
    "#assignments = map(int, assignments)\n",
    "#print assignments\n",
    "\n",
    "# assign each NMF cluster to a (not necessarily unique) SCDB cluster by picking the highest element of each row of compare_mat\n",
    "assignments = map(np.argmax, compare_mat)\n",
    "print assignments\n",
    "# this would be the equivalent way of assigning each SCDB cluster to an NMF cluster; this is printed here but not used anywhere\n",
    "print map(np.argmax, np.array(compare_mat).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.271639042357\n"
     ]
    }
   ],
   "source": [
    "# use the assignments above to convert the clusters into equivalent SCDB categories for each document\n",
    "new_clusters = map(lambda cluster : assignments[cluster], clusters)\n",
    "# compute and output accuracy rate\n",
    "correct = map(lambda (c1,c2) : c1==c2, zip(new_clusters, prec_train_issue_areas))\n",
    "print float(sum(correct)) / len(correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cv_results_unsupervised(predict, x, y, score_func, nfolds=5):\n",
    "    results = []\n",
    "    for train, test in KFold(len(y), nfolds):\n",
    "        pred = predict(x[train],y[train],x[test])#clf.fit(x[train]).transform(x[test])\n",
    "        results += [score_func(pred, y[test])] # evaluate score function on held-out data\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fits an NMF model with n_components clusters on the data in x_train and y_train, and use that to predict clusters for x_test.\n",
    "# (Since NMF is unsupervised, the data in y_train is only used to create a mapping from NMF clusters to SCDB clusters, not to\n",
    "# actually fit the model.)\n",
    "def fit_transform(x_train, y_train, x_test, n_components):\n",
    "    # compute an NMF fit on x_train, and produce\n",
    "    fit = sklearn.decomposition.NMF(n_components=n_components, init='nndsvda').fit(x_train)\n",
    "    W_train = fit.transform(x_train)\n",
    "    clusters_train = map(np.argmax, W_train)\n",
    "    \n",
    "    # create a 14x14 matrix (where each row is an NMF cluster and each column is an SCDB cluster) measuring the degree of\n",
    "    # related-ness between each cluster pair\n",
    "    compare_mat = map(lambda r : map(int, r), np.zeros((n_components,14)))\n",
    "    # first, assign the (i,j) element in the matrix to the number of cases in NMF cluster i and SCDB cluster j, for each (i,j)\n",
    "    for i,j in zip(clusters_train, y_train):\n",
    "        compare_mat[i][j] += 1\n",
    "    assignments = map(np.argmax, compare_mat)\n",
    "    \n",
    "    W_test = fit.transform(x_test)\n",
    "    clusters_test = map(np.argmax, W_test)\n",
    "    # use the assignments above to convert the clusters into equivalent SCDB categories for each document\n",
    "    new_clusters = map(lambda cluster : assignments[cluster], clusters_test)\n",
    "    return new_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def accuracy(predicted,actual):\n",
    "    correct = map(lambda (c1,c2) : c1==c2, zip(predicted,actual))\n",
    "    #print type(correct),correct[0]\n",
    "    return float(sum(correct)) / len(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rand_index(predicted,actual):\n",
    "    # use itertools to create pairs of documents, using the NMF clusters and SCDB categories\n",
    "    cluster_pairs_pred = itertools.product(predicted,predicted)\n",
    "    cluster_pairs_actual = itertools.product(actual,actual)\n",
    "    # loop through all document pairs to compute tp, tn, fp, and fn\n",
    "    (tp,tn,fp,fn) = 0, 0, 0, 0\n",
    "    for pair1,pair2 in zip(cluster_pairs_pred, cluster_pairs_actual):\n",
    "        if (pair1[0]==pair1[1]):\n",
    "            if (pair2[0]==pair2[1]):\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "        else:\n",
    "            if (pair2[0]==pair2[1]):\n",
    "                fn += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "    \n",
    "    # compute and output Rand index using formula from above\n",
    "    rand_index_train = float(tp+tn)/(tp+tn+fp+fn)\n",
    "    return rand_index_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cv_example = cv_results_unsupervised(lambda x,y,z : fit_transform(x,y,z,14), noun_train_mat, noun_train_issue_areas,\n",
    "                                     lambda a,b : (accuracy(a,b), rand_index(a,b)), nfolds=5)\n",
    "#cv_example = cv_results_unsupervised(sklearn.decomposition.NMF(n_components=14, init='nndsvda'), noun_train_mat,\n",
    "#                                     noun_train_issue_areas, score_func, nfolds=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.481981981981982,\n",
       " 0.5,\n",
       " 0.46396396396396394,\n",
       " 0.4774774774774775,\n",
       " 0.45495495495495497]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with 5 clusters\n",
      "Testing with 10 clusters\n",
      "Testing with 15 clusters\n",
      "Testing with 20 clusters\n",
      "Testing with 25 clusters\n",
      "Testing with 30 clusters\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_clusters_lst = [5, 10, 15, 20, 25, 30]\n",
    "\n",
    "results = dict()\n",
    "for k in num_clusters_lst:\n",
    "    print 'Testing with %d clusters' % k\n",
    "    results[k] = cv_results_unsupervised(lambda x,y,z : fit_transform(x,y,z,k), noun_train_mat, noun_train_issue_areas,\n",
    "                                         lambda pred,act : (accuracy(pred,act), rand_index(pred,act)), nfolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{5: (0.42612612612612616, 0.68445743040337637),\n",
       " 10: (0.463963963963964, 0.7381624868111355),\n",
       " 15: (0.49279279279279276, 0.7727944160376593),\n",
       " 20: (0.51891891891891895, 0.78395422449476515),\n",
       " 25: (0.51081081081081092, 0.78098368638909177),\n",
       " 30: (0.5, 0.77091145199253297)}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_avg = dict()\n",
    "for k in results.keys():\n",
    "    accuracies, rand_indices = zip(*results[k])\n",
    "    results_avg[k] = (np.mean(accuracies), np.mean(rand_indices))\n",
    "results_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
