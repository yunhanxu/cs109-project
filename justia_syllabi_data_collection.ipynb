{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection  |  Yunhan\n",
    "\n",
    "After deciding that using the full text of opinions would be an inefficient approach, we looked for ways to collect summarized legal data. In Supreme Court opinions, the *syllabus* is a formal, abbreviated section of the text that includes the major facts, arguments, and outcomes of the case. Fortunately, Justia has curated an open-source repository of Supreme Court cases (syllabi and opinions) organized by year and volume. We scraped all of the syllabi from years 1946 and beyond since our validation data set is limited by these temporal parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests as rq\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ---------- grab all case urls from respective year directory pages ----------\n",
    "\n",
    "# base url directory page for each year\n",
    "base_url = \"https://supreme.justia.com/cases/federal/us/year/%s.html\"\n",
    "\n",
    "# base url text page for each case\n",
    "case_url = \"https://supreme.justia.com\"\n",
    "id_url = []\n",
    "\n",
    "# iterate through years 1946 to 2015\n",
    "years = range(1946,2016)\n",
    "for year in years:\n",
    "    soup = BeautifulSoup(rq.get(base_url % year).text, \"lxml\")\n",
    "    results = soup.findAll(\"div\", attrs={\"class\":\"result\"})\n",
    "    \n",
    "    # collect all case urls on each year page\n",
    "    for result in results:\n",
    "        id_url.append(result.a[\"href\"])\n",
    "    \n",
    "    # prevent connection error\n",
    "    sleep(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ---------- visit each case page, scrape syllabus, store data ----------\n",
    "\n",
    "# initially split page into metadata and text (irregular formatting, some null)\n",
    "metadata,syllabus=[],[]\n",
    "\n",
    "# iterate through unique ids collected above\n",
    "for url in id_url:\n",
    "    # go to section of the DOM with text\n",
    "    soup = BeautifulSoup(rq.get(case_url + url).text, \"lxml\")\n",
    "    \n",
    "    # check if syllabus exists\n",
    "    header = soup.find(\"ul\", attrs={\"class\":\"centered-list clear\"})\n",
    "    exists = False\n",
    "    \n",
    "    if header is not None:\n",
    "        if header.text.lower().find(\"syllabus\") > -1:\n",
    "            exists = True\n",
    "\n",
    "        # if syllabus exists, collect text\n",
    "        if exists:\n",
    "            # save name of case\n",
    "            name = soup.find(\"h1\", attrs={\"class\":\"title\"}).text\n",
    "\n",
    "            page_text = soup.find(\"span\", attrs={\"class\":\"headertext\"}) \n",
    "            if page_text is None:\n",
    "                page_text = soup.find(\"div\", attrs={\"id\":\"opinion\"})\n",
    "            \n",
    "            # collect syllabus text\n",
    "            syllabus_list = \"\"\n",
    "            for index in range(0,len(page_text.findAll(\"p\"))):\n",
    "\n",
    "                # don't append blank lines or returns\n",
    "                if page_text.findAll(\"p\")[index] != \"\":\n",
    "                    syllabus_list += ((page_text.findAll(\"p\")[index].text) + \" \")\n",
    "\n",
    "            metadata.append(name)\n",
    "            syllabus.append(syllabus_list)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ---------- create dataframe ----------\n",
    "rawdict = {}\n",
    "rawdict[\"citation\"] = metadata\n",
    "rawdict[\"text\"] = syllabus\n",
    "dfclean = pd.DataFrame(rawdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ---------- clean up citation column -----------\n",
    "years,names = [],[]\n",
    "for x in range(0,len(dftest)):\n",
    "    years.append(re.findall(\"\\s\\((.*)\",dfclean.citation[x])[0][:-1])\n",
    "    names.append(re.findall(\".+?(?=\\s\\d)\",dfclean.citation[x])[0])\n",
    "    \n",
    "dfclean[\"year\"] = years\n",
    "dfclean[\"case\"] = names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfclean.to_csv(\"justia_data_clean.csv\", sep=',', encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
