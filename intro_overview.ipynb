{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling with Supreme Court Cases\n",
    "\n",
    "**Max Liebeskind, Madhu Vijay, and Yunhan Xu**<br>\n",
    "**CS 109, Fall 2015**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/4/43/Supreme_Court_US_2010.jpg\" height=\"500\" width=\"500\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview and Motivation\n",
    "\n",
    "Every year, thousands of legal opinions are written in the United States. The authors of these opinions range from local judges to the Chief Justice of the Supreme Court, yet one thing that many of these opinions share in common is that they count as *precedent*. Because the American legal system is a common law system, judges rely on precedents—previous judicial decisions—to make decisions about new cases. [1] Accordingly, lawyers research precedents to form legal arguments and write legal briefs. Because so many cases exist in the American legal canon, however, it is quite difficult for lawyers to effectively find the case that best fits their research needs. (To get an idea of how many cases are heard every year in the United States, consider that the Supreme Court receives about 10,000 appeals per year, yet these 10,000 cases only represent a subset of the cases heard in Federal Appeals courts and state supreme courts. [2])\n",
    "\n",
    "Our project attempts to reduce the burden on lawyers by making it easier to classify court cases. The millions of cases in the American legal canon are, for the most part, not classified, which means that a tax lawyer who wants a comprehensive list of tax law cases cannot easily obtain such a list. Moreover, classifying cases manually is extremely costly in time and monetary terms. If we could use computerized text analysis to classify cases by topic area (or \"issue area,\" to use a more legal term), this would make classification much easier. Our project takes a first step towards doing this by classifying Supreme Court cases by issue area using text analysis methods. Because Supreme Court cases are widely studied, political scientists have already manually classified them into issue areas; this makes Supreme Court cases an ideal set of cases on which to train text analysis models. In our project, we train a number of different models—both supervised and unsupervised—to classify cases by issue area. We also briefly examine how we can use text analysis to predict the partisanship of cases (i.e., whether the decision leans conservative or liberal).  \n",
    "\n",
    "The remainder of this notebook proceeds as follows. First, we describe the data we use. Second, we give a brief overview of the models we have trained, including how we prepared the data for these models. Finally, we provide a table of contents for the rest of our project. The table of contents gives the order in which our notebooks should be read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] \"The Common Law and Civil Law Traditions.\" The Robbins Collection, *University of California Berkeley*. [Online](https://www.law.berkeley.edu/library/robbins/CommonLawCivilLawTraditions.html).<br>\n",
    "[2] \"How Many Cases are Appealed to the Court Each Year and How Many Cases Does the Court Hear?\" *Supreme Court Frequently Asked Questions*. [Online](http://www.supremecourt.gov/faq.aspx#faqgi9).<br> \n",
    "[3] Supreme Court Image. *Wikipedia*. [Online](https://upload.wikimedia.org/wikipedia/commons/4/43/Supreme_Court_US_2010.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "We obtain our data from two sources: the Justia database [3] and the Supreme Court Database [4]. From the Justia database, we scraped the *syllabus* of every Supreme Court case since 1946 (about 11,000 cases). A syllabus is a summary of the Court's decision in a case. Syllabi range in length from a paragraph to a few pages, and they provide an excellent source of text on which to classify cases by issue area. Because syllabi summarize the basic facts of the case, they generally include key words indicating the issues (and issue areas) at stake in the case. They also frequently reference precedents (i.e., past cases), which are likely to be good indicators of issue area, since cases in the same issue areas will consistently reference the same precedents. Another benefit of syllabi is that they are relatively short in length (a few paragraphs to a few pages). This means that much less memory is needed to analyze syllabi than to analyze actual Supreme Court opinions, which can be hundreds of pages in length. For reference, the *Citizens United* opinion was 183 pages and 48,000 words long. [5]\n",
    "\n",
    "From the Supreme Court Database (SCDB), we download a csv file that contains information on every case the Supreme Court has heard since 1946. (Technically, the level of observation is the \"citation\" or \"dispute,\" which means that if the Supreme Court has heard the same case multiple times, all of these hearings are consolidated into one row. This makes sense, since each hearing would be the same issue area.) SCDB labels each case by issue area, splitting cases into 14 issue areas total. (We describe these issue areas in more detail in `data_merging.ipynb`.) In addition, SCDB labels the outcome of each case as either conservative or liberal (with a simple dummy variable). By merging the SCDB data with the Justia syllabi, we are able to train models in which the output/dependent variable is issue area or partisanship, and the input variable is the text of the case.\n",
    "\n",
    "As we have worked on the project, our approach to using the data has changed slightly. We initially planned to analyze the text of Supreme Court *opinions*, rather than syllabi. However, the text of all Supreme Court opinions since 1946 takes an enormous amount of memory, and we don't think that opinions would give much more information than syllabi. We therefore chose to use syllabi, rather than opinions, as the basis of our text analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://imgs.xkcd.com/comics/supreme_court.png)\n",
    "\n",
    "[3] \"U.S. Case Law.\" *Justia*. [Online](https://supreme.justia.com/cases/).<br>\n",
    "[4] \"Current Dataset: 2015 Release 01.\" *Supreme Court Database*. [Online](http://scdb.wustl.edu/data.php).<br>\n",
    "[5] A. Liptaknov, 2010. \"Justices Are Long on Words but Short on Guidance.\" *The New York Times*. [Online](http://www.nytimes.com/2010/11/18/us/18rulings.html?_r=0). <br>\n",
    "[6] \"Supreme Court.\" *xkcd*. [Online](https://imgs.xkcd.com/comics/supreme_court.png). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "\n",
    "We ran seven different models to classify Supreme Court cases by issue area. Before running models, we clean the syllabi text and convert them into bag-of-words form. The process of data cleaning is described in great detail in `data_merging.ipynb` and `data_cleaning.ipynb`, but essentially it consists of the following steps: (1) cleaning the syllabi to remove unnecessary characters and words, such as html tags that remained from the scraping; (2) removing useless cases from the dataframe; (3) tokenizing the syllabi and converting them into bag-of-words form. Step (3) is most important: using a library for natural language processing, we split each syllabus into a list of words, and then divide the words by word type (noun, verb, etc.). We also separately extract the precedents that each syllabus cites, since precedents are unique to court cases (and are potentially very useful in classifying cases, as noted above.) We then create bags of words: this means that, for each syllabus, we simply count the number of times each word is used, so that we end up with a vector of word frequencies. All of our models use bags of words as inputs. Our models therefore assume that we can classify syllabi simply based on the *words* present in a case and their frequencies, without considering the order of these words or how they interact. This is an important assumption, but we feel it is justified for an initial analysis, since legal terms and precedents tend to be issue area-specific and should therefore be fairly good predictors of issue area.\n",
    "\n",
    "Using bags of words, we ran eight models. Note that some models we test are supervised (i.e., the syllabi (input variable) are labeled by issue area (output variable), and the model tries to best match syllabi with issue areas), while other models are unsupervised (the model classifies syllabi (input variable) based on \"latent variables\"). The models are the following:\n",
    "- Naive Bayes (supervised) \n",
    "- SVM (supervised)\n",
    "- NMF (unsupervised)\n",
    "- k-means (unsupervised)\n",
    "- mean-shift (unsupervised)\n",
    "- lda (unsupervised)\n",
    "- lsi (unsupervised)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please follow the following order when reading the documents in our repository: \n",
    "- [intro_overview.ipynb](https://github.com/yunhanxu/cs109-project/blob/project_data_collection/intro_overview.ipynb) (this document)\n",
    "- [data_scraping.ipynb](https://github.com/yunhanxu/cs109-project/blob/project_data_collection/data_scraping.ipynb)\n",
    "- [data_merging.ipynb](https://github.com/yunhanxu/cs109-project/blob/project_data_collection/data_merging.ipynb)\n",
    "- [data_cleaning.ipynb](https://github.com/yunhanxu/cs109-project/blob/project_data_collection/data_cleaning.ipynb)\n",
    "- [naive_bayes_classification.ipynb](https://github.com/yunhanxu/cs109-project/blob/project_data_collection/naive_bayes_classification.ipynb)\n",
    "- [SVM.ipynb](https://github.com/yunhanxu/cs109-project/blob/project_data_collection/SVM.ipynb)\n",
    "- [NMF.ipynb](https://github.com/yunhanxu/cs109-project/blob/project_data_collection/NMF.ipynb)\n",
    "- [k-means.ipynb](https://github.com/yunhanxu/cs109-project/blob/project_data_collection/k-means.ipynb)\n",
    "- [lda.ipynb](https://github.com/yunhanxu/cs109-project/blob/project_data_collection/lda.ipynb) (note that lda.ipynb includes both the lda and the lsi models)\n",
    "- [mean-shift.ipynb](https://github.com/yunhanxu/cs109-project/blob/project_data_collection/mean_shift.ipynb)\n",
    "- [comparisons_and_conclusion.ipynb](https://github.com/yunhanxu/cs109-project/blob/project_data_collection/comparisons_and_conclusion.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
